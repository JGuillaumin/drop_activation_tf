{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pprint import pprint\n",
    "from math import ceil\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version :  1.10.0\n",
      "Keras version :  2.1.6-tf\n",
      "Built with CUDA :  True\n",
      "Available GPU :  True\n",
      "keras data_format :  channels_first\n"
     ]
    }
   ],
   "source": [
    "print(\"TF version : \", tf.__version__)\n",
    "print(\"Keras version : \", keras.__version__)\n",
    "with_cuda = tf.test.is_built_with_cuda()\n",
    "with_gpu = tf.test.is_gpu_available()\n",
    "print(\"Built with CUDA : \", with_cuda)\n",
    "print(\"Available GPU : \", with_gpu)\n",
    "\n",
    "if with_cuda and with_gpu:\n",
    "    keras.backend.set_image_data_format('channels_first')\n",
    "else: \n",
    "    keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "print(\"keras data_format : \", keras.backend.image_data_format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 200\n",
    "INIT_LR = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading CIFAR10 dataset ...\n",
      "\tTRAIN - images (40000, 3, 32, 32) | float32  - labels (40000, 10) - float32\n",
      "\tVAL - images (10000, 3, 32, 32) | float32  - labels (10000, 10) - float32\n",
      "\tTEST - images (10000, 3, 32, 32) | float32  - labels (10000, 10) - float32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"... loading CIFAR10 dataset ...\")\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "y_train = np.squeeze(y_train)\n",
    "y_test = np.squeeze(y_test)\n",
    "\n",
    "x_train, y_train = shuffle(x_train, y_train, random_state=51)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train,\n",
    "                                                  test_size=0.2,\n",
    "                                                  stratify=y_train,\n",
    "                                                  random_state=51)\n",
    "# cast samples and labels\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_val = x_val.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)\n",
    "y_train = keras.utils.to_categorical(y_train.astype(np.int32), num_classes=10)\n",
    "y_val = keras.utils.to_categorical(y_val.astype(np.int32), num_classes=10)\n",
    "y_test = keras.utils.to_categorical(y_test.astype(np.int32), num_classes=10)\n",
    "\n",
    "print(\"\\tTRAIN - images {} | {}  - labels {} - {}\".format(x_train.shape, x_train.dtype, y_train.shape, y_train.dtype))\n",
    "print(\"\\tVAL - images {} | {}  - labels {} - {}\".format(x_val.shape, x_val.dtype, y_val.shape, y_val.dtype))\n",
    "print(\"\\tTEST - images {} | {}  - labels {} - {}\\n\".format(x_test.shape, x_test.dtype, y_test.shape, y_test.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_aug = keras.preprocessing.image.ImageDataGenerator(samplewise_center=True,\n",
    "                                                             samplewise_std_normalization=True,\n",
    "                                                             width_shift_range=5,\n",
    "                                                             height_shift_range=5,\n",
    "                                                             fill_mode='constant',\n",
    "                                                             cval=0.0,\n",
    "                                                             horizontal_flip=True,\n",
    "                                                             vertical_flip=False,\n",
    "                                                             data_format=keras.backend.image_data_format())\n",
    "\n",
    "generator = keras.preprocessing.image.ImageDataGenerator(samplewise_center=True,\n",
    "                                                         samplewise_std_normalization=True,\n",
    "                                                         data_format=keras.backend.image_data_format())\n",
    "\n",
    "# python iterator object that yields augmented samples \n",
    "iterator_train_aug = generator_aug.flow(x_train, y_train, batch_size=BATCH_SIZE)\n",
    "\n",
    "# python iterators object that yields not augmented samples \n",
    "iterator_train = generator.flow(x_train, y_train, batch_size=BATCH_SIZE)\n",
    "iterator_valid = generator.flow(x_val, y_val, batch_size=BATCH_SIZE)\n",
    "iterator_test = generator.flow(x_test, y_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "steps_per_epoch_train = int(ceil(iterator_train.n/BATCH_SIZE))\n",
    "steps_per_epoch_val = int(ceil(iterator_valid.n/BATCH_SIZE))\n",
    "steps_per_epoch_test = int(ceil(iterator_test.n/BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : (128, 3, 32, 32) | float32\n",
      "y : (128, 10) | float32\n"
     ]
    }
   ],
   "source": [
    "# test iterator with data augmentation\n",
    "x, y = iterator_train_aug.next()\n",
    "\n",
    "print(\"x : {} | {}\".format(x.shape, x.dtype))\n",
    "print(\"y : {} | {}\".format(y.shape, y.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resnet import ResNet56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/zenith/miniconda3/envs/dl-1.10-gpu/lib/python3.5/site-packages/tensorflow/python/keras/initializers.py:104: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`normal` is a deprecated alias for `truncated_normal`\n"
     ]
    }
   ],
   "source": [
    "shape = [3, 32, 32] if keras.backend.image_data_format()=='channels_first' else [32, 32, 3]\n",
    "\n",
    "model = ResNet56(input_shape=shape, classes=10, a=3, b=8, activation='randomized-relu').build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 3, 32, 32)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 16, 30, 30)   432         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 16, 30, 30)   64          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras (Randomi (None, 16, 30, 30)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 30, 30)   2304        randomized_re_lu_keras[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 16, 30, 30)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_1 (Rando (None, 16, 30, 30)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 16, 30, 30)   2304        randomized_re_lu_keras_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 16, 30, 30)   256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 16, 30, 30)   0           conv2d_3[0][0]                   \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16, 30, 30)   64          add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_2 (Rando (None, 16, 30, 30)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 16, 30, 30)   2304        randomized_re_lu_keras_2[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 16, 30, 30)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_3 (Rando (None, 16, 30, 30)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 16, 30, 30)   2304        randomized_re_lu_keras_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 16, 30, 30)   0           conv2d_5[0][0]                   \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 16, 30, 30)   64          add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_4 (Rando (None, 16, 30, 30)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 30, 30)   2304        randomized_re_lu_keras_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 16, 30, 30)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_5 (Rando (None, 16, 30, 30)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 30, 30)   2304        randomized_re_lu_keras_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 16, 30, 30)   0           conv2d_7[0][0]                   \n",
      "                                                                 add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 30, 30)   64          add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_6 (Rando (None, 16, 30, 30)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 30, 30)   2304        randomized_re_lu_keras_6[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 30, 30)   64          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_7 (Rando (None, 16, 30, 30)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 30, 30)   2304        randomized_re_lu_keras_7[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 16, 30, 30)   0           conv2d_9[0][0]                   \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 30, 30)   64          add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_8 (Rando (None, 16, 30, 30)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 30, 30)   2304        randomized_re_lu_keras_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 30, 30)   64          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_9 (Rando (None, 16, 30, 30)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 30, 30)   2304        randomized_re_lu_keras_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 30, 30)   0           conv2d_11[0][0]                  \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 30, 30)   64          add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_10 (Rand (None, 16, 30, 30)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 30, 30)   2304        randomized_re_lu_keras_10[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 30, 30)   64          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_11 (Rand (None, 16, 30, 30)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 30, 30)   2304        randomized_re_lu_keras_11[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 30, 30)   0           conv2d_13[0][0]                  \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 30, 30)   64          add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_12 (Rand (None, 16, 30, 30)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 30, 30)   2304        randomized_re_lu_keras_12[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 30, 30)   64          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_13 (Rand (None, 16, 30, 30)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 30, 30)   2304        randomized_re_lu_keras_13[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 30, 30)   0           conv2d_15[0][0]                  \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 30, 30)   64          add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_14 (Rand (None, 16, 30, 30)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 30, 30)   2304        randomized_re_lu_keras_14[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 30, 30)   64          conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_15 (Rand (None, 16, 30, 30)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 30, 30)   2304        randomized_re_lu_keras_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 16, 30, 30)   0           conv2d_17[0][0]                  \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 30, 30)   64          add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_16 (Rand (None, 16, 30, 30)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 30, 30)   2304        randomized_re_lu_keras_16[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 30, 30)   64          conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_17 (Rand (None, 16, 30, 30)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 16, 30, 30)   2304        randomized_re_lu_keras_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 16, 30, 30)   0           conv2d_19[0][0]                  \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 16, 30, 30)   64          add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_18 (Rand (None, 16, 30, 30)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 16, 32, 32)   0           randomized_re_lu_keras_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 32, 15, 15)   4608        zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 32, 15, 15)   128         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_19 (Rand (None, 32, 15, 15)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 32, 15, 15)   9216        randomized_re_lu_keras_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 32, 15, 15)   512         add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 32, 15, 15)   0           conv2d_22[0][0]                  \n",
      "                                                                 conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 32, 15, 15)   128         add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_20 (Rand (None, 32, 15, 15)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 32, 15, 15)   9216        randomized_re_lu_keras_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 32, 15, 15)   128         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_21 (Rand (None, 32, 15, 15)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 32, 15, 15)   9216        randomized_re_lu_keras_21[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 32, 15, 15)   0           conv2d_24[0][0]                  \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 32, 15, 15)   128         add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_22 (Rand (None, 32, 15, 15)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 32, 15, 15)   9216        randomized_re_lu_keras_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 32, 15, 15)   128         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_23 (Rand (None, 32, 15, 15)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 32, 15, 15)   9216        randomized_re_lu_keras_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 32, 15, 15)   0           conv2d_26[0][0]                  \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 32, 15, 15)   128         add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_24 (Rand (None, 32, 15, 15)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 32, 15, 15)   9216        randomized_re_lu_keras_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 32, 15, 15)   128         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_25 (Rand (None, 32, 15, 15)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 32, 15, 15)   9216        randomized_re_lu_keras_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 32, 15, 15)   0           conv2d_28[0][0]                  \n",
      "                                                                 add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 32, 15, 15)   128         add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_26 (Rand (None, 32, 15, 15)   0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 32, 15, 15)   9216        randomized_re_lu_keras_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 32, 15, 15)   128         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_27 (Rand (None, 32, 15, 15)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 32, 15, 15)   9216        randomized_re_lu_keras_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 32, 15, 15)   0           conv2d_30[0][0]                  \n",
      "                                                                 add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 32, 15, 15)   128         add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_28 (Rand (None, 32, 15, 15)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 32, 15, 15)   9216        randomized_re_lu_keras_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 32, 15, 15)   128         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_29 (Rand (None, 32, 15, 15)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 32, 15, 15)   9216        randomized_re_lu_keras_29[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 32, 15, 15)   0           conv2d_32[0][0]                  \n",
      "                                                                 add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 32, 15, 15)   128         add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_30 (Rand (None, 32, 15, 15)   0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 32, 15, 15)   9216        randomized_re_lu_keras_30[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 32, 15, 15)   128         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_31 (Rand (None, 32, 15, 15)   0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 32, 15, 15)   9216        randomized_re_lu_keras_31[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 32, 15, 15)   0           conv2d_34[0][0]                  \n",
      "                                                                 add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 32, 15, 15)   128         add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_32 (Rand (None, 32, 15, 15)   0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 32, 15, 15)   9216        randomized_re_lu_keras_32[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 32, 15, 15)   128         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_33 (Rand (None, 32, 15, 15)   0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 32, 15, 15)   9216        randomized_re_lu_keras_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 32, 15, 15)   0           conv2d_36[0][0]                  \n",
      "                                                                 add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 32, 15, 15)   128         add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_34 (Rand (None, 32, 15, 15)   0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 32, 15, 15)   9216        randomized_re_lu_keras_34[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 32, 15, 15)   128         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_35 (Rand (None, 32, 15, 15)   0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 32, 15, 15)   9216        randomized_re_lu_keras_35[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 32, 15, 15)   0           conv2d_38[0][0]                  \n",
      "                                                                 add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 32, 15, 15)   128         add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_36 (Rand (None, 32, 15, 15)   0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 32, 17, 17)   0           randomized_re_lu_keras_36[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 64, 8, 8)     18432       zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 64, 8, 8)     256         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_37 (Rand (None, 64, 8, 8)     0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 64, 8, 8)     36864       randomized_re_lu_keras_37[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 64, 8, 8)     2048        add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 64, 8, 8)     0           conv2d_41[0][0]                  \n",
      "                                                                 conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 64, 8, 8)     256         add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_38 (Rand (None, 64, 8, 8)     0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 64, 8, 8)     36864       randomized_re_lu_keras_38[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 64, 8, 8)     256         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_39 (Rand (None, 64, 8, 8)     0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 64, 8, 8)     36864       randomized_re_lu_keras_39[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 64, 8, 8)     0           conv2d_43[0][0]                  \n",
      "                                                                 add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 64, 8, 8)     256         add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_40 (Rand (None, 64, 8, 8)     0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 64, 8, 8)     36864       randomized_re_lu_keras_40[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 64, 8, 8)     256         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_41 (Rand (None, 64, 8, 8)     0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 64, 8, 8)     36864       randomized_re_lu_keras_41[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 64, 8, 8)     0           conv2d_45[0][0]                  \n",
      "                                                                 add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 64, 8, 8)     256         add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_42 (Rand (None, 64, 8, 8)     0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 64, 8, 8)     36864       randomized_re_lu_keras_42[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 64, 8, 8)     256         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_43 (Rand (None, 64, 8, 8)     0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 64, 8, 8)     36864       randomized_re_lu_keras_43[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 64, 8, 8)     0           conv2d_47[0][0]                  \n",
      "                                                                 add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 64, 8, 8)     256         add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_44 (Rand (None, 64, 8, 8)     0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 64, 8, 8)     36864       randomized_re_lu_keras_44[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 64, 8, 8)     256         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_45 (Rand (None, 64, 8, 8)     0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 64, 8, 8)     36864       randomized_re_lu_keras_45[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 64, 8, 8)     0           conv2d_49[0][0]                  \n",
      "                                                                 add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 64, 8, 8)     256         add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_46 (Rand (None, 64, 8, 8)     0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 64, 8, 8)     36864       randomized_re_lu_keras_46[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 64, 8, 8)     256         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_47 (Rand (None, 64, 8, 8)     0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 64, 8, 8)     36864       randomized_re_lu_keras_47[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 64, 8, 8)     0           conv2d_51[0][0]                  \n",
      "                                                                 add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 64, 8, 8)     256         add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_48 (Rand (None, 64, 8, 8)     0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 64, 8, 8)     36864       randomized_re_lu_keras_48[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 64, 8, 8)     256         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_49 (Rand (None, 64, 8, 8)     0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 64, 8, 8)     36864       randomized_re_lu_keras_49[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 64, 8, 8)     0           conv2d_53[0][0]                  \n",
      "                                                                 add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 64, 8, 8)     256         add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_50 (Rand (None, 64, 8, 8)     0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 64, 8, 8)     36864       randomized_re_lu_keras_50[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 64, 8, 8)     256         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_51 (Rand (None, 64, 8, 8)     0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 64, 8, 8)     36864       randomized_re_lu_keras_51[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 64, 8, 8)     0           conv2d_55[0][0]                  \n",
      "                                                                 add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 64, 8, 8)     256         add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_52 (Rand (None, 64, 8, 8)     0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 64, 8, 8)     36864       randomized_re_lu_keras_52[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 64, 8, 8)     256         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_53 (Rand (None, 64, 8, 8)     0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 64, 8, 8)     36864       randomized_re_lu_keras_53[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 64, 8, 8)     0           conv2d_57[0][0]                  \n",
      "                                                                 add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 64, 8, 8)     256         add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "randomized_re_lu_keras_54 (Rand (None, 64, 8, 8)     0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 64)           0           randomized_re_lu_keras_54[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10)           650         avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 859,898\n",
      "Trainable params: 855,834\n",
      "Non-trainable params: 4,064\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=INIT_LR, momentum=0.9)\n",
    "loss = 'categorical_crossentropy'\n",
    "metrics = ['acc', ]\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights('random_weights.h5')\n",
    "model.load_weights('random_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "\n",
    "model_ckpt = keras.callbacks.ModelCheckpoint(\"model_ckpt_dropactivation_best_randomized-relu.h5\",\n",
    "                                             monitor='val_acc', verbose=1, save_best_only=True, \n",
    "                                             save_weights_only=True)\n",
    "callbacks.append(model_ckpt)\n",
    "\n",
    "def schedule(epoch):\n",
    "    if epoch < 91:\n",
    "        return INIT_LR\n",
    "    if epoch < 136:\n",
    "        return 0.1*INIT_LR\n",
    "    if epoch < 182:\n",
    "        return 0.01*INIT_LR\n",
    "    else:\n",
    "        return 0.001*INIT_LR\n",
    "    \n",
    "lr_schedule = keras.callbacks.LearningRateScheduler(schedule, verbose=1)\n",
    "callbacks.append(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 1/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 2.4023 - acc: 0.3980\n",
      "Epoch 00001: val_acc improved from -inf to 0.24990, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 55s 174ms/step - loss: 2.4011 - acc: 0.3984 - val_loss: 4.5791 - val_acc: 0.2499\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 2/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.8578 - acc: 0.5607\n",
      "Epoch 00002: val_acc improved from 0.24990 to 0.46660, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 34s 107ms/step - loss: 1.8577 - acc: 0.5607 - val_loss: 2.1057 - val_acc: 0.4666\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 3/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.6052 - acc: 0.6172- ETA: 2s - loss: 1\n",
      "Epoch 00003: val_acc improved from 0.46660 to 0.53450, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 33s 107ms/step - loss: 1.6046 - acc: 0.6174 - val_loss: 1.8450 - val_acc: 0.5345\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 4/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.4052 - acc: 0.6634\n",
      "Epoch 00004: val_acc improved from 0.53450 to 0.56840, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 34s 108ms/step - loss: 1.4049 - acc: 0.6635 - val_loss: 1.5857 - val_acc: 0.5684\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 5/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.2698 - acc: 0.6952\n",
      "Epoch 00005: val_acc did not improve from 0.56840\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 1.2697 - acc: 0.6952 - val_loss: 1.7179 - val_acc: 0.5594\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 6/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.1604 - acc: 0.7178\n",
      "Epoch 00006: val_acc improved from 0.56840 to 0.63780, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 34s 107ms/step - loss: 1.1602 - acc: 0.7179 - val_loss: 1.3416 - val_acc: 0.6378\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 7/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.0870 - acc: 0.7315\n",
      "Epoch 00007: val_acc did not improve from 0.63780\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 1.0873 - acc: 0.7314 - val_loss: 1.6130 - val_acc: 0.5702\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 8/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.0174 - acc: 0.7489\n",
      "Epoch 00008: val_acc improved from 0.63780 to 0.68680, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 34s 107ms/step - loss: 1.0174 - acc: 0.7489 - val_loss: 1.2111 - val_acc: 0.6868\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 9/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9707 - acc: 0.7574\n",
      "Epoch 00009: val_acc did not improve from 0.68680\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.9706 - acc: 0.7574 - val_loss: 1.3802 - val_acc: 0.6269\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 10/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9243 - acc: 0.7712\n",
      "Epoch 00010: val_acc did not improve from 0.68680\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.9238 - acc: 0.7713 - val_loss: 1.5029 - val_acc: 0.5815\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 11/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9024 - acc: 0.7723\n",
      "Epoch 00011: val_acc did not improve from 0.68680\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.9030 - acc: 0.7723 - val_loss: 1.1723 - val_acc: 0.6797\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 12/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.8750 - acc: 0.7807\n",
      "Epoch 00012: val_acc improved from 0.68680 to 0.70570, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 34s 107ms/step - loss: 0.8755 - acc: 0.7805 - val_loss: 1.1000 - val_acc: 0.7057\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 13/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.8541 - acc: 0.7864\n",
      "Epoch 00013: val_acc improved from 0.70570 to 0.70920, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 33s 107ms/step - loss: 0.8537 - acc: 0.7865 - val_loss: 1.0786 - val_acc: 0.7092\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 14/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.8324 - acc: 0.7932\n",
      "Epoch 00014: val_acc did not improve from 0.70920\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.8329 - acc: 0.7929 - val_loss: 1.4118 - val_acc: 0.5778\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 15/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.8178 - acc: 0.7981\n",
      "Epoch 00015: val_acc did not improve from 0.70920\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.8179 - acc: 0.7979 - val_loss: 1.2210 - val_acc: 0.6603\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 16/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.8052 - acc: 0.8019\n",
      "Epoch 00016: val_acc improved from 0.70920 to 0.75050, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 34s 107ms/step - loss: 0.8058 - acc: 0.8017 - val_loss: 0.9801 - val_acc: 0.7505\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 17/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7971 - acc: 0.8035\n",
      "Epoch 00017: val_acc did not improve from 0.75050\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.7969 - acc: 0.8035 - val_loss: 1.1907 - val_acc: 0.6655\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 18/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7890 - acc: 0.8060\n",
      "Epoch 00018: val_acc did not improve from 0.75050\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.7891 - acc: 0.8060 - val_loss: 1.0218 - val_acc: 0.7243\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 19/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7748 - acc: 0.8109\n",
      "Epoch 00019: val_acc did not improve from 0.75050\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.7749 - acc: 0.8110 - val_loss: 1.1389 - val_acc: 0.6865\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 20/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7640 - acc: 0.8171\n",
      "Epoch 00020: val_acc did not improve from 0.75050\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.7639 - acc: 0.8172 - val_loss: 0.9972 - val_acc: 0.7379\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 21/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7656 - acc: 0.8131\n",
      "Epoch 00021: val_acc did not improve from 0.75050\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.7655 - acc: 0.8132 - val_loss: 1.0719 - val_acc: 0.6948\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 22/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7609 - acc: 0.8170\n",
      "Epoch 00022: val_acc improved from 0.75050 to 0.75140, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 34s 107ms/step - loss: 0.7609 - acc: 0.8170 - val_loss: 0.9449 - val_acc: 0.7514\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 23/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/313 [============================>.] - ETA: 0s - loss: 0.7553 - acc: 0.8199- ETA: 1s - loss: 0.7537\n",
      "Epoch 00023: val_acc did not improve from 0.75140\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.7553 - acc: 0.8199 - val_loss: 1.2459 - val_acc: 0.6588\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 24/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7516 - acc: 0.8211\n",
      "Epoch 00024: val_acc did not improve from 0.75140\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.7516 - acc: 0.8210 - val_loss: 1.1807 - val_acc: 0.6749\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 25/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7466 - acc: 0.8222\n",
      "Epoch 00025: val_acc did not improve from 0.75140\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.7469 - acc: 0.8221 - val_loss: 0.9788 - val_acc: 0.7439\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 26/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7368 - acc: 0.8282\n",
      "Epoch 00026: val_acc did not improve from 0.75140\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.7376 - acc: 0.8281 - val_loss: 1.1911 - val_acc: 0.6793\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 27/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7361 - acc: 0.8276\n",
      "Epoch 00027: val_acc improved from 0.75140 to 0.76010, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 34s 107ms/step - loss: 0.7362 - acc: 0.8276 - val_loss: 0.9631 - val_acc: 0.7601\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 28/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7386 - acc: 0.8280\n",
      "Epoch 00028: val_acc did not improve from 0.76010\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.7382 - acc: 0.8282 - val_loss: 1.0340 - val_acc: 0.7298\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 29/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7254 - acc: 0.8324\n",
      "Epoch 00029: val_acc did not improve from 0.76010\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.7259 - acc: 0.8322 - val_loss: 1.1869 - val_acc: 0.6908\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 30/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7243 - acc: 0.8330\n",
      "Epoch 00030: val_acc did not improve from 0.76010\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.7245 - acc: 0.8331 - val_loss: 1.0202 - val_acc: 0.7240\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 31/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7242 - acc: 0.8349\n",
      "Epoch 00031: val_acc did not improve from 0.76010\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.7241 - acc: 0.8349 - val_loss: 1.0080 - val_acc: 0.7306\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 32/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7197 - acc: 0.8363\n",
      "Epoch 00032: val_acc did not improve from 0.76010\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.7198 - acc: 0.8362 - val_loss: 1.0060 - val_acc: 0.7395\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 33/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7198 - acc: 0.8353\n",
      "Epoch 00033: val_acc did not improve from 0.76010\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.7204 - acc: 0.8352 - val_loss: 1.0772 - val_acc: 0.7205\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 34/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7148 - acc: 0.8363\n",
      "Epoch 00034: val_acc did not improve from 0.76010\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.7150 - acc: 0.8362 - val_loss: 1.0480 - val_acc: 0.7214\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 35/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7124 - acc: 0.8390\n",
      "Epoch 00035: val_acc did not improve from 0.76010\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.7123 - acc: 0.8389 - val_loss: 1.0062 - val_acc: 0.7482\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 36/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7123 - acc: 0.8401\n",
      "Epoch 00036: val_acc did not improve from 0.76010\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.7122 - acc: 0.8401 - val_loss: 1.2590 - val_acc: 0.6511\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 37/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7094 - acc: 0.8405\n",
      "Epoch 00037: val_acc improved from 0.76010 to 0.77000, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 34s 107ms/step - loss: 0.7096 - acc: 0.8404 - val_loss: 0.9347 - val_acc: 0.7700\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 38/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7106 - acc: 0.8400\n",
      "Epoch 00038: val_acc improved from 0.77000 to 0.79400, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 34s 107ms/step - loss: 0.7108 - acc: 0.8400 - val_loss: 0.8668 - val_acc: 0.7940\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 39/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7012 - acc: 0.8440\n",
      "Epoch 00039: val_acc did not improve from 0.79400\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.7016 - acc: 0.8439 - val_loss: 1.1026 - val_acc: 0.7036\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 40/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7002 - acc: 0.8459\n",
      "Epoch 00040: val_acc did not improve from 0.79400\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6999 - acc: 0.8460 - val_loss: 1.0159 - val_acc: 0.7378\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 41/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7003 - acc: 0.8460\n",
      "Epoch 00041: val_acc did not improve from 0.79400\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.7006 - acc: 0.8459 - val_loss: 1.1138 - val_acc: 0.6951\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 42/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7038 - acc: 0.8435\n",
      "Epoch 00042: val_acc did not improve from 0.79400\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.7036 - acc: 0.8437 - val_loss: 0.9714 - val_acc: 0.7510\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 43/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6974 - acc: 0.8474\n",
      "Epoch 00043: val_acc did not improve from 0.79400\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6970 - acc: 0.8475 - val_loss: 1.1990 - val_acc: 0.6919\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 44/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6933 - acc: 0.8475- ETA: 2s - loss: \n",
      "Epoch 00044: val_acc did not improve from 0.79400\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6931 - acc: 0.8475 - val_loss: 1.1790 - val_acc: 0.6756\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 45/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6958 - acc: 0.8473\n",
      "Epoch 00045: val_acc did not improve from 0.79400\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6956 - acc: 0.8473 - val_loss: 0.9064 - val_acc: 0.7850\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 46/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6851 - acc: 0.8517\n",
      "Epoch 00046: val_acc did not improve from 0.79400\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6850 - acc: 0.8516 - val_loss: 1.0600 - val_acc: 0.7232\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 47/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/313 [============================>.] - ETA: 0s - loss: 0.6991 - acc: 0.8467\n",
      "Epoch 00047: val_acc did not improve from 0.79400\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6989 - acc: 0.8466 - val_loss: 1.3093 - val_acc: 0.6423\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 48/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6971 - acc: 0.8478\n",
      "Epoch 00048: val_acc did not improve from 0.79400\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6969 - acc: 0.8479 - val_loss: 0.8855 - val_acc: 0.7940\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 49/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6893 - acc: 0.8497\n",
      "Epoch 00049: val_acc did not improve from 0.79400\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6893 - acc: 0.8498 - val_loss: 1.2230 - val_acc: 0.6830\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 50/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6933 - acc: 0.8502\n",
      "Epoch 00050: val_acc did not improve from 0.79400\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6935 - acc: 0.8500 - val_loss: 0.9083 - val_acc: 0.7794\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 51/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6911 - acc: 0.8515- ETA: 1s - loss: 0.6922 - \n",
      "Epoch 00051: val_acc improved from 0.79400 to 0.80690, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 34s 107ms/step - loss: 0.6913 - acc: 0.8514 - val_loss: 0.8528 - val_acc: 0.8069\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 52/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6831 - acc: 0.8521\n",
      "Epoch 00052: val_acc did not improve from 0.80690\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6832 - acc: 0.8521 - val_loss: 1.0789 - val_acc: 0.7215\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 53/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6836 - acc: 0.8546\n",
      "Epoch 00053: val_acc did not improve from 0.80690\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6840 - acc: 0.8546 - val_loss: 1.1516 - val_acc: 0.7007\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 54/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6825 - acc: 0.8547\n",
      "Epoch 00054: val_acc did not improve from 0.80690\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6826 - acc: 0.8547 - val_loss: 1.1039 - val_acc: 0.7224\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 55/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6863 - acc: 0.8519\n",
      "Epoch 00055: val_acc did not improve from 0.80690\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6865 - acc: 0.8519 - val_loss: 0.9404 - val_acc: 0.7646\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 56/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6816 - acc: 0.8553\n",
      "Epoch 00056: val_acc did not improve from 0.80690\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6815 - acc: 0.8553 - val_loss: 1.2065 - val_acc: 0.6589\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 57/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6751 - acc: 0.8562\n",
      "Epoch 00057: val_acc did not improve from 0.80690\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6751 - acc: 0.8563 - val_loss: 0.9198 - val_acc: 0.7882\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 58/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6806 - acc: 0.8554\n",
      "Epoch 00058: val_acc did not improve from 0.80690\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6804 - acc: 0.8556 - val_loss: 1.1174 - val_acc: 0.7191\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 59/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6783 - acc: 0.8565\n",
      "Epoch 00059: val_acc did not improve from 0.80690\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6785 - acc: 0.8566 - val_loss: 0.9436 - val_acc: 0.7752\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 60/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6734 - acc: 0.8568- ETA: 1s - loss: 0.6728\n",
      "Epoch 00060: val_acc did not improve from 0.80690\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6735 - acc: 0.8568 - val_loss: 0.8667 - val_acc: 0.8009\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 61/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6716 - acc: 0.8581- ETA: 2s - l\n",
      "Epoch 00061: val_acc did not improve from 0.80690\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6717 - acc: 0.8581 - val_loss: 0.8515 - val_acc: 0.8015\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 62/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6788 - acc: 0.8557\n",
      "Epoch 00062: val_acc did not improve from 0.80690\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6791 - acc: 0.8556 - val_loss: 0.9958 - val_acc: 0.7517\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 63/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6735 - acc: 0.8579\n",
      "Epoch 00063: val_acc did not improve from 0.80690\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6734 - acc: 0.8580 - val_loss: 1.0506 - val_acc: 0.7362\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 64/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6730 - acc: 0.8577\n",
      "Epoch 00064: val_acc did not improve from 0.80690\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6732 - acc: 0.8577 - val_loss: 1.0740 - val_acc: 0.7202\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 65/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6706 - acc: 0.8617\n",
      "Epoch 00065: val_acc did not improve from 0.80690\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6706 - acc: 0.8618 - val_loss: 0.9350 - val_acc: 0.7800\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 66/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6692 - acc: 0.8599\n",
      "Epoch 00066: val_acc did not improve from 0.80690\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6695 - acc: 0.8599 - val_loss: 0.9832 - val_acc: 0.7562\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 67/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6733 - acc: 0.8573\n",
      "Epoch 00067: val_acc did not improve from 0.80690\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6731 - acc: 0.8574 - val_loss: 1.0909 - val_acc: 0.7320\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 68/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6686 - acc: 0.8609\n",
      "Epoch 00068: val_acc did not improve from 0.80690\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6686 - acc: 0.8608 - val_loss: 1.0400 - val_acc: 0.7330\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 69/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6655 - acc: 0.8601\n",
      "Epoch 00069: val_acc did not improve from 0.80690\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6660 - acc: 0.8598 - val_loss: 0.9397 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 70/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6657 - acc: 0.8599\n",
      "Epoch 00070: val_acc did not improve from 0.80690\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6658 - acc: 0.8599 - val_loss: 0.8425 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 71/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/313 [============================>.] - ETA: 0s - loss: 0.6656 - acc: 0.8631\n",
      "Epoch 00071: val_acc improved from 0.80690 to 0.81500, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 34s 107ms/step - loss: 0.6654 - acc: 0.8631 - val_loss: 0.8220 - val_acc: 0.8150\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 72/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6660 - acc: 0.8625\n",
      "Epoch 00072: val_acc did not improve from 0.81500\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6662 - acc: 0.8624 - val_loss: 1.1099 - val_acc: 0.7114\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 73/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6693 - acc: 0.8605\n",
      "Epoch 00073: val_acc did not improve from 0.81500\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6690 - acc: 0.8605 - val_loss: 1.0574 - val_acc: 0.7345\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 74/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6722 - acc: 0.8583\n",
      "Epoch 00074: val_acc did not improve from 0.81500\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6718 - acc: 0.8585 - val_loss: 0.9500 - val_acc: 0.7705\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 75/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6595 - acc: 0.8619- ETA: 5s - loss: 0.6\n",
      "Epoch 00075: val_acc did not improve from 0.81500\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6596 - acc: 0.8618 - val_loss: 1.2147 - val_acc: 0.6609\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 76/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6701 - acc: 0.8618\n",
      "Epoch 00076: val_acc did not improve from 0.81500\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6701 - acc: 0.8618 - val_loss: 0.9262 - val_acc: 0.7819\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 77/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6646 - acc: 0.8618\n",
      "Epoch 00077: val_acc improved from 0.81500 to 0.82680, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 34s 107ms/step - loss: 0.6647 - acc: 0.8617 - val_loss: 0.7796 - val_acc: 0.8268\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 78/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6561 - acc: 0.8666\n",
      "Epoch 00078: val_acc did not improve from 0.82680\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6570 - acc: 0.8664 - val_loss: 0.8752 - val_acc: 0.7984\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 79/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6553 - acc: 0.8662\n",
      "Epoch 00079: val_acc did not improve from 0.82680\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6552 - acc: 0.8663 - val_loss: 0.8764 - val_acc: 0.7901\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 80/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6591 - acc: 0.8644\n",
      "Epoch 00080: val_acc did not improve from 0.82680\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6590 - acc: 0.8645 - val_loss: 0.9559 - val_acc: 0.7682\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 81/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6608 - acc: 0.8641\n",
      "Epoch 00081: val_acc did not improve from 0.82680\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6612 - acc: 0.8638 - val_loss: 1.0617 - val_acc: 0.7295\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 82/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6600 - acc: 0.8638\n",
      "Epoch 00082: val_acc did not improve from 0.82680\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6599 - acc: 0.8639 - val_loss: 0.9020 - val_acc: 0.7855\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 83/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6557 - acc: 0.8673\n",
      "Epoch 00083: val_acc did not improve from 0.82680\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6555 - acc: 0.8673 - val_loss: 0.9025 - val_acc: 0.7869\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 84/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6582 - acc: 0.8634\n",
      "Epoch 00084: val_acc did not improve from 0.82680\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6579 - acc: 0.8635 - val_loss: 0.9316 - val_acc: 0.7744\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 85/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6594 - acc: 0.8639- ETA: 2s - lo\n",
      "Epoch 00085: val_acc did not improve from 0.82680\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6596 - acc: 0.8639 - val_loss: 0.8934 - val_acc: 0.8019\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 86/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6623 - acc: 0.8642\n",
      "Epoch 00086: val_acc did not improve from 0.82680\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6620 - acc: 0.8643 - val_loss: 0.9747 - val_acc: 0.7544\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 87/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6535 - acc: 0.8662\n",
      "Epoch 00087: val_acc did not improve from 0.82680\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6538 - acc: 0.8660 - val_loss: 0.8535 - val_acc: 0.8058\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 88/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6509 - acc: 0.8690- ETA: 0s - loss: 0.6512 - acc: 0\n",
      "Epoch 00088: val_acc did not improve from 0.82680\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.6514 - acc: 0.8689 - val_loss: 0.9705 - val_acc: 0.7695\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 89/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6616 - acc: 0.8640\n",
      "Epoch 00089: val_acc did not improve from 0.82680\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6616 - acc: 0.8640 - val_loss: 0.9451 - val_acc: 0.7820\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 90/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6536 - acc: 0.8680\n",
      "Epoch 00090: val_acc did not improve from 0.82680\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.6536 - acc: 0.8680 - val_loss: 0.9961 - val_acc: 0.7483\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 91/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6569 - acc: 0.8663\n",
      "Epoch 00091: val_acc did not improve from 0.82680\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.6570 - acc: 0.8662 - val_loss: 0.8772 - val_acc: 0.7955\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 92/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.5425 - acc: 0.9057\n",
      "Epoch 00092: val_acc improved from 0.82680 to 0.85110, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 33s 107ms/step - loss: 0.5423 - acc: 0.9058 - val_loss: 0.7484 - val_acc: 0.8511\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 93/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4662 - acc: 0.9310\n",
      "Epoch 00093: val_acc did not improve from 0.85110\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.4660 - acc: 0.9310 - val_loss: 0.7314 - val_acc: 0.8467\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 94/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4429 - acc: 0.9356\n",
      "Epoch 00094: val_acc improved from 0.85110 to 0.86250, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 33s 106ms/step - loss: 0.4427 - acc: 0.9357 - val_loss: 0.6750 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 95/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4252 - acc: 0.9399\n",
      "Epoch 00095: val_acc improved from 0.86250 to 0.87660, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 33s 107ms/step - loss: 0.4252 - acc: 0.9398 - val_loss: 0.6288 - val_acc: 0.8766\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 96/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4143 - acc: 0.9415\n",
      "Epoch 00096: val_acc improved from 0.87660 to 0.88470, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.4141 - acc: 0.9415 - val_loss: 0.6032 - val_acc: 0.8847\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 97/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3993 - acc: 0.9456\n",
      "Epoch 00097: val_acc did not improve from 0.88470\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.3992 - acc: 0.9457 - val_loss: 0.6270 - val_acc: 0.8751\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 98/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3889 - acc: 0.9469\n",
      "Epoch 00098: val_acc improved from 0.88470 to 0.88650, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 33s 107ms/step - loss: 0.3886 - acc: 0.9470 - val_loss: 0.5723 - val_acc: 0.8865\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 99/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3806 - acc: 0.9480\n",
      "Epoch 00099: val_acc did not improve from 0.88650\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.3807 - acc: 0.9480 - val_loss: 0.5817 - val_acc: 0.8817\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 100/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3739 - acc: 0.9484\n",
      "Epoch 00100: val_acc improved from 0.88650 to 0.89250, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.3740 - acc: 0.9484 - val_loss: 0.5475 - val_acc: 0.8925\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 101/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3625 - acc: 0.9522\n",
      "Epoch 00101: val_acc did not improve from 0.89250\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.3624 - acc: 0.9523 - val_loss: 0.5557 - val_acc: 0.8895\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 102/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3516 - acc: 0.9544\n",
      "Epoch 00102: val_acc improved from 0.89250 to 0.89270, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.3515 - acc: 0.9544 - val_loss: 0.5402 - val_acc: 0.8927\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 103/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3476 - acc: 0.9527- ETA: 1s - loss: 0.347\n",
      "Epoch 00103: val_acc did not improve from 0.89270\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.3476 - acc: 0.9527 - val_loss: 0.5514 - val_acc: 0.8844\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 104/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3349 - acc: 0.9569\n",
      "Epoch 00104: val_acc did not improve from 0.89270\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.3346 - acc: 0.9570 - val_loss: 0.5468 - val_acc: 0.8843\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 105/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3352 - acc: 0.9558- ETA: 2s\n",
      "Epoch 00105: val_acc did not improve from 0.89270\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.3350 - acc: 0.9558 - val_loss: 0.5342 - val_acc: 0.8899\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 106/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3236 - acc: 0.9568\n",
      "Epoch 00106: val_acc improved from 0.89270 to 0.89280, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.3235 - acc: 0.9568 - val_loss: 0.5179 - val_acc: 0.8928\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 107/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3206 - acc: 0.9571\n",
      "Epoch 00107: val_acc did not improve from 0.89280\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.3206 - acc: 0.9571 - val_loss: 0.5280 - val_acc: 0.8867\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 108/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3146 - acc: 0.9577\n",
      "Epoch 00108: val_acc did not improve from 0.89280\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.3144 - acc: 0.9578 - val_loss: 0.5275 - val_acc: 0.8850\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 109/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3093 - acc: 0.9583\n",
      "Epoch 00109: val_acc did not improve from 0.89280\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.3094 - acc: 0.9583 - val_loss: 0.5234 - val_acc: 0.8881\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 110/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3071 - acc: 0.9575\n",
      "Epoch 00110: val_acc improved from 0.89280 to 0.89390, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.3070 - acc: 0.9575 - val_loss: 0.5111 - val_acc: 0.8939\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 111/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2997 - acc: 0.9595\n",
      "Epoch 00111: val_acc did not improve from 0.89390\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.2996 - acc: 0.9596 - val_loss: 0.5283 - val_acc: 0.8844\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 112/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2950 - acc: 0.9608\n",
      "Epoch 00112: val_acc did not improve from 0.89390\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.2952 - acc: 0.9606 - val_loss: 0.5332 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 113/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2899 - acc: 0.9619\n",
      "Epoch 00113: val_acc did not improve from 0.89390\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.2903 - acc: 0.9619 - val_loss: 0.5500 - val_acc: 0.8765\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 114/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2894 - acc: 0.9598\n",
      "Epoch 00114: val_acc did not improve from 0.89390\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.2895 - acc: 0.9597 - val_loss: 0.5139 - val_acc: 0.8885\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 115/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2824 - acc: 0.9618\n",
      "Epoch 00115: val_acc did not improve from 0.89390\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.2824 - acc: 0.9618 - val_loss: 0.5396 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 116/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/313 [============================>.] - ETA: 0s - loss: 0.2820 - acc: 0.9610\n",
      "Epoch 00116: val_acc did not improve from 0.89390\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.2823 - acc: 0.9608 - val_loss: 0.5211 - val_acc: 0.8832\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 117/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2766 - acc: 0.9624\n",
      "Epoch 00117: val_acc did not improve from 0.89390\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.2765 - acc: 0.9625 - val_loss: 0.4980 - val_acc: 0.8906\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 118/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2753 - acc: 0.9622\n",
      "Epoch 00118: val_acc did not improve from 0.89390\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.2752 - acc: 0.9622 - val_loss: 0.4838 - val_acc: 0.8916\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 119/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2743 - acc: 0.9617\n",
      "Epoch 00119: val_acc did not improve from 0.89390\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.2741 - acc: 0.9618 - val_loss: 0.5351 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 120/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2696 - acc: 0.9626- ETA: 1s - loss: 0.26\n",
      "Epoch 00120: val_acc did not improve from 0.89390\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.2695 - acc: 0.9626 - val_loss: 0.4989 - val_acc: 0.8847\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 121/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2658 - acc: 0.9624\n",
      "Epoch 00121: val_acc did not improve from 0.89390\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.2661 - acc: 0.9623 - val_loss: 0.5083 - val_acc: 0.8832\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 122/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2635 - acc: 0.9630\n",
      "Epoch 00122: val_acc did not improve from 0.89390\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.2636 - acc: 0.9630 - val_loss: 0.5228 - val_acc: 0.8762\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 123/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2633 - acc: 0.9632\n",
      "Epoch 00123: val_acc did not improve from 0.89390\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.2634 - acc: 0.9632 - val_loss: 0.5320 - val_acc: 0.8711\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 124/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2587 - acc: 0.9643\n",
      "Epoch 00124: val_acc did not improve from 0.89390\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.2586 - acc: 0.9643 - val_loss: 0.4859 - val_acc: 0.8874\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 125/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2585 - acc: 0.9632\n",
      "Epoch 00125: val_acc did not improve from 0.89390\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.2583 - acc: 0.9633 - val_loss: 0.4968 - val_acc: 0.8868\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 126/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2605 - acc: 0.9611\n",
      "Epoch 00126: val_acc did not improve from 0.89390\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.2607 - acc: 0.9609 - val_loss: 0.5029 - val_acc: 0.8806\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 127/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2558 - acc: 0.9635\n",
      "Epoch 00127: val_acc did not improve from 0.89390\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.2559 - acc: 0.9634 - val_loss: 0.5022 - val_acc: 0.8810\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 128/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2567 - acc: 0.9625\n",
      "Epoch 00128: val_acc did not improve from 0.89390\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.2566 - acc: 0.9626 - val_loss: 0.4764 - val_acc: 0.8919\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 129/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2519 - acc: 0.9640\n",
      "Epoch 00129: val_acc did not improve from 0.89390\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.2519 - acc: 0.9641 - val_loss: 0.5159 - val_acc: 0.8788\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 130/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2459 - acc: 0.9647\n",
      "Epoch 00130: val_acc did not improve from 0.89390\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.2460 - acc: 0.9646 - val_loss: 0.5160 - val_acc: 0.8787\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 131/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2517 - acc: 0.9631\n",
      "Epoch 00131: val_acc did not improve from 0.89390\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.2518 - acc: 0.9631 - val_loss: 0.4750 - val_acc: 0.8934\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 132/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2552 - acc: 0.9607\n",
      "Epoch 00132: val_acc did not improve from 0.89390\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.2550 - acc: 0.9608 - val_loss: 0.4887 - val_acc: 0.8846\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 133/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2486 - acc: 0.9630\n",
      "Epoch 00133: val_acc did not improve from 0.89390\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.2485 - acc: 0.9630 - val_loss: 0.5398 - val_acc: 0.8653\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 134/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2464 - acc: 0.9644\n",
      "Epoch 00134: val_acc did not improve from 0.89390\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.2463 - acc: 0.9644 - val_loss: 0.4900 - val_acc: 0.8838\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 135/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2483 - acc: 0.9627\n",
      "Epoch 00135: val_acc did not improve from 0.89390\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.2482 - acc: 0.9627 - val_loss: 0.5041 - val_acc: 0.8791\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 136/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2418 - acc: 0.9652\n",
      "Epoch 00136: val_acc did not improve from 0.89390\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.2418 - acc: 0.9652 - val_loss: 0.4972 - val_acc: 0.8795\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 137/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2144 - acc: 0.9750\n",
      "Epoch 00137: val_acc improved from 0.89390 to 0.90110, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 33s 107ms/step - loss: 0.2143 - acc: 0.9750 - val_loss: 0.4377 - val_acc: 0.9011\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 138/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1944 - acc: 0.9830\n",
      "Epoch 00138: val_acc improved from 0.90110 to 0.90150, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.1945 - acc: 0.9830 - val_loss: 0.4340 - val_acc: 0.9015\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 139/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/313 [============================>.] - ETA: 0s - loss: 0.1877 - acc: 0.9853\n",
      "Epoch 00139: val_acc improved from 0.90150 to 0.90230, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.1879 - acc: 0.9852 - val_loss: 0.4340 - val_acc: 0.9023\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 140/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1858 - acc: 0.9860\n",
      "Epoch 00140: val_acc improved from 0.90230 to 0.90370, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.1858 - acc: 0.9860 - val_loss: 0.4283 - val_acc: 0.9037\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 141/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1827 - acc: 0.9873\n",
      "Epoch 00141: val_acc improved from 0.90370 to 0.90490, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.1826 - acc: 0.9873 - val_loss: 0.4267 - val_acc: 0.9049\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 142/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1791 - acc: 0.9884\n",
      "Epoch 00142: val_acc did not improve from 0.90490\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1791 - acc: 0.9884 - val_loss: 0.4305 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 143/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1773 - acc: 0.9885\n",
      "Epoch 00143: val_acc improved from 0.90490 to 0.90610, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 33s 107ms/step - loss: 0.1774 - acc: 0.9884 - val_loss: 0.4270 - val_acc: 0.9061\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 144/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1754 - acc: 0.9894\n",
      "Epoch 00144: val_acc improved from 0.90610 to 0.90640, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.1754 - acc: 0.9894 - val_loss: 0.4242 - val_acc: 0.9064\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 145/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1731 - acc: 0.9898\n",
      "Epoch 00145: val_acc did not improve from 0.90640\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1731 - acc: 0.9898 - val_loss: 0.4250 - val_acc: 0.9060\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 146/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1730 - acc: 0.9894\n",
      "Epoch 00146: val_acc did not improve from 0.90640\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1730 - acc: 0.9895 - val_loss: 0.4259 - val_acc: 0.9054\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 147/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1715 - acc: 0.9897\n",
      "Epoch 00147: val_acc improved from 0.90640 to 0.90660, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.1716 - acc: 0.9897 - val_loss: 0.4264 - val_acc: 0.9066\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 148/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1702 - acc: 0.9908\n",
      "Epoch 00148: val_acc improved from 0.90660 to 0.90750, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.1702 - acc: 0.9908 - val_loss: 0.4239 - val_acc: 0.9075\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 149/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1684 - acc: 0.9911\n",
      "Epoch 00149: val_acc did not improve from 0.90750\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1685 - acc: 0.9910 - val_loss: 0.4312 - val_acc: 0.9055\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 150/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1678 - acc: 0.9912\n",
      "Epoch 00150: val_acc did not improve from 0.90750\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1680 - acc: 0.9911 - val_loss: 0.4275 - val_acc: 0.9072\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 151/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1684 - acc: 0.9913\n",
      "Epoch 00151: val_acc improved from 0.90750 to 0.90760, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.1684 - acc: 0.9913 - val_loss: 0.4220 - val_acc: 0.9076\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 152/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1666 - acc: 0.9917\n",
      "Epoch 00152: val_acc did not improve from 0.90760\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1666 - acc: 0.9918 - val_loss: 0.4297 - val_acc: 0.9064\n",
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 153/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1640 - acc: 0.9926\n",
      "Epoch 00153: val_acc did not improve from 0.90760\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1639 - acc: 0.9926 - val_loss: 0.4274 - val_acc: 0.9067\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 154/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1643 - acc: 0.9919\n",
      "Epoch 00154: val_acc did not improve from 0.90760\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1644 - acc: 0.9919 - val_loss: 0.4283 - val_acc: 0.9065\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 155/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1646 - acc: 0.9915\n",
      "Epoch 00155: val_acc improved from 0.90760 to 0.90790, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 33s 107ms/step - loss: 0.1646 - acc: 0.9915 - val_loss: 0.4255 - val_acc: 0.9079\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 156/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1631 - acc: 0.9922\n",
      "Epoch 00156: val_acc did not improve from 0.90790\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1631 - acc: 0.9922 - val_loss: 0.4308 - val_acc: 0.9063\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 157/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1642 - acc: 0.9920\n",
      "Epoch 00157: val_acc did not improve from 0.90790\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1643 - acc: 0.9920 - val_loss: 0.4316 - val_acc: 0.9054\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 158/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1610 - acc: 0.9924\n",
      "Epoch 00158: val_acc improved from 0.90790 to 0.90810, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.1610 - acc: 0.9924 - val_loss: 0.4244 - val_acc: 0.9081\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 159/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1603 - acc: 0.9925\n",
      "Epoch 00159: val_acc did not improve from 0.90810\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1605 - acc: 0.9924 - val_loss: 0.4296 - val_acc: 0.9065\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 160/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1602 - acc: 0.9928- ETA: 2s \n",
      "Epoch 00160: val_acc did not improve from 0.90810\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1602 - acc: 0.9928 - val_loss: 0.4253 - val_acc: 0.9075\n",
      "\n",
      "Epoch 00161: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 161/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/313 [============================>.] - ETA: 0s - loss: 0.1590 - acc: 0.9932\n",
      "Epoch 00161: val_acc improved from 0.90810 to 0.91060, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 33s 107ms/step - loss: 0.1590 - acc: 0.9932 - val_loss: 0.4253 - val_acc: 0.9106\n",
      "\n",
      "Epoch 00162: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 162/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1582 - acc: 0.9933\n",
      "Epoch 00162: val_acc did not improve from 0.91060\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1582 - acc: 0.9933 - val_loss: 0.4256 - val_acc: 0.9091\n",
      "\n",
      "Epoch 00163: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 163/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1588 - acc: 0.9931\n",
      "Epoch 00163: val_acc did not improve from 0.91060\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1589 - acc: 0.9930 - val_loss: 0.4267 - val_acc: 0.9090\n",
      "\n",
      "Epoch 00164: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 164/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1576 - acc: 0.9931\n",
      "Epoch 00164: val_acc did not improve from 0.91060\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1576 - acc: 0.9931 - val_loss: 0.4260 - val_acc: 0.9083\n",
      "\n",
      "Epoch 00165: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 165/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1557 - acc: 0.9939\n",
      "Epoch 00165: val_acc did not improve from 0.91060\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1557 - acc: 0.9939 - val_loss: 0.4278 - val_acc: 0.9068\n",
      "\n",
      "Epoch 00166: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 166/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1579 - acc: 0.9927\n",
      "Epoch 00166: val_acc did not improve from 0.91060\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1579 - acc: 0.9927 - val_loss: 0.4271 - val_acc: 0.9096\n",
      "\n",
      "Epoch 00167: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 167/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1556 - acc: 0.9937\n",
      "Epoch 00167: val_acc improved from 0.91060 to 0.91180, saving model to model_ckpt_dropactivation_best_randomized-relu.h5\n",
      "313/313 [==============================] - 33s 107ms/step - loss: 0.1557 - acc: 0.9937 - val_loss: 0.4202 - val_acc: 0.9118\n",
      "\n",
      "Epoch 00168: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 168/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1541 - acc: 0.9937\n",
      "Epoch 00168: val_acc did not improve from 0.91180\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1541 - acc: 0.9936 - val_loss: 0.4215 - val_acc: 0.9091\n",
      "\n",
      "Epoch 00169: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 169/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1549 - acc: 0.9935\n",
      "Epoch 00169: val_acc did not improve from 0.91180\n",
      "313/313 [==============================] - 33s 106ms/step - loss: 0.1549 - acc: 0.9935 - val_loss: 0.4214 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00170: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 170/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1537 - acc: 0.9937\n",
      "Epoch 00170: val_acc did not improve from 0.91180\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1537 - acc: 0.9937 - val_loss: 0.4269 - val_acc: 0.9080\n",
      "\n",
      "Epoch 00171: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 171/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1542 - acc: 0.9937\n",
      "Epoch 00171: val_acc did not improve from 0.91180\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1543 - acc: 0.9936 - val_loss: 0.4267 - val_acc: 0.9066\n",
      "\n",
      "Epoch 00172: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 172/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1513 - acc: 0.9945\n",
      "Epoch 00172: val_acc did not improve from 0.91180\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1514 - acc: 0.9944 - val_loss: 0.4254 - val_acc: 0.9097\n",
      "\n",
      "Epoch 00173: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 173/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1515 - acc: 0.9943\n",
      "Epoch 00173: val_acc did not improve from 0.91180\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1515 - acc: 0.9943 - val_loss: 0.4274 - val_acc: 0.9083\n",
      "\n",
      "Epoch 00174: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 174/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1504 - acc: 0.9942\n",
      "Epoch 00174: val_acc did not improve from 0.91180\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1504 - acc: 0.9942 - val_loss: 0.4233 - val_acc: 0.9089\n",
      "\n",
      "Epoch 00175: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 175/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1513 - acc: 0.9942\n",
      "Epoch 00175: val_acc did not improve from 0.91180\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1513 - acc: 0.9942 - val_loss: 0.4274 - val_acc: 0.9084\n",
      "\n",
      "Epoch 00176: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 176/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1512 - acc: 0.9935- ETA: 1s - loss: 0.\n",
      "Epoch 00176: val_acc did not improve from 0.91180\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1512 - acc: 0.9935 - val_loss: 0.4310 - val_acc: 0.9055\n",
      "\n",
      "Epoch 00177: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 177/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1495 - acc: 0.9945\n",
      "Epoch 00177: val_acc did not improve from 0.91180\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1495 - acc: 0.9944 - val_loss: 0.4236 - val_acc: 0.9090\n",
      "\n",
      "Epoch 00178: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 178/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1496 - acc: 0.9944\n",
      "Epoch 00178: val_acc did not improve from 0.91180\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1495 - acc: 0.9944 - val_loss: 0.4305 - val_acc: 0.9077\n",
      "\n",
      "Epoch 00179: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 179/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1500 - acc: 0.9943\n",
      "Epoch 00179: val_acc did not improve from 0.91180\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1500 - acc: 0.9944 - val_loss: 0.4267 - val_acc: 0.9083\n",
      "\n",
      "Epoch 00180: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 180/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1488 - acc: 0.9941\n",
      "Epoch 00180: val_acc did not improve from 0.91180\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1489 - acc: 0.9941 - val_loss: 0.4320 - val_acc: 0.9060\n",
      "\n",
      "Epoch 00181: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 181/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1484 - acc: 0.9942\n",
      "Epoch 00181: val_acc did not improve from 0.91180\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1485 - acc: 0.9943 - val_loss: 0.4311 - val_acc: 0.9077\n",
      "\n",
      "Epoch 00182: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 182/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1474 - acc: 0.9946\n",
      "Epoch 00182: val_acc did not improve from 0.91180\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1475 - acc: 0.9946 - val_loss: 0.4278 - val_acc: 0.9086\n",
      "\n",
      "Epoch 00183: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 183/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1471 - acc: 0.9942\n",
      "Epoch 00183: val_acc did not improve from 0.91180\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1471 - acc: 0.9943 - val_loss: 0.4276 - val_acc: 0.9094\n",
      "\n",
      "Epoch 00184: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 184/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1458 - acc: 0.9950\n",
      "Epoch 00184: val_acc did not improve from 0.91180\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1458 - acc: 0.9950 - val_loss: 0.4271 - val_acc: 0.9096\n",
      "\n",
      "Epoch 00185: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 185/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/313 [============================>.] - ETA: 0s - loss: 0.1462 - acc: 0.9950\n",
      "Epoch 00185: val_acc did not improve from 0.91180\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1461 - acc: 0.9950 - val_loss: 0.4262 - val_acc: 0.9100\n",
      "\n",
      "Epoch 00186: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 186/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1464 - acc: 0.9948\n",
      "Epoch 00186: val_acc did not improve from 0.91180\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1466 - acc: 0.9947 - val_loss: 0.4260 - val_acc: 0.9091\n",
      "\n",
      "Epoch 00187: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 187/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1447 - acc: 0.9957\n",
      "Epoch 00187: val_acc did not improve from 0.91180\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1448 - acc: 0.9957 - val_loss: 0.4253 - val_acc: 0.9090\n",
      "\n",
      "Epoch 00188: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 188/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1457 - acc: 0.9951\n",
      "Epoch 00188: val_acc did not improve from 0.91180\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1456 - acc: 0.9951 - val_loss: 0.4259 - val_acc: 0.9091\n",
      "\n",
      "Epoch 00189: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 189/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1455 - acc: 0.9953\n",
      "Epoch 00189: val_acc did not improve from 0.91180\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1455 - acc: 0.9953 - val_loss: 0.4256 - val_acc: 0.9093\n",
      "\n",
      "Epoch 00190: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 190/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1452 - acc: 0.9955\n",
      "Epoch 00190: val_acc did not improve from 0.91180\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1452 - acc: 0.9955 - val_loss: 0.4264 - val_acc: 0.9091\n",
      "\n",
      "Epoch 00191: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 191/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1456 - acc: 0.9951\n",
      "Epoch 00191: val_acc did not improve from 0.91180\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1456 - acc: 0.9951 - val_loss: 0.4254 - val_acc: 0.9090\n",
      "\n",
      "Epoch 00192: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 192/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1460 - acc: 0.9950\n",
      "Epoch 00192: val_acc did not improve from 0.91180\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1459 - acc: 0.9951 - val_loss: 0.4247 - val_acc: 0.9095\n",
      "\n",
      "Epoch 00193: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 193/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1450 - acc: 0.9954\n",
      "Epoch 00193: val_acc did not improve from 0.91180\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1450 - acc: 0.9954 - val_loss: 0.4249 - val_acc: 0.9091\n",
      "\n",
      "Epoch 00194: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 194/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1455 - acc: 0.9946\n",
      "Epoch 00194: val_acc did not improve from 0.91180\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1455 - acc: 0.9946 - val_loss: 0.4253 - val_acc: 0.9092\n",
      "\n",
      "Epoch 00195: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 195/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1445 - acc: 0.9956\n",
      "Epoch 00195: val_acc did not improve from 0.91180\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1446 - acc: 0.9956 - val_loss: 0.4249 - val_acc: 0.9096\n",
      "\n",
      "Epoch 00196: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 196/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1450 - acc: 0.9951\n",
      "Epoch 00196: val_acc did not improve from 0.91180\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1450 - acc: 0.9951 - val_loss: 0.4249 - val_acc: 0.9088\n",
      "\n",
      "Epoch 00197: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 197/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1451 - acc: 0.9954\n",
      "Epoch 00197: val_acc did not improve from 0.91180\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1450 - acc: 0.9955 - val_loss: 0.4254 - val_acc: 0.9088\n",
      "\n",
      "Epoch 00198: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 198/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1438 - acc: 0.9960\n",
      "Epoch 00198: val_acc did not improve from 0.91180\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1438 - acc: 0.9960 - val_loss: 0.4258 - val_acc: 0.9083\n",
      "\n",
      "Epoch 00199: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 199/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1443 - acc: 0.9956\n",
      "Epoch 00199: val_acc did not improve from 0.91180\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1443 - acc: 0.9956 - val_loss: 0.4246 - val_acc: 0.9089\n",
      "\n",
      "Epoch 00200: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 200/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1442 - acc: 0.9954\n",
      "Epoch 00200: val_acc did not improve from 0.91180\n",
      "313/313 [==============================] - 33s 105ms/step - loss: 0.1442 - acc: 0.9955 - val_loss: 0.4249 - val_acc: 0.9088\n",
      "CPU times: user 1h 49min 10s, sys: 16min 44s, total: 2h 5min 55s\n",
      "Wall time: 1h 50min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "history = model.fit_generator(iterator_train_aug, \n",
    "                              steps_per_epoch=steps_per_epoch_train,\n",
    "                              epochs=EPOCHS,\n",
    "                              verbose=1,\n",
    "                              validation_data=iterator_valid,\n",
    "                              validation_steps=steps_per_epoch_val,\n",
    "                              callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('randomized-relu-history.dict', 'wb') as f:\n",
    "    pickle.dump(history.history, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['val_loss', 'acc', 'loss', 'val_acc']\n"
     ]
    }
   ],
   "source": [
    "hist_dict = history.history\n",
    "print(list(hist_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "model.load_weights('model_ckpt_dropactivation_best_randomized-relu.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best epoch : 166  |  0.9118\n"
     ]
    }
   ],
   "source": [
    "best_epoch = np.argmax(hist_dict['val_acc'])\n",
    "print(\"best epoch : {}  |  {}\".format(best_epoch, hist_dict['val_acc'][best_epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 2s 22ms/step\n",
      "ACC (test) :  0.9065\n",
      "LOSS (test) :  0.4296488745689392\n"
     ]
    }
   ],
   "source": [
    "loss_test, acc_test = model.evaluate_generator(generator=iterator_test, steps=steps_per_epoch_test, verbose=1)\n",
    "print(\"ACC (test) : \", acc_test)\n",
    "print(\"LOSS (test) : \", loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f1aefd4aa58>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "plt.plot(list(range(EPOCHS)), hist_dict['acc'], color='red', label='train acc')\n",
    "plt.plot(list(range(EPOCHS)), hist_dict['val_acc'], color='green', label='val acc')\n",
    "plt.scatter([best_epoch, ], [acc_test, ], color='blue', label='test acc')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f1b95744390>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "plt.plot(list(range(EPOCHS)), hist_dict['loss'], color='red', label='train loss')\n",
    "plt.plot(list(range(EPOCHS)), hist_dict['val_loss'], color='green', label='val loss')\n",
    "plt.scatter([best_epoch, ], [loss_test, ], color='blue', label='test loss')\n",
    "plt.legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
