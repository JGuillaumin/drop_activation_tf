{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pprint import pprint\n",
    "from math import ceil\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version :  1.10.0\n",
      "Keras version :  2.1.6-tf\n",
      "Built with CUDA :  True\n",
      "Available GPU :  True\n",
      "keras data_format :  channels_first\n"
     ]
    }
   ],
   "source": [
    "print(\"TF version : \", tf.__version__)\n",
    "print(\"Keras version : \", keras.__version__)\n",
    "with_cuda = tf.test.is_built_with_cuda()\n",
    "with_gpu = tf.test.is_gpu_available()\n",
    "print(\"Built with CUDA : \", with_cuda)\n",
    "print(\"Available GPU : \", with_gpu)\n",
    "\n",
    "if with_cuda and with_gpu:\n",
    "    keras.backend.set_image_data_format('channels_first')\n",
    "else: \n",
    "    keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "print(\"keras data_format : \", keras.backend.image_data_format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 200\n",
    "INIT_LR = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading CIFAR10 dataset ...\n",
      "\tTRAIN - images (40000, 3, 32, 32) | float32  - labels (40000, 10) - float32\n",
      "\tVAL - images (10000, 3, 32, 32) | float32  - labels (10000, 10) - float32\n",
      "\tTEST - images (10000, 3, 32, 32) | float32  - labels (10000, 10) - float32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"... loading CIFAR10 dataset ...\")\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "y_train = np.squeeze(y_train)\n",
    "y_test = np.squeeze(y_test)\n",
    "\n",
    "x_train, y_train = shuffle(x_train, y_train, random_state=51)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train,\n",
    "                                                  test_size=0.2,\n",
    "                                                  stratify=y_train,\n",
    "                                                  random_state=51)\n",
    "# cast samples and labels\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_val = x_val.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)\n",
    "y_train = keras.utils.to_categorical(y_train.astype(np.int32), num_classes=10)\n",
    "y_val = keras.utils.to_categorical(y_val.astype(np.int32), num_classes=10)\n",
    "y_test = keras.utils.to_categorical(y_test.astype(np.int32), num_classes=10)\n",
    "\n",
    "print(\"\\tTRAIN - images {} | {}  - labels {} - {}\".format(x_train.shape, x_train.dtype, y_train.shape, y_train.dtype))\n",
    "print(\"\\tVAL - images {} | {}  - labels {} - {}\".format(x_val.shape, x_val.dtype, y_val.shape, y_val.dtype))\n",
    "print(\"\\tTEST - images {} | {}  - labels {} - {}\\n\".format(x_test.shape, x_test.dtype, y_test.shape, y_test.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_aug = keras.preprocessing.image.ImageDataGenerator(samplewise_center=True,\n",
    "                                                             samplewise_std_normalization=True,\n",
    "                                                             width_shift_range=5,\n",
    "                                                             height_shift_range=5,\n",
    "                                                             fill_mode='constant',\n",
    "                                                             cval=0.0,\n",
    "                                                             horizontal_flip=True,\n",
    "                                                             vertical_flip=False,\n",
    "                                                             data_format=keras.backend.image_data_format())\n",
    "\n",
    "generator = keras.preprocessing.image.ImageDataGenerator(samplewise_center=True,\n",
    "                                                         samplewise_std_normalization=True,\n",
    "                                                         data_format=keras.backend.image_data_format())\n",
    "\n",
    "# python iterator object that yields augmented samples \n",
    "iterator_train_aug = generator_aug.flow(x_train, y_train, batch_size=BATCH_SIZE)\n",
    "\n",
    "# python iterators object that yields not augmented samples \n",
    "iterator_train = generator.flow(x_train, y_train, batch_size=BATCH_SIZE)\n",
    "iterator_valid = generator.flow(x_val, y_val, batch_size=BATCH_SIZE)\n",
    "iterator_test = generator.flow(x_test, y_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "steps_per_epoch_train = int(ceil(iterator_train.n/BATCH_SIZE))\n",
    "steps_per_epoch_val = int(ceil(iterator_valid.n/BATCH_SIZE))\n",
    "steps_per_epoch_test = int(ceil(iterator_test.n/BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : (128, 3, 32, 32) | float32\n",
      "y : (128, 10) | float32\n"
     ]
    }
   ],
   "source": [
    "# test iterator with data augmentation\n",
    "x, y = iterator_train_aug.next()\n",
    "\n",
    "print(\"x : {} | {}\".format(x.shape, x.dtype))\n",
    "print(\"y : {} | {}\".format(y.shape, y.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resnet import ResNet56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/zenith/miniconda3/envs/dl-1.10-gpu/lib/python3.5/site-packages/tensorflow/python/keras/initializers.py:104: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`normal` is a deprecated alias for `truncated_normal`\n"
     ]
    }
   ],
   "source": [
    "shape = [3, 32, 32] if keras.backend.image_data_format()=='channels_first' else [32, 32, 3]\n",
    "model = ResNet56(input_shape=shape, classes=10, p=0.95, activation='relu').build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 3, 32, 32)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 16, 30, 30)   432         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 16, 30, 30)   64          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16, 30, 30)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 30, 30)   2304        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 16, 30, 30)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 16, 30, 30)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 16, 30, 30)   2304        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 16, 30, 30)   256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 16, 30, 30)   0           conv2d_3[0][0]                   \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16, 30, 30)   64          add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 16, 30, 30)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 16, 30, 30)   2304        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 16, 30, 30)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16, 30, 30)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 16, 30, 30)   2304        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 16, 30, 30)   0           conv2d_5[0][0]                   \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 16, 30, 30)   64          add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 16, 30, 30)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 30, 30)   2304        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 16, 30, 30)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 16, 30, 30)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 30, 30)   2304        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 16, 30, 30)   0           conv2d_7[0][0]                   \n",
      "                                                                 add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 30, 30)   64          add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 16, 30, 30)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 30, 30)   2304        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 30, 30)   64          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 30, 30)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 30, 30)   2304        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 16, 30, 30)   0           conv2d_9[0][0]                   \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 30, 30)   64          add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 30, 30)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 30, 30)   2304        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 30, 30)   64          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 30, 30)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 30, 30)   2304        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 30, 30)   0           conv2d_11[0][0]                  \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 30, 30)   64          add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 30, 30)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 30, 30)   2304        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 30, 30)   64          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 30, 30)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 30, 30)   2304        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 30, 30)   0           conv2d_13[0][0]                  \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 30, 30)   64          add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 30, 30)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 30, 30)   2304        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 30, 30)   64          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 30, 30)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 30, 30)   2304        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 30, 30)   0           conv2d_15[0][0]                  \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 30, 30)   64          add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 30, 30)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 30, 30)   2304        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 30, 30)   64          conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 30, 30)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 30, 30)   2304        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 16, 30, 30)   0           conv2d_17[0][0]                  \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 30, 30)   64          add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 30, 30)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 30, 30)   2304        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 30, 30)   64          conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 30, 30)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 16, 30, 30)   2304        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 16, 30, 30)   0           conv2d_19[0][0]                  \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 16, 30, 30)   64          add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16, 30, 30)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 16, 32, 32)   0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 32, 15, 15)   4608        zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 32, 15, 15)   128         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 32, 15, 15)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 32, 15, 15)   9216        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 32, 15, 15)   512         add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 32, 15, 15)   0           conv2d_22[0][0]                  \n",
      "                                                                 conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 32, 15, 15)   128         add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 32, 15, 15)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 32, 15, 15)   9216        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 32, 15, 15)   128         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 32, 15, 15)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 32, 15, 15)   9216        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 32, 15, 15)   0           conv2d_24[0][0]                  \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 32, 15, 15)   128         add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 32, 15, 15)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 32, 15, 15)   9216        activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 32, 15, 15)   128         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 32, 15, 15)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 32, 15, 15)   9216        activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 32, 15, 15)   0           conv2d_26[0][0]                  \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 32, 15, 15)   128         add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 32, 15, 15)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 32, 15, 15)   9216        activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 32, 15, 15)   128         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 32, 15, 15)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 32, 15, 15)   9216        activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 32, 15, 15)   0           conv2d_28[0][0]                  \n",
      "                                                                 add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 32, 15, 15)   128         add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 32, 15, 15)   0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 32, 15, 15)   9216        activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 32, 15, 15)   128         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 32, 15, 15)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 32, 15, 15)   9216        activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 32, 15, 15)   0           conv2d_30[0][0]                  \n",
      "                                                                 add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 32, 15, 15)   128         add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 32, 15, 15)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 32, 15, 15)   9216        activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 32, 15, 15)   128         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 32, 15, 15)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 32, 15, 15)   9216        activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 32, 15, 15)   0           conv2d_32[0][0]                  \n",
      "                                                                 add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 32, 15, 15)   128         add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 32, 15, 15)   0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 32, 15, 15)   9216        activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 32, 15, 15)   128         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 32, 15, 15)   0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 32, 15, 15)   9216        activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 32, 15, 15)   0           conv2d_34[0][0]                  \n",
      "                                                                 add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 32, 15, 15)   128         add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 32, 15, 15)   0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 32, 15, 15)   9216        activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 32, 15, 15)   128         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 32, 15, 15)   0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 32, 15, 15)   9216        activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 32, 15, 15)   0           conv2d_36[0][0]                  \n",
      "                                                                 add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 32, 15, 15)   128         add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 32, 15, 15)   0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 32, 15, 15)   9216        activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 32, 15, 15)   128         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 32, 15, 15)   0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 32, 15, 15)   9216        activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 32, 15, 15)   0           conv2d_38[0][0]                  \n",
      "                                                                 add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 32, 15, 15)   128         add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 32, 15, 15)   0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 32, 17, 17)   0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 64, 8, 8)     18432       zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 64, 8, 8)     256         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 64, 8, 8)     0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 64, 8, 8)     36864       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 64, 8, 8)     2048        add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 64, 8, 8)     0           conv2d_41[0][0]                  \n",
      "                                                                 conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 64, 8, 8)     256         add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 64, 8, 8)     0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 64, 8, 8)     36864       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 64, 8, 8)     256         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 64, 8, 8)     0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 64, 8, 8)     36864       activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 64, 8, 8)     0           conv2d_43[0][0]                  \n",
      "                                                                 add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 64, 8, 8)     256         add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 64, 8, 8)     0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 64, 8, 8)     36864       activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 64, 8, 8)     256         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 64, 8, 8)     0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 64, 8, 8)     36864       activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 64, 8, 8)     0           conv2d_45[0][0]                  \n",
      "                                                                 add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 64, 8, 8)     256         add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 64, 8, 8)     0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 64, 8, 8)     36864       activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 64, 8, 8)     256         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 64, 8, 8)     0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 64, 8, 8)     36864       activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 64, 8, 8)     0           conv2d_47[0][0]                  \n",
      "                                                                 add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 64, 8, 8)     256         add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 64, 8, 8)     0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 64, 8, 8)     36864       activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 64, 8, 8)     256         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 64, 8, 8)     0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 64, 8, 8)     36864       activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 64, 8, 8)     0           conv2d_49[0][0]                  \n",
      "                                                                 add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 64, 8, 8)     256         add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 64, 8, 8)     0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 64, 8, 8)     36864       activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 64, 8, 8)     256         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 64, 8, 8)     0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 64, 8, 8)     36864       activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 64, 8, 8)     0           conv2d_51[0][0]                  \n",
      "                                                                 add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 64, 8, 8)     256         add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 64, 8, 8)     0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 64, 8, 8)     36864       activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 64, 8, 8)     256         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 64, 8, 8)     0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 64, 8, 8)     36864       activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 64, 8, 8)     0           conv2d_53[0][0]                  \n",
      "                                                                 add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 64, 8, 8)     256         add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 64, 8, 8)     0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 64, 8, 8)     36864       activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 64, 8, 8)     256         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 64, 8, 8)     0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 64, 8, 8)     36864       activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 64, 8, 8)     0           conv2d_55[0][0]                  \n",
      "                                                                 add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 64, 8, 8)     256         add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 64, 8, 8)     0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 64, 8, 8)     36864       activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 64, 8, 8)     256         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 64, 8, 8)     0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 64, 8, 8)     36864       activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 64, 8, 8)     0           conv2d_57[0][0]                  \n",
      "                                                                 add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 64, 8, 8)     256         add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 64, 8, 8)     0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 64)           0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10)           650         avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 859,898\n",
      "Trainable params: 855,834\n",
      "Non-trainable params: 4,064\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=INIT_LR, momentum=0.9)\n",
    "loss = 'categorical_crossentropy'\n",
    "metrics = ['acc', ]\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_weights('random_weights.h5')\n",
    "model.load_weights('random_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "\n",
    "model_ckpt = keras.callbacks.ModelCheckpoint(\"model_ckpt_dropactivation_best_relu.h5\",\n",
    "                                             monitor='val_acc', verbose=1, save_best_only=True, \n",
    "                                             save_weights_only=True)\n",
    "callbacks.append(model_ckpt)\n",
    "\n",
    "def schedule(epoch):\n",
    "    if epoch < 91:\n",
    "        return INIT_LR\n",
    "    if epoch < 136:\n",
    "        return 0.1*INIT_LR\n",
    "    if epoch < 182:\n",
    "        return 0.01*INIT_LR\n",
    "    else:\n",
    "        return 0.001*INIT_LR\n",
    "    \n",
    "lr_schedule = keras.callbacks.LearningRateScheduler(schedule, verbose=1)\n",
    "callbacks.append(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 1/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 2.3755 - acc: 0.4072\n",
      "Epoch 00001: val_acc improved from -inf to 0.21160, saving model to model_ckpt_dropactivation_best_relu.h5\n",
      "313/313 [==============================] - 40s 128ms/step - loss: 2.3740 - acc: 0.4077 - val_loss: 4.3827 - val_acc: 0.2116\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 2/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.7954 - acc: 0.5854\n",
      "Epoch 00002: val_acc improved from 0.21160 to 0.48150, saving model to model_ckpt_dropactivation_best_relu.h5\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 1.7948 - acc: 0.5853 - val_loss: 2.1297 - val_acc: 0.4815\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 3/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.5310 - acc: 0.6466\n",
      "Epoch 00003: val_acc improved from 0.48150 to 0.51050, saving model to model_ckpt_dropactivation_best_relu.h5\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 1.5304 - acc: 0.6468 - val_loss: 1.9456 - val_acc: 0.5105\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 4/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.3370 - acc: 0.6917\n",
      "Epoch 00004: val_acc improved from 0.51050 to 0.53410, saving model to model_ckpt_dropactivation_best_relu.h5\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 1.3371 - acc: 0.6917 - val_loss: 1.7854 - val_acc: 0.5341\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 5/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.1948 - acc: 0.7253\n",
      "Epoch 00005: val_acc improved from 0.53410 to 0.59110, saving model to model_ckpt_dropactivation_best_relu.h5\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 1.1944 - acc: 0.7253 - val_loss: 1.6537 - val_acc: 0.5911\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 6/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.0800 - acc: 0.7532\n",
      "Epoch 00006: val_acc improved from 0.59110 to 0.64980, saving model to model_ckpt_dropactivation_best_relu.h5\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 1.0796 - acc: 0.7534 - val_loss: 1.3910 - val_acc: 0.6498\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 7/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9921 - acc: 0.7735\n",
      "Epoch 00007: val_acc did not improve from 0.64980\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.9918 - acc: 0.7736 - val_loss: 2.1420 - val_acc: 0.4668\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 8/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9321 - acc: 0.7895\n",
      "Epoch 00008: val_acc improved from 0.64980 to 0.67550, saving model to model_ckpt_dropactivation_best_relu.h5\n",
      "313/313 [==============================] - 23s 75ms/step - loss: 0.9320 - acc: 0.7895 - val_loss: 1.2576 - val_acc: 0.6755\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 9/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.8756 - acc: 0.7989\n",
      "Epoch 00009: val_acc improved from 0.67550 to 0.73450, saving model to model_ckpt_dropactivation_best_relu.h5\n",
      "313/313 [==============================] - 24s 77ms/step - loss: 0.8750 - acc: 0.7991 - val_loss: 1.1164 - val_acc: 0.7345\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 10/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.8446 - acc: 0.8065\n",
      "Epoch 00010: val_acc did not improve from 0.73450\n",
      "313/313 [==============================] - 24s 77ms/step - loss: 0.8442 - acc: 0.8067 - val_loss: 1.4365 - val_acc: 0.6078\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 11/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.8203 - acc: 0.8116\n",
      "Epoch 00011: val_acc did not improve from 0.73450\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.8200 - acc: 0.8117 - val_loss: 1.0714 - val_acc: 0.7297\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 12/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7864 - acc: 0.8213\n",
      "Epoch 00012: val_acc did not improve from 0.73450\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.7865 - acc: 0.8213 - val_loss: 1.4272 - val_acc: 0.6514\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 13/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7702 - acc: 0.8246\n",
      "Epoch 00013: val_acc improved from 0.73450 to 0.77790, saving model to model_ckpt_dropactivation_best_relu.h5\n",
      "313/313 [==============================] - 24s 77ms/step - loss: 0.7700 - acc: 0.8247 - val_loss: 0.9328 - val_acc: 0.7779\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 14/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7545 - acc: 0.8300\n",
      "Epoch 00014: val_acc did not improve from 0.77790\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.7543 - acc: 0.8300 - val_loss: 1.6141 - val_acc: 0.5900\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 15/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7407 - acc: 0.8343\n",
      "Epoch 00015: val_acc did not improve from 0.77790\n",
      "313/313 [==============================] - 23s 74ms/step - loss: 0.7411 - acc: 0.8342 - val_loss: 1.1441 - val_acc: 0.7035\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 16/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7294 - acc: 0.8391\n",
      "Epoch 00016: val_acc did not improve from 0.77790\n",
      "313/313 [==============================] - 23s 74ms/step - loss: 0.7293 - acc: 0.8392 - val_loss: 1.9046 - val_acc: 0.5497\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 17/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7259 - acc: 0.8396\n",
      "Epoch 00017: val_acc did not improve from 0.77790\n",
      "313/313 [==============================] - 23s 74ms/step - loss: 0.7264 - acc: 0.8395 - val_loss: 1.2240 - val_acc: 0.6633\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 18/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7152 - acc: 0.8441\n",
      "Epoch 00018: val_acc did not improve from 0.77790\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.7152 - acc: 0.8442 - val_loss: 1.4750 - val_acc: 0.6511\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 19/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7104 - acc: 0.8471\n",
      "Epoch 00019: val_acc did not improve from 0.77790\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.7103 - acc: 0.8473 - val_loss: 1.6954 - val_acc: 0.6248\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 20/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7001 - acc: 0.8510\n",
      "Epoch 00020: val_acc did not improve from 0.77790\n",
      "313/313 [==============================] - 23s 75ms/step - loss: 0.6998 - acc: 0.8512 - val_loss: 0.9787 - val_acc: 0.7633\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 21/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6996 - acc: 0.8518\n",
      "Epoch 00021: val_acc did not improve from 0.77790\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.6997 - acc: 0.8517 - val_loss: 1.2205 - val_acc: 0.6835\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 22/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6873 - acc: 0.8557\n",
      "Epoch 00022: val_acc did not improve from 0.77790\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.6871 - acc: 0.8558 - val_loss: 1.2841 - val_acc: 0.6961\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 23/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6948 - acc: 0.8536\n",
      "Epoch 00023: val_acc did not improve from 0.77790\n",
      "313/313 [==============================] - 24s 77ms/step - loss: 0.6944 - acc: 0.8538 - val_loss: 1.2873 - val_acc: 0.6966\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 24/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/313 [============================>.] - ETA: 0s - loss: 0.6750 - acc: 0.8609\n",
      "Epoch 00024: val_acc did not improve from 0.77790\n",
      "313/313 [==============================] - 24s 77ms/step - loss: 0.6752 - acc: 0.8609 - val_loss: 1.1307 - val_acc: 0.7212\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 25/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6809 - acc: 0.8583\n",
      "Epoch 00025: val_acc did not improve from 0.77790\n",
      "313/313 [==============================] - 24s 77ms/step - loss: 0.6809 - acc: 0.8584 - val_loss: 0.9758 - val_acc: 0.7572\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 26/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6730 - acc: 0.8623\n",
      "Epoch 00026: val_acc did not improve from 0.77790\n",
      "313/313 [==============================] - 24s 77ms/step - loss: 0.6733 - acc: 0.8622 - val_loss: 0.9763 - val_acc: 0.7726\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 27/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6675 - acc: 0.8649\n",
      "Epoch 00027: val_acc did not improve from 0.77790\n",
      "313/313 [==============================] - 24s 77ms/step - loss: 0.6678 - acc: 0.8648 - val_loss: 1.7032 - val_acc: 0.5970\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 28/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6719 - acc: 0.8625\n",
      "Epoch 00028: val_acc did not improve from 0.77790\n",
      "313/313 [==============================] - 24s 77ms/step - loss: 0.6720 - acc: 0.8625 - val_loss: 1.1935 - val_acc: 0.7068\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 29/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6650 - acc: 0.8671\n",
      "Epoch 00029: val_acc did not improve from 0.77790\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.6649 - acc: 0.8671 - val_loss: 1.1013 - val_acc: 0.7272\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 30/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6582 - acc: 0.8690\n",
      "Epoch 00030: val_acc improved from 0.77790 to 0.79620, saving model to model_ckpt_dropactivation_best_relu.h5\n",
      "313/313 [==============================] - 25s 79ms/step - loss: 0.6581 - acc: 0.8690 - val_loss: 0.9037 - val_acc: 0.7962\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 31/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6608 - acc: 0.8690\n",
      "Epoch 00031: val_acc did not improve from 0.79620\n",
      "313/313 [==============================] - 24s 77ms/step - loss: 0.6609 - acc: 0.8690 - val_loss: 0.9780 - val_acc: 0.7828\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 32/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6564 - acc: 0.8701\n",
      "Epoch 00032: val_acc did not improve from 0.79620\n",
      "313/313 [==============================] - 24s 78ms/step - loss: 0.6560 - acc: 0.8702 - val_loss: 1.3643 - val_acc: 0.6918\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 33/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6620 - acc: 0.8697\n",
      "Epoch 00033: val_acc did not improve from 0.79620\n",
      "313/313 [==============================] - 24s 78ms/step - loss: 0.6622 - acc: 0.8697 - val_loss: 1.9302 - val_acc: 0.6052\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 34/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6515 - acc: 0.8739\n",
      "Epoch 00034: val_acc did not improve from 0.79620\n",
      "313/313 [==============================] - 24s 78ms/step - loss: 0.6522 - acc: 0.8738 - val_loss: 1.1312 - val_acc: 0.7385\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 35/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6616 - acc: 0.8718\n",
      "Epoch 00035: val_acc improved from 0.79620 to 0.79670, saving model to model_ckpt_dropactivation_best_relu.h5\n",
      "313/313 [==============================] - 24s 78ms/step - loss: 0.6617 - acc: 0.8718 - val_loss: 0.9182 - val_acc: 0.7967\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 36/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6580 - acc: 0.8734\n",
      "Epoch 00036: val_acc did not improve from 0.79670\n",
      "313/313 [==============================] - 24s 78ms/step - loss: 0.6578 - acc: 0.8734 - val_loss: 1.2266 - val_acc: 0.7161\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 37/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6496 - acc: 0.8754\n",
      "Epoch 00037: val_acc did not improve from 0.79670\n",
      "313/313 [==============================] - 24s 78ms/step - loss: 0.6496 - acc: 0.8753 - val_loss: 1.1359 - val_acc: 0.7425\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 38/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6504 - acc: 0.8773\n",
      "Epoch 00038: val_acc did not improve from 0.79670\n",
      "313/313 [==============================] - 24s 78ms/step - loss: 0.6503 - acc: 0.8773 - val_loss: 1.1970 - val_acc: 0.7424\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 39/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6480 - acc: 0.8778\n",
      "Epoch 00039: val_acc did not improve from 0.79670\n",
      "313/313 [==============================] - 24s 78ms/step - loss: 0.6478 - acc: 0.8779 - val_loss: 1.1625 - val_acc: 0.7234\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 40/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6504 - acc: 0.8780\n",
      "Epoch 00040: val_acc did not improve from 0.79670\n",
      "313/313 [==============================] - 24s 78ms/step - loss: 0.6505 - acc: 0.8780 - val_loss: 0.9348 - val_acc: 0.7948\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 41/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6477 - acc: 0.8794\n",
      "Epoch 00041: val_acc improved from 0.79670 to 0.80350, saving model to model_ckpt_dropactivation_best_relu.h5\n",
      "313/313 [==============================] - 24s 78ms/step - loss: 0.6477 - acc: 0.8793 - val_loss: 0.8845 - val_acc: 0.8035\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 42/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6471 - acc: 0.8778\n",
      "Epoch 00042: val_acc did not improve from 0.80350\n",
      "313/313 [==============================] - 24s 78ms/step - loss: 0.6474 - acc: 0.8776 - val_loss: 2.3586 - val_acc: 0.5623\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 43/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6457 - acc: 0.8807\n",
      "Epoch 00043: val_acc did not improve from 0.80350\n",
      "313/313 [==============================] - 24s 77ms/step - loss: 0.6454 - acc: 0.8807 - val_loss: 0.9827 - val_acc: 0.7736\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 44/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6463 - acc: 0.8785\n",
      "Epoch 00044: val_acc did not improve from 0.80350\n",
      "313/313 [==============================] - 24s 77ms/step - loss: 0.6462 - acc: 0.8785 - val_loss: 0.9168 - val_acc: 0.7990\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 45/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6410 - acc: 0.8824\n",
      "Epoch 00045: val_acc did not improve from 0.80350\n",
      "313/313 [==============================] - 24s 77ms/step - loss: 0.6410 - acc: 0.8825 - val_loss: 0.9691 - val_acc: 0.7689\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 46/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6404 - acc: 0.8832\n",
      "Epoch 00046: val_acc did not improve from 0.80350\n",
      "313/313 [==============================] - 24s 78ms/step - loss: 0.6404 - acc: 0.8832 - val_loss: 1.9723 - val_acc: 0.6185\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 47/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6383 - acc: 0.8835\n",
      "Epoch 00047: val_acc did not improve from 0.80350\n",
      "313/313 [==============================] - 26s 82ms/step - loss: 0.6384 - acc: 0.8835 - val_loss: 1.0636 - val_acc: 0.7566\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 48/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6436 - acc: 0.8816\n",
      "Epoch 00048: val_acc did not improve from 0.80350\n",
      "313/313 [==============================] - 25s 81ms/step - loss: 0.6433 - acc: 0.8816 - val_loss: 0.9991 - val_acc: 0.7819\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 49/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6378 - acc: 0.8827\n",
      "Epoch 00049: val_acc did not improve from 0.80350\n",
      "313/313 [==============================] - 25s 81ms/step - loss: 0.6380 - acc: 0.8826 - val_loss: 1.0861 - val_acc: 0.7571\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 50/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6376 - acc: 0.8840\n",
      "Epoch 00050: val_acc did not improve from 0.80350\n",
      "313/313 [==============================] - 25s 81ms/step - loss: 0.6377 - acc: 0.8839 - val_loss: 1.0303 - val_acc: 0.7532\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 51/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6462 - acc: 0.8800\n",
      "Epoch 00051: val_acc did not improve from 0.80350\n",
      "313/313 [==============================] - 25s 80ms/step - loss: 0.6464 - acc: 0.8800 - val_loss: 1.5010 - val_acc: 0.6742\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 52/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6353 - acc: 0.8837\n",
      "Epoch 00052: val_acc did not improve from 0.80350\n",
      "313/313 [==============================] - 25s 80ms/step - loss: 0.6352 - acc: 0.8838 - val_loss: 1.2476 - val_acc: 0.7104\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 53/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6400 - acc: 0.8838\n",
      "Epoch 00053: val_acc did not improve from 0.80350\n",
      "313/313 [==============================] - 25s 80ms/step - loss: 0.6403 - acc: 0.8837 - val_loss: 1.3535 - val_acc: 0.6895\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 54/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6378 - acc: 0.8865\n",
      "Epoch 00054: val_acc did not improve from 0.80350\n",
      "313/313 [==============================] - 25s 81ms/step - loss: 0.6379 - acc: 0.8865 - val_loss: 1.0491 - val_acc: 0.7660\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 55/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6315 - acc: 0.8882\n",
      "Epoch 00055: val_acc did not improve from 0.80350\n",
      "313/313 [==============================] - 25s 79ms/step - loss: 0.6322 - acc: 0.8880 - val_loss: 1.1593 - val_acc: 0.7171\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 56/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6334 - acc: 0.8876\n",
      "Epoch 00056: val_acc did not improve from 0.80350\n",
      "313/313 [==============================] - 24s 78ms/step - loss: 0.6339 - acc: 0.8875 - val_loss: 1.0987 - val_acc: 0.7630\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 57/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6380 - acc: 0.8864\n",
      "Epoch 00057: val_acc did not improve from 0.80350\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.6379 - acc: 0.8863 - val_loss: 0.9224 - val_acc: 0.7998\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 58/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6341 - acc: 0.8860\n",
      "Epoch 00058: val_acc improved from 0.80350 to 0.81450, saving model to model_ckpt_dropactivation_best_relu.h5\n",
      "313/313 [==============================] - 24s 77ms/step - loss: 0.6339 - acc: 0.8860 - val_loss: 0.8753 - val_acc: 0.8145\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 59/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6313 - acc: 0.8884\n",
      "Epoch 00059: val_acc did not improve from 0.81450\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.6309 - acc: 0.8886 - val_loss: 0.9550 - val_acc: 0.7945\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 60/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6340 - acc: 0.8901\n",
      "Epoch 00060: val_acc did not improve from 0.81450\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.6342 - acc: 0.8901 - val_loss: 1.3507 - val_acc: 0.6958\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 61/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6274 - acc: 0.8903\n",
      "Epoch 00061: val_acc did not improve from 0.81450\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.6275 - acc: 0.8903 - val_loss: 1.3697 - val_acc: 0.6897\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 62/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6296 - acc: 0.8910\n",
      "Epoch 00062: val_acc did not improve from 0.81450\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.6294 - acc: 0.8911 - val_loss: 1.6986 - val_acc: 0.6620\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 63/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6293 - acc: 0.8880\n",
      "Epoch 00063: val_acc did not improve from 0.81450\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.6291 - acc: 0.8880 - val_loss: 1.7920 - val_acc: 0.6462\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 64/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6270 - acc: 0.8904\n",
      "Epoch 00064: val_acc did not improve from 0.81450\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.6271 - acc: 0.8905 - val_loss: 1.1118 - val_acc: 0.7690\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 65/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6281 - acc: 0.8891\n",
      "Epoch 00065: val_acc did not improve from 0.81450\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.6283 - acc: 0.8891 - val_loss: 1.2632 - val_acc: 0.7025\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 66/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6324 - acc: 0.8907\n",
      "Epoch 00066: val_acc did not improve from 0.81450\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.6324 - acc: 0.8907 - val_loss: 1.4704 - val_acc: 0.6960\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 67/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6297 - acc: 0.8917\n",
      "Epoch 00067: val_acc did not improve from 0.81450\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.6298 - acc: 0.8917 - val_loss: 1.2974 - val_acc: 0.7273\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 68/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6185 - acc: 0.8936\n",
      "Epoch 00068: val_acc did not improve from 0.81450\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.6184 - acc: 0.8937 - val_loss: 1.1828 - val_acc: 0.7391\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 69/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6323 - acc: 0.8898\n",
      "Epoch 00069: val_acc improved from 0.81450 to 0.83160, saving model to model_ckpt_dropactivation_best_relu.h5\n",
      "313/313 [==============================] - 24s 77ms/step - loss: 0.6322 - acc: 0.8899 - val_loss: 0.8085 - val_acc: 0.8316\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 70/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6244 - acc: 0.8932\n",
      "Epoch 00070: val_acc did not improve from 0.83160\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.6242 - acc: 0.8932 - val_loss: 0.9982 - val_acc: 0.7901\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 71/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6208 - acc: 0.8953\n",
      "Epoch 00071: val_acc did not improve from 0.83160\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.6207 - acc: 0.8953 - val_loss: 0.8269 - val_acc: 0.8306\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 72/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6260 - acc: 0.8914\n",
      "Epoch 00072: val_acc did not improve from 0.83160\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.6259 - acc: 0.8914 - val_loss: 0.9313 - val_acc: 0.7969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 73/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6258 - acc: 0.8919\n",
      "Epoch 00073: val_acc did not improve from 0.83160\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.6258 - acc: 0.8919 - val_loss: 1.0767 - val_acc: 0.7502\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 74/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6203 - acc: 0.8947\n",
      "Epoch 00074: val_acc did not improve from 0.83160\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.6207 - acc: 0.8946 - val_loss: 0.9669 - val_acc: 0.7984\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 75/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6288 - acc: 0.8919\n",
      "Epoch 00075: val_acc did not improve from 0.83160\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.6290 - acc: 0.8917 - val_loss: 1.3191 - val_acc: 0.7204\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 76/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6234 - acc: 0.8941\n",
      "Epoch 00076: val_acc did not improve from 0.83160\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.6235 - acc: 0.8941 - val_loss: 0.9787 - val_acc: 0.7910\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 77/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6233 - acc: 0.8944\n",
      "Epoch 00077: val_acc did not improve from 0.83160\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.6230 - acc: 0.8946 - val_loss: 0.8511 - val_acc: 0.8260\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 78/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6236 - acc: 0.8946\n",
      "Epoch 00078: val_acc did not improve from 0.83160\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.6235 - acc: 0.8946 - val_loss: 1.2806 - val_acc: 0.7518\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 79/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6220 - acc: 0.8937\n",
      "Epoch 00079: val_acc did not improve from 0.83160\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.6219 - acc: 0.8938 - val_loss: 0.9564 - val_acc: 0.8039\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 80/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6218 - acc: 0.8947\n",
      "Epoch 00080: val_acc did not improve from 0.83160\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.6221 - acc: 0.8945 - val_loss: 0.9789 - val_acc: 0.7798\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 81/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6230 - acc: 0.8953\n",
      "Epoch 00081: val_acc did not improve from 0.83160\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.6231 - acc: 0.8952 - val_loss: 0.9850 - val_acc: 0.7854\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 82/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6255 - acc: 0.8939\n",
      "Epoch 00082: val_acc did not improve from 0.83160\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.6254 - acc: 0.8939 - val_loss: 1.4599 - val_acc: 0.7045\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 83/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6231 - acc: 0.8949\n",
      "Epoch 00083: val_acc did not improve from 0.83160\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.6234 - acc: 0.8949 - val_loss: 1.0259 - val_acc: 0.7972\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 84/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6235 - acc: 0.8978\n",
      "Epoch 00084: val_acc did not improve from 0.83160\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.6236 - acc: 0.8979 - val_loss: 1.0559 - val_acc: 0.7707\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 85/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6204 - acc: 0.8963\n",
      "Epoch 00085: val_acc did not improve from 0.83160\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.6204 - acc: 0.8962 - val_loss: 0.8622 - val_acc: 0.8293\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 86/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6164 - acc: 0.8963\n",
      "Epoch 00086: val_acc did not improve from 0.83160\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.6166 - acc: 0.8961 - val_loss: 0.9364 - val_acc: 0.8129\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 87/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6194 - acc: 0.8965\n",
      "Epoch 00087: val_acc did not improve from 0.83160\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.6196 - acc: 0.8965 - val_loss: 1.2068 - val_acc: 0.7285\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 88/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6249 - acc: 0.8952\n",
      "Epoch 00088: val_acc did not improve from 0.83160\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.6250 - acc: 0.8951 - val_loss: 1.0898 - val_acc: 0.7604\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 89/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6224 - acc: 0.8947\n",
      "Epoch 00089: val_acc did not improve from 0.83160\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.6225 - acc: 0.8946 - val_loss: 0.9414 - val_acc: 0.7940\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 90/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6175 - acc: 0.8984\n",
      "Epoch 00090: val_acc did not improve from 0.83160\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.6173 - acc: 0.8984 - val_loss: 1.0343 - val_acc: 0.7697\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 91/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6196 - acc: 0.8970\n",
      "Epoch 00091: val_acc did not improve from 0.83160\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.6194 - acc: 0.8970 - val_loss: 1.0999 - val_acc: 0.7473\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 92/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.5239 - acc: 0.9316\n",
      "Epoch 00092: val_acc improved from 0.83160 to 0.90750, saving model to model_ckpt_dropactivation_best_relu.h5\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.5239 - acc: 0.9315 - val_loss: 0.5937 - val_acc: 0.9075\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 93/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4480 - acc: 0.9543\n",
      "Epoch 00093: val_acc improved from 0.90750 to 0.91050, saving model to model_ckpt_dropactivation_best_relu.h5\n",
      "313/313 [==============================] - 24s 77ms/step - loss: 0.4478 - acc: 0.9543 - val_loss: 0.5855 - val_acc: 0.9105\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 94/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4235 - acc: 0.9610\n",
      "Epoch 00094: val_acc improved from 0.91050 to 0.91130, saving model to model_ckpt_dropactivation_best_relu.h5\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.4233 - acc: 0.9610 - val_loss: 0.5791 - val_acc: 0.9113\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 95/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4018 - acc: 0.9666\n",
      "Epoch 00095: val_acc improved from 0.91130 to 0.91490, saving model to model_ckpt_dropactivation_best_relu.h5\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.4016 - acc: 0.9667 - val_loss: 0.5724 - val_acc: 0.9149\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 96/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3886 - acc: 0.9686\n",
      "Epoch 00096: val_acc improved from 0.91490 to 0.91800, saving model to model_ckpt_dropactivation_best_relu.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 24s 76ms/step - loss: 0.3887 - acc: 0.9685 - val_loss: 0.5569 - val_acc: 0.9180\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 97/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3718 - acc: 0.9722\n",
      "Epoch 00097: val_acc did not improve from 0.91800\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.3719 - acc: 0.9721 - val_loss: 0.5602 - val_acc: 0.9166\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 98/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3580 - acc: 0.9746\n",
      "Epoch 00098: val_acc did not improve from 0.91800\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.3579 - acc: 0.9746 - val_loss: 0.5545 - val_acc: 0.9169\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 99/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3477 - acc: 0.9749\n",
      "Epoch 00099: val_acc improved from 0.91800 to 0.91880, saving model to model_ckpt_dropactivation_best_relu.h5\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.3477 - acc: 0.9749 - val_loss: 0.5478 - val_acc: 0.9188\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 100/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3370 - acc: 0.9757\n",
      "Epoch 00100: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.3369 - acc: 0.9758 - val_loss: 0.5695 - val_acc: 0.9134\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 101/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3274 - acc: 0.9784\n",
      "Epoch 00101: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.3274 - acc: 0.9784 - val_loss: 0.5500 - val_acc: 0.9159\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 102/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3183 - acc: 0.9793\n",
      "Epoch 00102: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.3183 - acc: 0.9792 - val_loss: 0.5613 - val_acc: 0.9143\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 103/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3080 - acc: 0.9812\n",
      "Epoch 00103: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.3080 - acc: 0.9812 - val_loss: 0.5743 - val_acc: 0.9091\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 104/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3005 - acc: 0.9819\n",
      "Epoch 00104: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.3004 - acc: 0.9819 - val_loss: 0.5508 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 105/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2924 - acc: 0.9836\n",
      "Epoch 00105: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.2924 - acc: 0.9836 - val_loss: 0.5446 - val_acc: 0.9169\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 106/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2856 - acc: 0.9841\n",
      "Epoch 00106: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.2856 - acc: 0.9841 - val_loss: 0.5761 - val_acc: 0.9110\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 107/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2787 - acc: 0.9848\n",
      "Epoch 00107: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.2788 - acc: 0.9848 - val_loss: 0.5691 - val_acc: 0.9127\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 108/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2709 - acc: 0.9860\n",
      "Epoch 00108: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.2710 - acc: 0.9859 - val_loss: 0.5539 - val_acc: 0.9169\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 109/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2661 - acc: 0.9854\n",
      "Epoch 00109: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.2661 - acc: 0.9854 - val_loss: 0.5702 - val_acc: 0.9091\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 110/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2591 - acc: 0.9863\n",
      "Epoch 00110: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.2591 - acc: 0.9863 - val_loss: 0.5477 - val_acc: 0.9127\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 111/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2558 - acc: 0.9864\n",
      "Epoch 00111: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.2557 - acc: 0.9864 - val_loss: 0.5361 - val_acc: 0.9123\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 112/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2494 - acc: 0.9868\n",
      "Epoch 00112: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.2494 - acc: 0.9869 - val_loss: 0.5578 - val_acc: 0.9111\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 113/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2430 - acc: 0.9876\n",
      "Epoch 00113: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.2431 - acc: 0.9875 - val_loss: 0.5363 - val_acc: 0.9186\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 114/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2393 - acc: 0.9878\n",
      "Epoch 00114: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.2393 - acc: 0.9878 - val_loss: 0.5344 - val_acc: 0.9147\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 115/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2408 - acc: 0.9868\n",
      "Epoch 00115: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.2408 - acc: 0.9868 - val_loss: 0.5648 - val_acc: 0.9029\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 116/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2335 - acc: 0.9877\n",
      "Epoch 00116: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.2334 - acc: 0.9878 - val_loss: 0.5383 - val_acc: 0.9125\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 117/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2264 - acc: 0.9890\n",
      "Epoch 00117: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.2264 - acc: 0.9889 - val_loss: 0.5826 - val_acc: 0.9064\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 118/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2251 - acc: 0.9878\n",
      "Epoch 00118: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.2250 - acc: 0.9878 - val_loss: 0.5747 - val_acc: 0.9104\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 119/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2236 - acc: 0.9871\n",
      "Epoch 00119: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.2235 - acc: 0.9871 - val_loss: 0.6113 - val_acc: 0.8964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 120/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2206 - acc: 0.9875\n",
      "Epoch 00120: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.2207 - acc: 0.9875 - val_loss: 0.5840 - val_acc: 0.9061\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 121/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2158 - acc: 0.9869\n",
      "Epoch 00121: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.2160 - acc: 0.9868 - val_loss: 0.5536 - val_acc: 0.9100\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 122/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2136 - acc: 0.9878\n",
      "Epoch 00122: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.2135 - acc: 0.9878 - val_loss: 0.5681 - val_acc: 0.9054\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 123/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2115 - acc: 0.9879\n",
      "Epoch 00123: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.2115 - acc: 0.9879 - val_loss: 0.5506 - val_acc: 0.9047\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 124/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2077 - acc: 0.9876\n",
      "Epoch 00124: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.2080 - acc: 0.9876 - val_loss: 0.6069 - val_acc: 0.8965\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 125/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2052 - acc: 0.9880\n",
      "Epoch 00125: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.2053 - acc: 0.9880 - val_loss: 0.5615 - val_acc: 0.9041\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 126/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2011 - acc: 0.9883\n",
      "Epoch 00126: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.2013 - acc: 0.9882 - val_loss: 0.5346 - val_acc: 0.9081\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 127/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1952 - acc: 0.9898\n",
      "Epoch 00127: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1951 - acc: 0.9899 - val_loss: 0.5698 - val_acc: 0.9080\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 128/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1951 - acc: 0.9890\n",
      "Epoch 00128: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1951 - acc: 0.9890 - val_loss: 0.5934 - val_acc: 0.9013\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 129/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1979 - acc: 0.9867\n",
      "Epoch 00129: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1981 - acc: 0.9866 - val_loss: 0.5288 - val_acc: 0.9094\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 130/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1971 - acc: 0.9868\n",
      "Epoch 00130: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1970 - acc: 0.9868 - val_loss: 0.5608 - val_acc: 0.9001\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 131/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1939 - acc: 0.9870\n",
      "Epoch 00131: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1938 - acc: 0.9870 - val_loss: 0.5611 - val_acc: 0.9004\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 132/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1914 - acc: 0.9877\n",
      "Epoch 00132: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1916 - acc: 0.9876 - val_loss: 0.5352 - val_acc: 0.9080\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 133/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1901 - acc: 0.9870\n",
      "Epoch 00133: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.1900 - acc: 0.9870 - val_loss: 0.6420 - val_acc: 0.8830\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 134/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1868 - acc: 0.9874\n",
      "Epoch 00134: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1867 - acc: 0.9874 - val_loss: 0.5732 - val_acc: 0.8960\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 135/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1855 - acc: 0.9874\n",
      "Epoch 00135: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1856 - acc: 0.9874 - val_loss: 0.6045 - val_acc: 0.8931\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 136/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1868 - acc: 0.9864\n",
      "Epoch 00136: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1868 - acc: 0.9864 - val_loss: 0.5755 - val_acc: 0.8984\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 137/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1785 - acc: 0.9897\n",
      "Epoch 00137: val_acc did not improve from 0.91880\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.1784 - acc: 0.9897 - val_loss: 0.4788 - val_acc: 0.9183\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 138/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1655 - acc: 0.9937\n",
      "Epoch 00138: val_acc improved from 0.91880 to 0.92020, saving model to model_ckpt_dropactivation_best_relu.h5\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.1654 - acc: 0.9937 - val_loss: 0.4701 - val_acc: 0.9202\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 139/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1621 - acc: 0.9950\n",
      "Epoch 00139: val_acc improved from 0.92020 to 0.92250, saving model to model_ckpt_dropactivation_best_relu.h5\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.1621 - acc: 0.9950 - val_loss: 0.4675 - val_acc: 0.9225\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 140/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1589 - acc: 0.9961\n",
      "Epoch 00140: val_acc improved from 0.92250 to 0.92350, saving model to model_ckpt_dropactivation_best_relu.h5\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.1589 - acc: 0.9961 - val_loss: 0.4702 - val_acc: 0.9235\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 141/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1592 - acc: 0.9953\n",
      "Epoch 00141: val_acc did not improve from 0.92350\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1592 - acc: 0.9953 - val_loss: 0.4726 - val_acc: 0.9226\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 142/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1580 - acc: 0.9962\n",
      "Epoch 00142: val_acc did not improve from 0.92350\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1579 - acc: 0.9962 - val_loss: 0.4739 - val_acc: 0.9225\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 143/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/313 [============================>.] - ETA: 0s - loss: 0.1547 - acc: 0.9975\n",
      "Epoch 00143: val_acc improved from 0.92350 to 0.92370, saving model to model_ckpt_dropactivation_best_relu.h5\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.1547 - acc: 0.9975 - val_loss: 0.4728 - val_acc: 0.9237\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 144/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1543 - acc: 0.9972\n",
      "Epoch 00144: val_acc did not improve from 0.92370\n",
      "313/313 [==============================] - 23s 75ms/step - loss: 0.1544 - acc: 0.9972 - val_loss: 0.4757 - val_acc: 0.9233\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 145/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1532 - acc: 0.9975\n",
      "Epoch 00145: val_acc did not improve from 0.92370\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1532 - acc: 0.9975 - val_loss: 0.4728 - val_acc: 0.9235\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 146/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1526 - acc: 0.9976\n",
      "Epoch 00146: val_acc improved from 0.92370 to 0.92430, saving model to model_ckpt_dropactivation_best_relu.h5\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.1525 - acc: 0.9977 - val_loss: 0.4738 - val_acc: 0.9243\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 147/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1518 - acc: 0.9980\n",
      "Epoch 00147: val_acc did not improve from 0.92430\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1518 - acc: 0.9980 - val_loss: 0.4786 - val_acc: 0.9223\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 148/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1517 - acc: 0.9976\n",
      "Epoch 00148: val_acc did not improve from 0.92430\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1517 - acc: 0.9976 - val_loss: 0.4786 - val_acc: 0.9239\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 149/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1504 - acc: 0.9977\n",
      "Epoch 00149: val_acc did not improve from 0.92430\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1504 - acc: 0.9978 - val_loss: 0.4811 - val_acc: 0.9242\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 150/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1497 - acc: 0.9980\n",
      "Epoch 00150: val_acc did not improve from 0.92430\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1498 - acc: 0.9980 - val_loss: 0.4811 - val_acc: 0.9241\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 151/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1499 - acc: 0.9976\n",
      "Epoch 00151: val_acc did not improve from 0.92430\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1499 - acc: 0.9976 - val_loss: 0.4815 - val_acc: 0.9237\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 152/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1486 - acc: 0.9981\n",
      "Epoch 00152: val_acc did not improve from 0.92430\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1487 - acc: 0.9981 - val_loss: 0.4842 - val_acc: 0.9234\n",
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 153/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1481 - acc: 0.9982\n",
      "Epoch 00153: val_acc did not improve from 0.92430\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1481 - acc: 0.9983 - val_loss: 0.4844 - val_acc: 0.9230\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 154/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1478 - acc: 0.9981\n",
      "Epoch 00154: val_acc did not improve from 0.92430\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1477 - acc: 0.9981 - val_loss: 0.4844 - val_acc: 0.9237\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 155/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1470 - acc: 0.9982\n",
      "Epoch 00155: val_acc did not improve from 0.92430\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1470 - acc: 0.9983 - val_loss: 0.4845 - val_acc: 0.9228\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 156/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1472 - acc: 0.9982\n",
      "Epoch 00156: val_acc did not improve from 0.92430\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1471 - acc: 0.9982 - val_loss: 0.4855 - val_acc: 0.9237\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 157/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1462 - acc: 0.9986\n",
      "Epoch 00157: val_acc did not improve from 0.92430\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1462 - acc: 0.9986 - val_loss: 0.4857 - val_acc: 0.9239\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 158/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1451 - acc: 0.9985\n",
      "Epoch 00158: val_acc did not improve from 0.92430\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.1451 - acc: 0.9985 - val_loss: 0.4846 - val_acc: 0.9234\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 159/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1447 - acc: 0.9986\n",
      "Epoch 00159: val_acc did not improve from 0.92430\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1446 - acc: 0.9987 - val_loss: 0.4854 - val_acc: 0.9240\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 160/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1448 - acc: 0.9983\n",
      "Epoch 00160: val_acc did not improve from 0.92430\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1447 - acc: 0.9984 - val_loss: 0.4882 - val_acc: 0.9242\n",
      "\n",
      "Epoch 00161: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 161/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1439 - acc: 0.9985\n",
      "Epoch 00161: val_acc improved from 0.92430 to 0.92460, saving model to model_ckpt_dropactivation_best_relu.h5\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.1439 - acc: 0.9986 - val_loss: 0.4850 - val_acc: 0.9246\n",
      "\n",
      "Epoch 00162: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 162/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1434 - acc: 0.9986\n",
      "Epoch 00162: val_acc did not improve from 0.92460\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1434 - acc: 0.9986 - val_loss: 0.4873 - val_acc: 0.9235\n",
      "\n",
      "Epoch 00163: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 163/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1435 - acc: 0.9983\n",
      "Epoch 00163: val_acc improved from 0.92460 to 0.92470, saving model to model_ckpt_dropactivation_best_relu.h5\n",
      "313/313 [==============================] - 24s 76ms/step - loss: 0.1435 - acc: 0.9983 - val_loss: 0.4857 - val_acc: 0.9247\n",
      "\n",
      "Epoch 00164: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 164/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1420 - acc: 0.9990\n",
      "Epoch 00164: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1420 - acc: 0.9990 - val_loss: 0.4889 - val_acc: 0.9232\n",
      "\n",
      "Epoch 00165: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 165/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1422 - acc: 0.9986\n",
      "Epoch 00165: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1422 - acc: 0.9987 - val_loss: 0.4864 - val_acc: 0.9244\n",
      "\n",
      "Epoch 00166: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 166/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1419 - acc: 0.9987\n",
      "Epoch 00166: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1419 - acc: 0.9987 - val_loss: 0.4905 - val_acc: 0.9233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00167: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 167/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1415 - acc: 0.9986\n",
      "Epoch 00167: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1415 - acc: 0.9986 - val_loss: 0.4931 - val_acc: 0.9234\n",
      "\n",
      "Epoch 00168: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 168/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1405 - acc: 0.9990\n",
      "Epoch 00168: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1404 - acc: 0.9990 - val_loss: 0.4940 - val_acc: 0.9244\n",
      "\n",
      "Epoch 00169: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 169/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1410 - acc: 0.9987\n",
      "Epoch 00169: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1410 - acc: 0.9987 - val_loss: 0.4915 - val_acc: 0.9236\n",
      "\n",
      "Epoch 00170: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 170/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1398 - acc: 0.9990\n",
      "Epoch 00170: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1398 - acc: 0.9990 - val_loss: 0.4914 - val_acc: 0.9239\n",
      "\n",
      "Epoch 00171: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 171/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1398 - acc: 0.9987\n",
      "Epoch 00171: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1398 - acc: 0.9987 - val_loss: 0.4936 - val_acc: 0.9233\n",
      "\n",
      "Epoch 00172: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 172/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1392 - acc: 0.9988\n",
      "Epoch 00172: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1392 - acc: 0.9988 - val_loss: 0.4898 - val_acc: 0.9233\n",
      "\n",
      "Epoch 00173: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 173/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1390 - acc: 0.9987\n",
      "Epoch 00173: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1390 - acc: 0.9987 - val_loss: 0.4939 - val_acc: 0.9232\n",
      "\n",
      "Epoch 00174: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 174/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1389 - acc: 0.9988\n",
      "Epoch 00174: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1389 - acc: 0.9988 - val_loss: 0.4936 - val_acc: 0.9241\n",
      "\n",
      "Epoch 00175: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 175/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1382 - acc: 0.9989\n",
      "Epoch 00175: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1382 - acc: 0.9989 - val_loss: 0.4963 - val_acc: 0.9228\n",
      "\n",
      "Epoch 00176: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 176/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1380 - acc: 0.9988\n",
      "Epoch 00176: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1380 - acc: 0.9988 - val_loss: 0.4965 - val_acc: 0.9237\n",
      "\n",
      "Epoch 00177: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 177/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1375 - acc: 0.9988\n",
      "Epoch 00177: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1376 - acc: 0.9988 - val_loss: 0.4966 - val_acc: 0.9238\n",
      "\n",
      "Epoch 00178: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 178/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1371 - acc: 0.9989\n",
      "Epoch 00178: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1371 - acc: 0.9989 - val_loss: 0.4988 - val_acc: 0.9224\n",
      "\n",
      "Epoch 00179: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 179/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1372 - acc: 0.9986\n",
      "Epoch 00179: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1372 - acc: 0.9986 - val_loss: 0.4959 - val_acc: 0.9216\n",
      "\n",
      "Epoch 00180: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 180/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.9986\n",
      "Epoch 00180: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1370 - acc: 0.9986 - val_loss: 0.4986 - val_acc: 0.9223\n",
      "\n",
      "Epoch 00181: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 181/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1358 - acc: 0.9991\n",
      "Epoch 00181: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1358 - acc: 0.9991 - val_loss: 0.5010 - val_acc: 0.9236\n",
      "\n",
      "Epoch 00182: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 182/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1350 - acc: 0.9990\n",
      "Epoch 00182: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1350 - acc: 0.9990 - val_loss: 0.4995 - val_acc: 0.9235\n",
      "\n",
      "Epoch 00183: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 183/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1354 - acc: 0.9987\n",
      "Epoch 00183: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1354 - acc: 0.9987 - val_loss: 0.4981 - val_acc: 0.9234\n",
      "\n",
      "Epoch 00184: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 184/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.9987\n",
      "Epoch 00184: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1354 - acc: 0.9988 - val_loss: 0.4973 - val_acc: 0.9233\n",
      "\n",
      "Epoch 00185: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 185/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.9991\n",
      "Epoch 00185: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1349 - acc: 0.9991 - val_loss: 0.4971 - val_acc: 0.9231\n",
      "\n",
      "Epoch 00186: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 186/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1350 - acc: 0.9991\n",
      "Epoch 00186: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1351 - acc: 0.9991 - val_loss: 0.4969 - val_acc: 0.9231\n",
      "\n",
      "Epoch 00187: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 187/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1360 - acc: 0.9988\n",
      "Epoch 00187: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1360 - acc: 0.9988 - val_loss: 0.4963 - val_acc: 0.9233\n",
      "\n",
      "Epoch 00188: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 188/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1350 - acc: 0.9990\n",
      "Epoch 00188: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1349 - acc: 0.9991 - val_loss: 0.4964 - val_acc: 0.9235\n",
      "\n",
      "Epoch 00189: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 189/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.9990\n",
      "Epoch 00189: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1353 - acc: 0.9990 - val_loss: 0.4959 - val_acc: 0.9237\n",
      "\n",
      "Epoch 00190: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 190/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.9989\n",
      "Epoch 00190: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1352 - acc: 0.9989 - val_loss: 0.4963 - val_acc: 0.9231\n",
      "\n",
      "Epoch 00191: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 191/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1348 - acc: 0.9990\n",
      "Epoch 00191: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1348 - acc: 0.9991 - val_loss: 0.4962 - val_acc: 0.9237\n",
      "\n",
      "Epoch 00192: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 192/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1350 - acc: 0.9989\n",
      "Epoch 00192: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1350 - acc: 0.9989 - val_loss: 0.4963 - val_acc: 0.9241\n",
      "\n",
      "Epoch 00193: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 193/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1344 - acc: 0.9994\n",
      "Epoch 00193: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1343 - acc: 0.9994 - val_loss: 0.4963 - val_acc: 0.9239\n",
      "\n",
      "Epoch 00194: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 194/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1344 - acc: 0.9992\n",
      "Epoch 00194: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1344 - acc: 0.9992 - val_loss: 0.4959 - val_acc: 0.9235\n",
      "\n",
      "Epoch 00195: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 195/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1341 - acc: 0.9993\n",
      "Epoch 00195: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1341 - acc: 0.9993 - val_loss: 0.4965 - val_acc: 0.9237\n",
      "\n",
      "Epoch 00196: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 196/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1345 - acc: 0.9991\n",
      "Epoch 00196: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1345 - acc: 0.9992 - val_loss: 0.4967 - val_acc: 0.9235\n",
      "\n",
      "Epoch 00197: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 197/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1347 - acc: 0.9990\n",
      "Epoch 00197: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1347 - acc: 0.9990 - val_loss: 0.4967 - val_acc: 0.9239\n",
      "\n",
      "Epoch 00198: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 198/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1341 - acc: 0.9993\n",
      "Epoch 00198: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1341 - acc: 0.9993 - val_loss: 0.4966 - val_acc: 0.9244\n",
      "\n",
      "Epoch 00199: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 199/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1344 - acc: 0.9989\n",
      "Epoch 00199: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1344 - acc: 0.9990 - val_loss: 0.4967 - val_acc: 0.9239\n",
      "\n",
      "Epoch 00200: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 200/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1343 - acc: 0.9991\n",
      "Epoch 00200: val_acc did not improve from 0.92470\n",
      "313/313 [==============================] - 24s 75ms/step - loss: 0.1343 - acc: 0.9991 - val_loss: 0.4967 - val_acc: 0.9234\n",
      "CPU times: user 1h 39min 53s, sys: 11min 27s, total: 1h 51min 20s\n",
      "Wall time: 1h 19min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "history = model.fit_generator(iterator_train_aug, \n",
    "                              steps_per_epoch=steps_per_epoch_train,\n",
    "                              epochs=EPOCHS,\n",
    "                              verbose=1,\n",
    "                              validation_data=iterator_valid,\n",
    "                              validation_steps=steps_per_epoch_val,\n",
    "                              callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('relu-history.dict', 'wb') as f:\n",
    "    pickle.dump(history.history, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'val_loss', 'acc', 'val_acc']\n"
     ]
    }
   ],
   "source": [
    "hist_dict = history.history\n",
    "print(list(hist_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "model.load_weights('model_ckpt_dropactivation_best_relu.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best epoch : 162  |  0.9247\n"
     ]
    }
   ],
   "source": [
    "best_epoch = np.argmax(hist_dict['val_acc'])\n",
    "print(\"best epoch : {}  |  {}\".format(best_epoch, hist_dict['val_acc'][best_epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 2s 19ms/step\n",
      "ACC (test) :  0.9258\n",
      "LOSS (test) :  0.46892707805633543\n"
     ]
    }
   ],
   "source": [
    "loss_test, acc_test = model.evaluate_generator(generator=iterator_test, steps=steps_per_epoch_test, verbose=1)\n",
    "print(\"ACC (test) : \", acc_test)\n",
    "print(\"LOSS (test) : \", loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f83c61df4e0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "plt.plot(list(range(EPOCHS)), hist_dict['acc'], color='red', label='train acc')\n",
    "plt.plot(list(range(EPOCHS)), hist_dict['val_acc'], color='green', label='val acc')\n",
    "plt.scatter([best_epoch, ], [acc_test, ], color='blue', label='test acc')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f83c61b06d8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "plt.plot(list(range(EPOCHS)), hist_dict['loss'], color='red', label='train loss')\n",
    "plt.plot(list(range(EPOCHS)), hist_dict['val_loss'], color='green', label='val loss')\n",
    "plt.scatter([best_epoch, ], [loss_test, ], color='blue', label='test loss')\n",
    "plt.legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
