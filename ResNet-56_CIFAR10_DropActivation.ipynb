{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pprint import pprint\n",
    "from math import ceil\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version :  1.10.0\n",
      "Keras version :  2.1.6-tf\n",
      "Built with CUDA :  True\n",
      "Available GPU :  True\n",
      "keras data_format :  channels_first\n"
     ]
    }
   ],
   "source": [
    "print(\"TF version : \", tf.__version__)\n",
    "print(\"Keras version : \", keras.__version__)\n",
    "with_cuda = tf.test.is_built_with_cuda()\n",
    "with_gpu = tf.test.is_gpu_available()\n",
    "print(\"Built with CUDA : \", with_cuda)\n",
    "print(\"Available GPU : \", with_gpu)\n",
    "\n",
    "if with_cuda and with_gpu:\n",
    "    keras.backend.set_image_data_format('channels_first')\n",
    "else: \n",
    "    keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "print(\"keras data_format : \", keras.backend.image_data_format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 200\n",
    "INIT_LR = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading CIFAR10 dataset ...\n",
      "\tTRAIN - images (40000, 3, 32, 32) | float32  - labels (40000, 10) - float32\n",
      "\tVAL - images (10000, 3, 32, 32) | float32  - labels (10000, 10) - float32\n",
      "\tTEST - images (10000, 3, 32, 32) | float32  - labels (10000, 10) - float32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"... loading CIFAR10 dataset ...\")\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "y_train = np.squeeze(y_train)\n",
    "y_test = np.squeeze(y_test)\n",
    "\n",
    "x_train, y_train = shuffle(x_train, y_train, random_state=51)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train,\n",
    "                                                  test_size=0.2,\n",
    "                                                  stratify=y_train,\n",
    "                                                  random_state=51)\n",
    "# cast samples and labels\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_val = x_val.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)\n",
    "y_train = keras.utils.to_categorical(y_train.astype(np.int32), num_classes=10)\n",
    "y_val = keras.utils.to_categorical(y_val.astype(np.int32), num_classes=10)\n",
    "y_test = keras.utils.to_categorical(y_test.astype(np.int32), num_classes=10)\n",
    "\n",
    "print(\"\\tTRAIN - images {} | {}  - labels {} - {}\".format(x_train.shape, x_train.dtype, y_train.shape, y_train.dtype))\n",
    "print(\"\\tVAL - images {} | {}  - labels {} - {}\".format(x_val.shape, x_val.dtype, y_val.shape, y_val.dtype))\n",
    "print(\"\\tTEST - images {} | {}  - labels {} - {}\\n\".format(x_test.shape, x_test.dtype, y_test.shape, y_test.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_aug = keras.preprocessing.image.ImageDataGenerator(samplewise_center=True,\n",
    "                                                             samplewise_std_normalization=True,\n",
    "                                                             width_shift_range=5,\n",
    "                                                             height_shift_range=5,\n",
    "                                                             fill_mode='constant',\n",
    "                                                             cval=0.0,\n",
    "                                                             horizontal_flip=True,\n",
    "                                                             vertical_flip=False,\n",
    "                                                             data_format=keras.backend.image_data_format())\n",
    "\n",
    "generator = keras.preprocessing.image.ImageDataGenerator(samplewise_center=True,\n",
    "                                                         samplewise_std_normalization=True,\n",
    "                                                         data_format=keras.backend.image_data_format())\n",
    "\n",
    "# python iterator object that yields augmented samples \n",
    "iterator_train_aug = generator_aug.flow(x_train, y_train, batch_size=BATCH_SIZE)\n",
    "\n",
    "# python iterators object that yields not augmented samples \n",
    "iterator_train = generator.flow(x_train, y_train, batch_size=BATCH_SIZE)\n",
    "iterator_valid = generator.flow(x_val, y_val, batch_size=BATCH_SIZE)\n",
    "iterator_test = generator.flow(x_test, y_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "steps_per_epoch_train = int(ceil(iterator_train.n/BATCH_SIZE))\n",
    "steps_per_epoch_val = int(ceil(iterator_valid.n/BATCH_SIZE))\n",
    "steps_per_epoch_test = int(ceil(iterator_test.n/BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : (128, 3, 32, 32) | float32\n",
      "y : (128, 10) | float32\n"
     ]
    }
   ],
   "source": [
    "# test iterator with data augmentation\n",
    "x, y = iterator_train_aug.next()\n",
    "\n",
    "print(\"x : {} | {}\".format(x.shape, x.dtype))\n",
    "print(\"y : {} | {}\".format(y.shape, y.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resnet import ResNet56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/zenith/miniconda3/envs/dl-1.10-gpu/lib/python3.5/site-packages/tensorflow/python/keras/initializers.py:104: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`normal` is a deprecated alias for `truncated_normal`\n"
     ]
    }
   ],
   "source": [
    "shape = [3, 32, 32] if keras.backend.image_data_format()=='channels_first' else [32, 32, 3]\n",
    "\n",
    "model = ResNet56(input_shape=shape, classes=10, p=0.95, activation='drop-activation').build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 3, 32, 32)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 16, 30, 30)   432         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 16, 30, 30)   64          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras (DropActi (None, 16, 30, 30)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 30, 30)   2304        drop_activation_keras[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 16, 30, 30)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_1 (DropAc (None, 16, 30, 30)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 16, 30, 30)   2304        drop_activation_keras_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 16, 30, 30)   256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 16, 30, 30)   0           conv2d_3[0][0]                   \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16, 30, 30)   64          add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_2 (DropAc (None, 16, 30, 30)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 16, 30, 30)   2304        drop_activation_keras_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 16, 30, 30)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_3 (DropAc (None, 16, 30, 30)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 16, 30, 30)   2304        drop_activation_keras_3[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 16, 30, 30)   0           conv2d_5[0][0]                   \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 16, 30, 30)   64          add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_4 (DropAc (None, 16, 30, 30)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 30, 30)   2304        drop_activation_keras_4[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 16, 30, 30)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_5 (DropAc (None, 16, 30, 30)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 30, 30)   2304        drop_activation_keras_5[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 16, 30, 30)   0           conv2d_7[0][0]                   \n",
      "                                                                 add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 30, 30)   64          add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_6 (DropAc (None, 16, 30, 30)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 30, 30)   2304        drop_activation_keras_6[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 30, 30)   64          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_7 (DropAc (None, 16, 30, 30)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 30, 30)   2304        drop_activation_keras_7[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 16, 30, 30)   0           conv2d_9[0][0]                   \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 30, 30)   64          add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_8 (DropAc (None, 16, 30, 30)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 30, 30)   2304        drop_activation_keras_8[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 30, 30)   64          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_9 (DropAc (None, 16, 30, 30)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 30, 30)   2304        drop_activation_keras_9[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 30, 30)   0           conv2d_11[0][0]                  \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 30, 30)   64          add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_10 (DropA (None, 16, 30, 30)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 30, 30)   2304        drop_activation_keras_10[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 30, 30)   64          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_11 (DropA (None, 16, 30, 30)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 30, 30)   2304        drop_activation_keras_11[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 30, 30)   0           conv2d_13[0][0]                  \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 30, 30)   64          add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_12 (DropA (None, 16, 30, 30)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 30, 30)   2304        drop_activation_keras_12[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 30, 30)   64          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_13 (DropA (None, 16, 30, 30)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 30, 30)   2304        drop_activation_keras_13[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 30, 30)   0           conv2d_15[0][0]                  \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 30, 30)   64          add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_14 (DropA (None, 16, 30, 30)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 30, 30)   2304        drop_activation_keras_14[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 30, 30)   64          conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_15 (DropA (None, 16, 30, 30)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 30, 30)   2304        drop_activation_keras_15[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 16, 30, 30)   0           conv2d_17[0][0]                  \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 30, 30)   64          add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_16 (DropA (None, 16, 30, 30)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 30, 30)   2304        drop_activation_keras_16[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 30, 30)   64          conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_17 (DropA (None, 16, 30, 30)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 16, 30, 30)   2304        drop_activation_keras_17[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 16, 30, 30)   0           conv2d_19[0][0]                  \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 16, 30, 30)   64          add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_18 (DropA (None, 16, 30, 30)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 16, 32, 32)   0           drop_activation_keras_18[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 32, 15, 15)   4608        zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 32, 15, 15)   128         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_19 (DropA (None, 32, 15, 15)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 32, 15, 15)   9216        drop_activation_keras_19[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 32, 15, 15)   512         add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 32, 15, 15)   0           conv2d_22[0][0]                  \n",
      "                                                                 conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 32, 15, 15)   128         add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_20 (DropA (None, 32, 15, 15)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 32, 15, 15)   9216        drop_activation_keras_20[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 32, 15, 15)   128         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_21 (DropA (None, 32, 15, 15)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 32, 15, 15)   9216        drop_activation_keras_21[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 32, 15, 15)   0           conv2d_24[0][0]                  \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 32, 15, 15)   128         add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_22 (DropA (None, 32, 15, 15)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 32, 15, 15)   9216        drop_activation_keras_22[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 32, 15, 15)   128         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_23 (DropA (None, 32, 15, 15)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 32, 15, 15)   9216        drop_activation_keras_23[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 32, 15, 15)   0           conv2d_26[0][0]                  \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 32, 15, 15)   128         add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_24 (DropA (None, 32, 15, 15)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 32, 15, 15)   9216        drop_activation_keras_24[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 32, 15, 15)   128         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_25 (DropA (None, 32, 15, 15)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 32, 15, 15)   9216        drop_activation_keras_25[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 32, 15, 15)   0           conv2d_28[0][0]                  \n",
      "                                                                 add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 32, 15, 15)   128         add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_26 (DropA (None, 32, 15, 15)   0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 32, 15, 15)   9216        drop_activation_keras_26[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 32, 15, 15)   128         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_27 (DropA (None, 32, 15, 15)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 32, 15, 15)   9216        drop_activation_keras_27[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 32, 15, 15)   0           conv2d_30[0][0]                  \n",
      "                                                                 add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 32, 15, 15)   128         add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_28 (DropA (None, 32, 15, 15)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 32, 15, 15)   9216        drop_activation_keras_28[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 32, 15, 15)   128         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_29 (DropA (None, 32, 15, 15)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 32, 15, 15)   9216        drop_activation_keras_29[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 32, 15, 15)   0           conv2d_32[0][0]                  \n",
      "                                                                 add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 32, 15, 15)   128         add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_30 (DropA (None, 32, 15, 15)   0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 32, 15, 15)   9216        drop_activation_keras_30[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 32, 15, 15)   128         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_31 (DropA (None, 32, 15, 15)   0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 32, 15, 15)   9216        drop_activation_keras_31[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 32, 15, 15)   0           conv2d_34[0][0]                  \n",
      "                                                                 add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 32, 15, 15)   128         add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_32 (DropA (None, 32, 15, 15)   0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 32, 15, 15)   9216        drop_activation_keras_32[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 32, 15, 15)   128         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_33 (DropA (None, 32, 15, 15)   0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 32, 15, 15)   9216        drop_activation_keras_33[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 32, 15, 15)   0           conv2d_36[0][0]                  \n",
      "                                                                 add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 32, 15, 15)   128         add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_34 (DropA (None, 32, 15, 15)   0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 32, 15, 15)   9216        drop_activation_keras_34[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 32, 15, 15)   128         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_35 (DropA (None, 32, 15, 15)   0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 32, 15, 15)   9216        drop_activation_keras_35[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 32, 15, 15)   0           conv2d_38[0][0]                  \n",
      "                                                                 add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 32, 15, 15)   128         add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_36 (DropA (None, 32, 15, 15)   0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 32, 17, 17)   0           drop_activation_keras_36[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 64, 8, 8)     18432       zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 64, 8, 8)     256         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_37 (DropA (None, 64, 8, 8)     0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 64, 8, 8)     36864       drop_activation_keras_37[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 64, 8, 8)     2048        add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 64, 8, 8)     0           conv2d_41[0][0]                  \n",
      "                                                                 conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 64, 8, 8)     256         add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_38 (DropA (None, 64, 8, 8)     0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 64, 8, 8)     36864       drop_activation_keras_38[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 64, 8, 8)     256         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_39 (DropA (None, 64, 8, 8)     0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 64, 8, 8)     36864       drop_activation_keras_39[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 64, 8, 8)     0           conv2d_43[0][0]                  \n",
      "                                                                 add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 64, 8, 8)     256         add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_40 (DropA (None, 64, 8, 8)     0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 64, 8, 8)     36864       drop_activation_keras_40[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 64, 8, 8)     256         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_41 (DropA (None, 64, 8, 8)     0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 64, 8, 8)     36864       drop_activation_keras_41[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 64, 8, 8)     0           conv2d_45[0][0]                  \n",
      "                                                                 add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 64, 8, 8)     256         add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_42 (DropA (None, 64, 8, 8)     0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 64, 8, 8)     36864       drop_activation_keras_42[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 64, 8, 8)     256         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_43 (DropA (None, 64, 8, 8)     0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 64, 8, 8)     36864       drop_activation_keras_43[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 64, 8, 8)     0           conv2d_47[0][0]                  \n",
      "                                                                 add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 64, 8, 8)     256         add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_44 (DropA (None, 64, 8, 8)     0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 64, 8, 8)     36864       drop_activation_keras_44[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 64, 8, 8)     256         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_45 (DropA (None, 64, 8, 8)     0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 64, 8, 8)     36864       drop_activation_keras_45[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 64, 8, 8)     0           conv2d_49[0][0]                  \n",
      "                                                                 add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 64, 8, 8)     256         add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_46 (DropA (None, 64, 8, 8)     0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 64, 8, 8)     36864       drop_activation_keras_46[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 64, 8, 8)     256         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_47 (DropA (None, 64, 8, 8)     0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 64, 8, 8)     36864       drop_activation_keras_47[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 64, 8, 8)     0           conv2d_51[0][0]                  \n",
      "                                                                 add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 64, 8, 8)     256         add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_48 (DropA (None, 64, 8, 8)     0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 64, 8, 8)     36864       drop_activation_keras_48[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 64, 8, 8)     256         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_49 (DropA (None, 64, 8, 8)     0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 64, 8, 8)     36864       drop_activation_keras_49[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 64, 8, 8)     0           conv2d_53[0][0]                  \n",
      "                                                                 add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 64, 8, 8)     256         add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_50 (DropA (None, 64, 8, 8)     0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 64, 8, 8)     36864       drop_activation_keras_50[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 64, 8, 8)     256         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_51 (DropA (None, 64, 8, 8)     0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 64, 8, 8)     36864       drop_activation_keras_51[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 64, 8, 8)     0           conv2d_55[0][0]                  \n",
      "                                                                 add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 64, 8, 8)     256         add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_52 (DropA (None, 64, 8, 8)     0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 64, 8, 8)     36864       drop_activation_keras_52[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 64, 8, 8)     256         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_53 (DropA (None, 64, 8, 8)     0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 64, 8, 8)     36864       drop_activation_keras_53[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 64, 8, 8)     0           conv2d_57[0][0]                  \n",
      "                                                                 add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 64, 8, 8)     256         add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "drop_activation_keras_54 (DropA (None, 64, 8, 8)     0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 64)           0           drop_activation_keras_54[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10)           650         avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 859,898\n",
      "Trainable params: 855,834\n",
      "Non-trainable params: 4,064\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=INIT_LR, momentum=0.9)\n",
    "loss = 'categorical_crossentropy'\n",
    "metrics = ['acc', ]\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights('random_weights.h5')\n",
    "model.load_weights('random_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "\n",
    "model_ckpt = keras.callbacks.ModelCheckpoint(\"model_ckpt_dropactivation_best_drop-activation.h5\",\n",
    "                                             monitor='val_acc', verbose=1, save_best_only=True, \n",
    "                                             save_weights_only=True)\n",
    "callbacks.append(model_ckpt)\n",
    "\n",
    "def schedule(epoch):\n",
    "    if epoch < 91:\n",
    "        return INIT_LR\n",
    "    if epoch < 136:\n",
    "        return 0.1*INIT_LR\n",
    "    if epoch < 182:\n",
    "        return 0.01*INIT_LR\n",
    "    else:\n",
    "        return 0.001*INIT_LR\n",
    "    \n",
    "lr_schedule = keras.callbacks.LearningRateScheduler(schedule, verbose=1)\n",
    "callbacks.append(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 1/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 2.3975 - acc: 0.4005\n",
      "Epoch 00001: val_acc improved from -inf to 0.20950, saving model to model_ckpt_dropactivation_best_drop-activation.h5\n",
      "313/313 [==============================] - 58s 186ms/step - loss: 2.3962 - acc: 0.4010 - val_loss: 5.5294 - val_acc: 0.2095\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 2/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.8522 - acc: 0.5649\n",
      "Epoch 00002: val_acc improved from 0.20950 to 0.40030, saving model to model_ckpt_dropactivation_best_drop-activation.h5\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 1.8518 - acc: 0.5651 - val_loss: 2.7484 - val_acc: 0.4003\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 3/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.5802 - acc: 0.6235\n",
      "Epoch 00003: val_acc improved from 0.40030 to 0.43860, saving model to model_ckpt_dropactivation_best_drop-activation.h5\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 1.5804 - acc: 0.6232 - val_loss: 2.6111 - val_acc: 0.4386\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 4/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.3955 - acc: 0.6680\n",
      "Epoch 00004: val_acc improved from 0.43860 to 0.56390, saving model to model_ckpt_dropactivation_best_drop-activation.h5\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 1.3957 - acc: 0.6679 - val_loss: 1.7243 - val_acc: 0.5639\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 5/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.2526 - acc: 0.6989\n",
      "Epoch 00005: val_acc did not improve from 0.56390\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 1.2528 - acc: 0.6988 - val_loss: 1.9276 - val_acc: 0.4724\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 6/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.1313 - acc: 0.7286\n",
      "Epoch 00006: val_acc did not improve from 0.56390\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 1.1314 - acc: 0.7285 - val_loss: 1.8194 - val_acc: 0.5525\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 7/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.0480 - acc: 0.7492\n",
      "Epoch 00007: val_acc improved from 0.56390 to 0.63360, saving model to model_ckpt_dropactivation_best_drop-activation.h5\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 1.0476 - acc: 0.7493 - val_loss: 1.4785 - val_acc: 0.6336\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 8/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9793 - acc: 0.7653\n",
      "Epoch 00008: val_acc improved from 0.63360 to 0.65850, saving model to model_ckpt_dropactivation_best_drop-activation.h5\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.9796 - acc: 0.7654 - val_loss: 1.3866 - val_acc: 0.6585\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 9/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9344 - acc: 0.7726\n",
      "Epoch 00009: val_acc improved from 0.65850 to 0.67400, saving model to model_ckpt_dropactivation_best_drop-activation.h5\n",
      "313/313 [==============================] - 36s 115ms/step - loss: 0.9343 - acc: 0.7725 - val_loss: 1.2472 - val_acc: 0.6740\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 10/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.8918 - acc: 0.7850\n",
      "Epoch 00010: val_acc did not improve from 0.67400\n",
      "313/313 [==============================] - 36s 113ms/step - loss: 0.8920 - acc: 0.7849 - val_loss: 1.4577 - val_acc: 0.6039\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 11/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.8605 - acc: 0.7916\n",
      "Epoch 00011: val_acc improved from 0.67400 to 0.67560, saving model to model_ckpt_dropactivation_best_drop-activation.h5\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.8607 - acc: 0.7914 - val_loss: 1.3102 - val_acc: 0.6756\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 12/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.8312 - acc: 0.8031\n",
      "Epoch 00012: val_acc improved from 0.67560 to 0.68980, saving model to model_ckpt_dropactivation_best_drop-activation.h5\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.8311 - acc: 0.8030 - val_loss: 1.2053 - val_acc: 0.6898\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 13/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.8161 - acc: 0.8035\n",
      "Epoch 00013: val_acc improved from 0.68980 to 0.75430, saving model to model_ckpt_dropactivation_best_drop-activation.h5\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.8164 - acc: 0.8034 - val_loss: 0.9878 - val_acc: 0.7543\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 14/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7946 - acc: 0.8116\n",
      "Epoch 00014: val_acc did not improve from 0.75430\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.7947 - acc: 0.8116 - val_loss: 0.9740 - val_acc: 0.7453\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 15/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7825 - acc: 0.8157\n",
      "Epoch 00015: val_acc improved from 0.75430 to 0.75700, saving model to model_ckpt_dropactivation_best_drop-activation.h5\n",
      "313/313 [==============================] - 36s 115ms/step - loss: 0.7819 - acc: 0.8159 - val_loss: 0.9534 - val_acc: 0.7570\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 16/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7734 - acc: 0.8186\n",
      "Epoch 00016: val_acc did not improve from 0.75700\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.7733 - acc: 0.8185 - val_loss: 1.0581 - val_acc: 0.7321\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 17/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7622 - acc: 0.8208\n",
      "Epoch 00017: val_acc did not improve from 0.75700\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.7621 - acc: 0.8209 - val_loss: 1.7850 - val_acc: 0.5781\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 18/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7508 - acc: 0.8257\n",
      "Epoch 00018: val_acc did not improve from 0.75700\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.7513 - acc: 0.8255 - val_loss: 1.1618 - val_acc: 0.7160\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 19/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7462 - acc: 0.8284\n",
      "Epoch 00019: val_acc did not improve from 0.75700\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.7459 - acc: 0.8285 - val_loss: 1.0778 - val_acc: 0.7371\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 20/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7350 - acc: 0.8333\n",
      "Epoch 00020: val_acc did not improve from 0.75700\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.7351 - acc: 0.8333 - val_loss: 1.0811 - val_acc: 0.7301\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 21/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7363 - acc: 0.8316\n",
      "Epoch 00021: val_acc did not improve from 0.75700\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.7362 - acc: 0.8317 - val_loss: 1.3010 - val_acc: 0.6958\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 22/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7256 - acc: 0.8360\n",
      "Epoch 00022: val_acc did not improve from 0.75700\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.7261 - acc: 0.8359 - val_loss: 1.3998 - val_acc: 0.6498\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 23/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/313 [============================>.] - ETA: 0s - loss: 0.7244 - acc: 0.8375\n",
      "Epoch 00023: val_acc did not improve from 0.75700\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.7244 - acc: 0.8375 - val_loss: 1.2299 - val_acc: 0.7056\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 24/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7213 - acc: 0.8368\n",
      "Epoch 00024: val_acc did not improve from 0.75700\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.7215 - acc: 0.8367 - val_loss: 1.0886 - val_acc: 0.7421\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 25/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7192 - acc: 0.8405\n",
      "Epoch 00025: val_acc improved from 0.75700 to 0.76350, saving model to model_ckpt_dropactivation_best_drop-activation.h5\n",
      "313/313 [==============================] - 36s 115ms/step - loss: 0.7186 - acc: 0.8408 - val_loss: 0.9987 - val_acc: 0.7635\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 26/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7134 - acc: 0.8442\n",
      "Epoch 00026: val_acc did not improve from 0.76350\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.7138 - acc: 0.8440 - val_loss: 1.1665 - val_acc: 0.7211\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 27/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7130 - acc: 0.8460\n",
      "Epoch 00027: val_acc did not improve from 0.76350\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.7127 - acc: 0.8461 - val_loss: 1.2453 - val_acc: 0.6938\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 28/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7036 - acc: 0.8472\n",
      "Epoch 00028: val_acc did not improve from 0.76350\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.7037 - acc: 0.8471 - val_loss: 0.9919 - val_acc: 0.7608\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 29/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7020 - acc: 0.8476\n",
      "Epoch 00029: val_acc did not improve from 0.76350\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.7021 - acc: 0.8476 - val_loss: 1.0445 - val_acc: 0.7463\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 30/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7014 - acc: 0.8499\n",
      "Epoch 00030: val_acc did not improve from 0.76350\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.7018 - acc: 0.8499 - val_loss: 1.0840 - val_acc: 0.7377\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 31/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7049 - acc: 0.8490\n",
      "Epoch 00031: val_acc did not improve from 0.76350\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.7052 - acc: 0.8488 - val_loss: 1.6167 - val_acc: 0.6112\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 32/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7010 - acc: 0.8497\n",
      "Epoch 00032: val_acc improved from 0.76350 to 0.79390, saving model to model_ckpt_dropactivation_best_drop-activation.h5\n",
      "313/313 [==============================] - 36s 115ms/step - loss: 0.7008 - acc: 0.8498 - val_loss: 0.9103 - val_acc: 0.7939\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 33/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6985 - acc: 0.8507\n",
      "Epoch 00033: val_acc did not improve from 0.79390\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6985 - acc: 0.8507 - val_loss: 1.0013 - val_acc: 0.7648\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 34/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6964 - acc: 0.8521\n",
      "Epoch 00034: val_acc did not improve from 0.79390\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6966 - acc: 0.8521 - val_loss: 0.9305 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 35/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6887 - acc: 0.8552\n",
      "Epoch 00035: val_acc did not improve from 0.79390\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6889 - acc: 0.8553 - val_loss: 1.2611 - val_acc: 0.6918\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 36/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6935 - acc: 0.8529\n",
      "Epoch 00036: val_acc did not improve from 0.79390\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6932 - acc: 0.8530 - val_loss: 1.1009 - val_acc: 0.7445\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 37/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6863 - acc: 0.8544\n",
      "Epoch 00037: val_acc did not improve from 0.79390\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6863 - acc: 0.8545 - val_loss: 1.0772 - val_acc: 0.7564\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 38/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6881 - acc: 0.8565\n",
      "Epoch 00038: val_acc did not improve from 0.79390\n",
      "313/313 [==============================] - 36s 113ms/step - loss: 0.6883 - acc: 0.8563 - val_loss: 1.0324 - val_acc: 0.7667\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 39/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6896 - acc: 0.8543\n",
      "Epoch 00039: val_acc did not improve from 0.79390\n",
      "313/313 [==============================] - 36s 113ms/step - loss: 0.6896 - acc: 0.8542 - val_loss: 1.2025 - val_acc: 0.7102\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 40/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6802 - acc: 0.8577\n",
      "Epoch 00040: val_acc did not improve from 0.79390\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6804 - acc: 0.8577 - val_loss: 1.2832 - val_acc: 0.6944\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 41/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6814 - acc: 0.8583\n",
      "Epoch 00041: val_acc improved from 0.79390 to 0.82610, saving model to model_ckpt_dropactivation_best_drop-activation.h5\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6814 - acc: 0.8585 - val_loss: 0.8405 - val_acc: 0.8261\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 42/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6875 - acc: 0.8569\n",
      "Epoch 00042: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6872 - acc: 0.8570 - val_loss: 1.2300 - val_acc: 0.7015\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 43/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6809 - acc: 0.8617\n",
      "Epoch 00043: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6809 - acc: 0.8616 - val_loss: 1.1076 - val_acc: 0.7309\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 44/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6851 - acc: 0.8604\n",
      "Epoch 00044: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6853 - acc: 0.8603 - val_loss: 0.9728 - val_acc: 0.7643\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 45/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6766 - acc: 0.8609\n",
      "Epoch 00045: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6765 - acc: 0.8610 - val_loss: 1.4653 - val_acc: 0.6492\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 46/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6770 - acc: 0.8605\n",
      "Epoch 00046: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6771 - acc: 0.8604 - val_loss: 1.6924 - val_acc: 0.6490\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 47/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/313 [============================>.] - ETA: 0s - loss: 0.6752 - acc: 0.8606\n",
      "Epoch 00047: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6751 - acc: 0.8606 - val_loss: 1.0994 - val_acc: 0.7529\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 48/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6814 - acc: 0.8613\n",
      "Epoch 00048: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6811 - acc: 0.8614 - val_loss: 0.9977 - val_acc: 0.7837\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 49/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6735 - acc: 0.8664\n",
      "Epoch 00049: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6736 - acc: 0.8662 - val_loss: 0.9649 - val_acc: 0.7623\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 50/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6781 - acc: 0.8623\n",
      "Epoch 00050: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 113ms/step - loss: 0.6777 - acc: 0.8625 - val_loss: 0.8742 - val_acc: 0.8075\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 51/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6718 - acc: 0.8654\n",
      "Epoch 00051: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6717 - acc: 0.8653 - val_loss: 0.8883 - val_acc: 0.8002\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 52/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6688 - acc: 0.8670\n",
      "Epoch 00052: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6691 - acc: 0.8669 - val_loss: 1.0187 - val_acc: 0.7855\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 53/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6708 - acc: 0.8648\n",
      "Epoch 00053: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6707 - acc: 0.8648 - val_loss: 1.4429 - val_acc: 0.6627\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 54/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6691 - acc: 0.8674\n",
      "Epoch 00054: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 113ms/step - loss: 0.6691 - acc: 0.8675 - val_loss: 0.8360 - val_acc: 0.8133\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 55/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6706 - acc: 0.8658\n",
      "Epoch 00055: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6706 - acc: 0.8659 - val_loss: 0.9455 - val_acc: 0.7807\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 56/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6663 - acc: 0.8689\n",
      "Epoch 00056: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6662 - acc: 0.8689 - val_loss: 0.8364 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 57/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6612 - acc: 0.8682\n",
      "Epoch 00057: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6611 - acc: 0.8682 - val_loss: 0.9343 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 58/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6744 - acc: 0.8651\n",
      "Epoch 00058: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6741 - acc: 0.8651 - val_loss: 1.0329 - val_acc: 0.7630\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 59/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6617 - acc: 0.8687\n",
      "Epoch 00059: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 113ms/step - loss: 0.6615 - acc: 0.8688 - val_loss: 0.9680 - val_acc: 0.7851\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 60/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6582 - acc: 0.8709\n",
      "Epoch 00060: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6581 - acc: 0.8710 - val_loss: 0.8903 - val_acc: 0.8036\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 61/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6617 - acc: 0.8694\n",
      "Epoch 00061: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6615 - acc: 0.8695 - val_loss: 1.1225 - val_acc: 0.7306\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 62/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6575 - acc: 0.8705\n",
      "Epoch 00062: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6573 - acc: 0.8706 - val_loss: 1.3813 - val_acc: 0.6747\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 63/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6685 - acc: 0.8687\n",
      "Epoch 00063: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6685 - acc: 0.8686 - val_loss: 0.9473 - val_acc: 0.7814\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 64/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6618 - acc: 0.8695\n",
      "Epoch 00064: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6622 - acc: 0.8694 - val_loss: 1.0920 - val_acc: 0.7713\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 65/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6641 - acc: 0.8694\n",
      "Epoch 00065: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6648 - acc: 0.8692 - val_loss: 0.8764 - val_acc: 0.8040\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 66/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6644 - acc: 0.8702\n",
      "Epoch 00066: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6644 - acc: 0.8702 - val_loss: 0.8484 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 67/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6613 - acc: 0.8708\n",
      "Epoch 00067: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6613 - acc: 0.8709 - val_loss: 0.8871 - val_acc: 0.8054\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 68/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6566 - acc: 0.8724\n",
      "Epoch 00068: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6570 - acc: 0.8723 - val_loss: 0.9273 - val_acc: 0.7948\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 69/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6495 - acc: 0.8732\n",
      "Epoch 00069: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6494 - acc: 0.8733 - val_loss: 0.8413 - val_acc: 0.8095\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 70/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6646 - acc: 0.8696\n",
      "Epoch 00070: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6648 - acc: 0.8695 - val_loss: 0.9155 - val_acc: 0.8039\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 71/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6618 - acc: 0.8718\n",
      "Epoch 00071: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6615 - acc: 0.8719 - val_loss: 0.8620 - val_acc: 0.8086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 72/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6580 - acc: 0.8727\n",
      "Epoch 00072: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 113ms/step - loss: 0.6582 - acc: 0.8726 - val_loss: 1.1229 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 73/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6511 - acc: 0.8749\n",
      "Epoch 00073: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6509 - acc: 0.8750 - val_loss: 1.0168 - val_acc: 0.7780\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 74/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6532 - acc: 0.8754\n",
      "Epoch 00074: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6532 - acc: 0.8754 - val_loss: 0.9351 - val_acc: 0.7817\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 75/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6617 - acc: 0.8717\n",
      "Epoch 00075: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6618 - acc: 0.8717 - val_loss: 1.1813 - val_acc: 0.7303\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 76/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6466 - acc: 0.8759\n",
      "Epoch 00076: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6468 - acc: 0.8758 - val_loss: 0.9619 - val_acc: 0.7937\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 77/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6506 - acc: 0.8753\n",
      "Epoch 00077: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6507 - acc: 0.8753 - val_loss: 1.0281 - val_acc: 0.7724\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 78/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6521 - acc: 0.8765\n",
      "Epoch 00078: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6523 - acc: 0.8764 - val_loss: 0.8466 - val_acc: 0.8181\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 79/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6513 - acc: 0.8758\n",
      "Epoch 00079: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6517 - acc: 0.8757 - val_loss: 0.8827 - val_acc: 0.8054\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 80/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6525 - acc: 0.8745\n",
      "Epoch 00080: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6526 - acc: 0.8744 - val_loss: 1.0914 - val_acc: 0.7620\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 81/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6562 - acc: 0.8754\n",
      "Epoch 00081: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6563 - acc: 0.8754 - val_loss: 1.1856 - val_acc: 0.7473\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 82/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6483 - acc: 0.8758\n",
      "Epoch 00082: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6483 - acc: 0.8758 - val_loss: 1.0654 - val_acc: 0.7642\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 83/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6425 - acc: 0.8786\n",
      "Epoch 00083: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 113ms/step - loss: 0.6425 - acc: 0.8786 - val_loss: 0.9575 - val_acc: 0.7959\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 84/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6507 - acc: 0.8758\n",
      "Epoch 00084: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6511 - acc: 0.8758 - val_loss: 0.8936 - val_acc: 0.8092\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 85/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6510 - acc: 0.8765\n",
      "Epoch 00085: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6508 - acc: 0.8765 - val_loss: 0.9222 - val_acc: 0.8016\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 86/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6452 - acc: 0.8781\n",
      "Epoch 00086: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6454 - acc: 0.8779 - val_loss: 0.8502 - val_acc: 0.8175\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 87/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6507 - acc: 0.8763\n",
      "Epoch 00087: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6505 - acc: 0.8764 - val_loss: 1.0079 - val_acc: 0.7664\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 88/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6473 - acc: 0.8773\n",
      "Epoch 00088: val_acc did not improve from 0.82610\n",
      "313/313 [==============================] - 36s 113ms/step - loss: 0.6472 - acc: 0.8773 - val_loss: 0.8494 - val_acc: 0.8149\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 89/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6430 - acc: 0.8784\n",
      "Epoch 00089: val_acc improved from 0.82610 to 0.82770, saving model to model_ckpt_dropactivation_best_drop-activation.h5\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6432 - acc: 0.8784 - val_loss: 0.8051 - val_acc: 0.8277\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 90/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6463 - acc: 0.8772\n",
      "Epoch 00090: val_acc did not improve from 0.82770\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6467 - acc: 0.8770 - val_loss: 0.9897 - val_acc: 0.7907\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 91/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6479 - acc: 0.8768\n",
      "Epoch 00091: val_acc did not improve from 0.82770\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.6479 - acc: 0.8767 - val_loss: 1.0170 - val_acc: 0.7855\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 92/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.5412 - acc: 0.9139\n",
      "Epoch 00092: val_acc improved from 0.82770 to 0.90600, saving model to model_ckpt_dropactivation_best_drop-activation.h5\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.5411 - acc: 0.9140 - val_loss: 0.5655 - val_acc: 0.9060\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 93/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4693 - acc: 0.9371\n",
      "Epoch 00093: val_acc improved from 0.90600 to 0.91020, saving model to model_ckpt_dropactivation_best_drop-activation.h5\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.4694 - acc: 0.9371 - val_loss: 0.5604 - val_acc: 0.9102\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 94/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4516 - acc: 0.9413\n",
      "Epoch 00094: val_acc improved from 0.91020 to 0.91370, saving model to model_ckpt_dropactivation_best_drop-activation.h5\n",
      "313/313 [==============================] - 36s 115ms/step - loss: 0.4516 - acc: 0.9413 - val_loss: 0.5379 - val_acc: 0.9137\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 95/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/313 [============================>.] - ETA: 0s - loss: 0.4312 - acc: 0.9457\n",
      "Epoch 00095: val_acc did not improve from 0.91370\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.4313 - acc: 0.9457 - val_loss: 0.5299 - val_acc: 0.9130\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 96/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4182 - acc: 0.9483\n",
      "Epoch 00096: val_acc improved from 0.91370 to 0.91710, saving model to model_ckpt_dropactivation_best_drop-activation.h5\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.4182 - acc: 0.9483 - val_loss: 0.5211 - val_acc: 0.9171\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 97/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4098 - acc: 0.9485\n",
      "Epoch 00097: val_acc did not improve from 0.91710\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.4097 - acc: 0.9484 - val_loss: 0.5383 - val_acc: 0.9098\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 98/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3946 - acc: 0.9525\n",
      "Epoch 00098: val_acc improved from 0.91710 to 0.91750, saving model to model_ckpt_dropactivation_best_drop-activation.h5\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.3947 - acc: 0.9525 - val_loss: 0.5072 - val_acc: 0.9175\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 99/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3895 - acc: 0.9518\n",
      "Epoch 00099: val_acc did not improve from 0.91750\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.3894 - acc: 0.9519 - val_loss: 0.5066 - val_acc: 0.9146\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 100/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3757 - acc: 0.9547\n",
      "Epoch 00100: val_acc improved from 0.91750 to 0.91760, saving model to model_ckpt_dropactivation_best_drop-activation.h5\n",
      "313/313 [==============================] - 36s 115ms/step - loss: 0.3760 - acc: 0.9546 - val_loss: 0.5021 - val_acc: 0.9176\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 101/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3690 - acc: 0.9556\n",
      "Epoch 00101: val_acc improved from 0.91760 to 0.91930, saving model to model_ckpt_dropactivation_best_drop-activation.h5\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.3689 - acc: 0.9555 - val_loss: 0.5017 - val_acc: 0.9193\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 102/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3593 - acc: 0.9563\n",
      "Epoch 00102: val_acc did not improve from 0.91930\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.3595 - acc: 0.9563 - val_loss: 0.5048 - val_acc: 0.9139\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 103/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3523 - acc: 0.9569\n",
      "Epoch 00103: val_acc did not improve from 0.91930\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.3525 - acc: 0.9568 - val_loss: 0.4971 - val_acc: 0.9157\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 104/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3475 - acc: 0.9578\n",
      "Epoch 00104: val_acc did not improve from 0.91930\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.3476 - acc: 0.9578 - val_loss: 0.4901 - val_acc: 0.9167\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 105/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3353 - acc: 0.9604\n",
      "Epoch 00105: val_acc did not improve from 0.91930\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.3353 - acc: 0.9603 - val_loss: 0.4903 - val_acc: 0.9149\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 106/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3332 - acc: 0.9588\n",
      "Epoch 00106: val_acc did not improve from 0.91930\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.3332 - acc: 0.9589 - val_loss: 0.4794 - val_acc: 0.9171\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 107/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3248 - acc: 0.9612\n",
      "Epoch 00107: val_acc did not improve from 0.91930\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.3246 - acc: 0.9612 - val_loss: 0.4943 - val_acc: 0.9136\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 108/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3180 - acc: 0.9612\n",
      "Epoch 00108: val_acc did not improve from 0.91930\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.3183 - acc: 0.9611 - val_loss: 0.4908 - val_acc: 0.9155\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 109/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3124 - acc: 0.9621\n",
      "Epoch 00109: val_acc did not improve from 0.91930\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.3124 - acc: 0.9621 - val_loss: 0.4884 - val_acc: 0.9153\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 110/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3074 - acc: 0.9632\n",
      "Epoch 00110: val_acc did not improve from 0.91930\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.3074 - acc: 0.9632 - val_loss: 0.4593 - val_acc: 0.9184\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 111/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3020 - acc: 0.9643\n",
      "Epoch 00111: val_acc did not improve from 0.91930\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.3019 - acc: 0.9644 - val_loss: 0.4837 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 112/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3007 - acc: 0.9632\n",
      "Epoch 00112: val_acc did not improve from 0.91930\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.3007 - acc: 0.9632 - val_loss: 0.5031 - val_acc: 0.9085\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 113/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2940 - acc: 0.9642\n",
      "Epoch 00113: val_acc did not improve from 0.91930\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.2940 - acc: 0.9642 - val_loss: 0.4816 - val_acc: 0.9118\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 114/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2840 - acc: 0.9653\n",
      "Epoch 00114: val_acc did not improve from 0.91930\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.2840 - acc: 0.9654 - val_loss: 0.5016 - val_acc: 0.9117\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 115/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2853 - acc: 0.9653\n",
      "Epoch 00115: val_acc did not improve from 0.91930\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.2855 - acc: 0.9652 - val_loss: 0.4747 - val_acc: 0.9166\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 116/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2814 - acc: 0.9642\n",
      "Epoch 00116: val_acc did not improve from 0.91930\n",
      "313/313 [==============================] - 36s 113ms/step - loss: 0.2814 - acc: 0.9642 - val_loss: 0.4727 - val_acc: 0.9120\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 117/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2767 - acc: 0.9651\n",
      "Epoch 00117: val_acc did not improve from 0.91930\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.2768 - acc: 0.9649 - val_loss: 0.5025 - val_acc: 0.9092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 118/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2733 - acc: 0.9665\n",
      "Epoch 00118: val_acc did not improve from 0.91930\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.2733 - acc: 0.9665 - val_loss: 0.4794 - val_acc: 0.9147\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 119/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2708 - acc: 0.9656\n",
      "Epoch 00119: val_acc did not improve from 0.91930\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.2712 - acc: 0.9655 - val_loss: 0.4908 - val_acc: 0.9099\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 120/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2722 - acc: 0.9645\n",
      "Epoch 00120: val_acc did not improve from 0.91930\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.2722 - acc: 0.9645 - val_loss: 0.4590 - val_acc: 0.9178\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 121/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2620 - acc: 0.9681\n",
      "Epoch 00121: val_acc did not improve from 0.91930\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.2621 - acc: 0.9681 - val_loss: 0.4869 - val_acc: 0.9128\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 122/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2625 - acc: 0.9665\n",
      "Epoch 00122: val_acc did not improve from 0.91930\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.2626 - acc: 0.9666 - val_loss: 0.5166 - val_acc: 0.9028\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 123/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2572 - acc: 0.9689\n",
      "Epoch 00123: val_acc did not improve from 0.91930\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.2572 - acc: 0.9689 - val_loss: 0.5474 - val_acc: 0.8988\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 124/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2577 - acc: 0.9671\n",
      "Epoch 00124: val_acc did not improve from 0.91930\n",
      "313/313 [==============================] - 36s 113ms/step - loss: 0.2578 - acc: 0.9671 - val_loss: 0.4627 - val_acc: 0.9158\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 125/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2566 - acc: 0.9665\n",
      "Epoch 00125: val_acc did not improve from 0.91930\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.2568 - acc: 0.9665 - val_loss: 0.4693 - val_acc: 0.9164\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 126/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2528 - acc: 0.9670\n",
      "Epoch 00126: val_acc did not improve from 0.91930\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.2528 - acc: 0.9671 - val_loss: 0.5364 - val_acc: 0.8909\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 127/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2462 - acc: 0.9695\n",
      "Epoch 00127: val_acc did not improve from 0.91930\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.2461 - acc: 0.9695 - val_loss: 0.5108 - val_acc: 0.9017\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 128/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2480 - acc: 0.9683\n",
      "Epoch 00128: val_acc did not improve from 0.91930\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.2480 - acc: 0.9684 - val_loss: 0.4882 - val_acc: 0.9070\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 129/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2472 - acc: 0.9684\n",
      "Epoch 00129: val_acc did not improve from 0.91930\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.2474 - acc: 0.9684 - val_loss: 0.5037 - val_acc: 0.9018\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 130/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2439 - acc: 0.9678\n",
      "Epoch 00130: val_acc did not improve from 0.91930\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.2438 - acc: 0.9679 - val_loss: 0.5313 - val_acc: 0.9002\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 131/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2423 - acc: 0.9691\n",
      "Epoch 00131: val_acc did not improve from 0.91930\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.2426 - acc: 0.9690 - val_loss: 0.4896 - val_acc: 0.9117\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 132/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2455 - acc: 0.9664\n",
      "Epoch 00132: val_acc did not improve from 0.91930\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.2454 - acc: 0.9665 - val_loss: 0.5152 - val_acc: 0.8989\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 133/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2402 - acc: 0.9668\n",
      "Epoch 00133: val_acc did not improve from 0.91930\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.2402 - acc: 0.9668 - val_loss: 0.5054 - val_acc: 0.8999\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 134/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2438 - acc: 0.9663\n",
      "Epoch 00134: val_acc did not improve from 0.91930\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.2442 - acc: 0.9662 - val_loss: 0.4611 - val_acc: 0.9102\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 135/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2344 - acc: 0.9688\n",
      "Epoch 00135: val_acc did not improve from 0.91930\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.2343 - acc: 0.9688 - val_loss: 0.5194 - val_acc: 0.9003\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 136/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2385 - acc: 0.9676\n",
      "Epoch 00136: val_acc did not improve from 0.91930\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.2387 - acc: 0.9676 - val_loss: 0.4503 - val_acc: 0.9130\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 137/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2153 - acc: 0.9766\n",
      "Epoch 00137: val_acc improved from 0.91930 to 0.92870, saving model to model_ckpt_dropactivation_best_drop-activation.h5\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.2153 - acc: 0.9766 - val_loss: 0.3974 - val_acc: 0.9287\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 138/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.2009 - acc: 0.9809\n",
      "Epoch 00138: val_acc improved from 0.92870 to 0.92990, saving model to model_ckpt_dropactivation_best_drop-activation.h5\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.2009 - acc: 0.9810 - val_loss: 0.3941 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 139/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1955 - acc: 0.9829\n",
      "Epoch 00139: val_acc improved from 0.92990 to 0.93230, saving model to model_ckpt_dropactivation_best_drop-activation.h5\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.1956 - acc: 0.9828 - val_loss: 0.3919 - val_acc: 0.9323\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 140/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1905 - acc: 0.9848\n",
      "Epoch 00140: val_acc did not improve from 0.93230\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1904 - acc: 0.9848 - val_loss: 0.3930 - val_acc: 0.9321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 141/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1883 - acc: 0.9858\n",
      "Epoch 00141: val_acc improved from 0.93230 to 0.93270, saving model to model_ckpt_dropactivation_best_drop-activation.h5\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.1885 - acc: 0.9858 - val_loss: 0.3965 - val_acc: 0.9327\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 142/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1859 - acc: 0.9862\n",
      "Epoch 00142: val_acc did not improve from 0.93270\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1858 - acc: 0.9862 - val_loss: 0.3961 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 143/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1859 - acc: 0.9866\n",
      "Epoch 00143: val_acc did not improve from 0.93270\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1858 - acc: 0.9866 - val_loss: 0.4015 - val_acc: 0.9317\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 144/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1835 - acc: 0.9871\n",
      "Epoch 00144: val_acc did not improve from 0.93270\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1835 - acc: 0.9871 - val_loss: 0.3992 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 145/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1816 - acc: 0.9876\n",
      "Epoch 00145: val_acc improved from 0.93270 to 0.93290, saving model to model_ckpt_dropactivation_best_drop-activation.h5\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.1816 - acc: 0.9876 - val_loss: 0.3948 - val_acc: 0.9329\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 146/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1807 - acc: 0.9877\n",
      "Epoch 00146: val_acc did not improve from 0.93290\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1807 - acc: 0.9876 - val_loss: 0.3985 - val_acc: 0.9316\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 147/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1788 - acc: 0.9885\n",
      "Epoch 00147: val_acc did not improve from 0.93290\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1788 - acc: 0.9884 - val_loss: 0.3984 - val_acc: 0.9324\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 148/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1776 - acc: 0.9888\n",
      "Epoch 00148: val_acc did not improve from 0.93290\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1776 - acc: 0.9888 - val_loss: 0.3981 - val_acc: 0.9311\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 149/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1789 - acc: 0.9875\n",
      "Epoch 00149: val_acc did not improve from 0.93290\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1789 - acc: 0.9875 - val_loss: 0.4011 - val_acc: 0.9325\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 150/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1771 - acc: 0.9883\n",
      "Epoch 00150: val_acc did not improve from 0.93290\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1770 - acc: 0.9883 - val_loss: 0.4034 - val_acc: 0.9310\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 151/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1756 - acc: 0.9890\n",
      "Epoch 00151: val_acc did not improve from 0.93290\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1757 - acc: 0.9890 - val_loss: 0.4056 - val_acc: 0.9326\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 152/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1733 - acc: 0.9897\n",
      "Epoch 00152: val_acc did not improve from 0.93290\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1734 - acc: 0.9897 - val_loss: 0.4095 - val_acc: 0.9303\n",
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 153/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1750 - acc: 0.9885\n",
      "Epoch 00153: val_acc improved from 0.93290 to 0.93330, saving model to model_ckpt_dropactivation_best_drop-activation.h5\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.1751 - acc: 0.9884 - val_loss: 0.4021 - val_acc: 0.9333\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 154/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1721 - acc: 0.9897\n",
      "Epoch 00154: val_acc did not improve from 0.93330\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1723 - acc: 0.9896 - val_loss: 0.4064 - val_acc: 0.9317\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 155/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1719 - acc: 0.9895\n",
      "Epoch 00155: val_acc did not improve from 0.93330\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1719 - acc: 0.9895 - val_loss: 0.4062 - val_acc: 0.9307\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 156/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1713 - acc: 0.9898\n",
      "Epoch 00156: val_acc did not improve from 0.93330\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1712 - acc: 0.9898 - val_loss: 0.4106 - val_acc: 0.9305\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 157/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1697 - acc: 0.9903\n",
      "Epoch 00157: val_acc did not improve from 0.93330\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1699 - acc: 0.9902 - val_loss: 0.4114 - val_acc: 0.9303\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 158/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1689 - acc: 0.9906\n",
      "Epoch 00158: val_acc did not improve from 0.93330\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1689 - acc: 0.9906 - val_loss: 0.4097 - val_acc: 0.9310\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 159/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1708 - acc: 0.9890\n",
      "Epoch 00159: val_acc did not improve from 0.93330\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1709 - acc: 0.9890 - val_loss: 0.4139 - val_acc: 0.9297\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 160/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1689 - acc: 0.9903\n",
      "Epoch 00160: val_acc did not improve from 0.93330\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1689 - acc: 0.9903 - val_loss: 0.4096 - val_acc: 0.9305\n",
      "\n",
      "Epoch 00161: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 161/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1679 - acc: 0.9904\n",
      "Epoch 00161: val_acc did not improve from 0.93330\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1679 - acc: 0.9904 - val_loss: 0.4094 - val_acc: 0.9323\n",
      "\n",
      "Epoch 00162: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 162/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1678 - acc: 0.9902\n",
      "Epoch 00162: val_acc did not improve from 0.93330\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1679 - acc: 0.9902 - val_loss: 0.4067 - val_acc: 0.9322\n",
      "\n",
      "Epoch 00163: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 163/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1663 - acc: 0.9906\n",
      "Epoch 00163: val_acc did not improve from 0.93330\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1663 - acc: 0.9906 - val_loss: 0.4108 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00164: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 164/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1681 - acc: 0.9896\n",
      "Epoch 00164: val_acc did not improve from 0.93330\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1681 - acc: 0.9896 - val_loss: 0.4080 - val_acc: 0.9324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00165: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 165/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1643 - acc: 0.9913\n",
      "Epoch 00165: val_acc did not improve from 0.93330\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1643 - acc: 0.9912 - val_loss: 0.4151 - val_acc: 0.9305\n",
      "\n",
      "Epoch 00166: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 166/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1661 - acc: 0.9909\n",
      "Epoch 00166: val_acc did not improve from 0.93330\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1661 - acc: 0.9909 - val_loss: 0.4098 - val_acc: 0.9324\n",
      "\n",
      "Epoch 00167: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 167/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1648 - acc: 0.9913\n",
      "Epoch 00167: val_acc did not improve from 0.93330\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1651 - acc: 0.9912 - val_loss: 0.4162 - val_acc: 0.9302\n",
      "\n",
      "Epoch 00168: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 168/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1648 - acc: 0.9908\n",
      "Epoch 00168: val_acc did not improve from 0.93330\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1647 - acc: 0.9908 - val_loss: 0.4129 - val_acc: 0.9308\n",
      "\n",
      "Epoch 00169: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 169/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1642 - acc: 0.9902\n",
      "Epoch 00169: val_acc did not improve from 0.93330\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1644 - acc: 0.9901 - val_loss: 0.4126 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00170: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 170/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1625 - acc: 0.9913\n",
      "Epoch 00170: val_acc improved from 0.93330 to 0.93360, saving model to model_ckpt_dropactivation_best_drop-activation.h5\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.1624 - acc: 0.9913 - val_loss: 0.4119 - val_acc: 0.9336\n",
      "\n",
      "Epoch 00171: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 171/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1625 - acc: 0.9913\n",
      "Epoch 00171: val_acc did not improve from 0.93360\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1625 - acc: 0.9913 - val_loss: 0.4145 - val_acc: 0.9306\n",
      "\n",
      "Epoch 00172: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 172/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1603 - acc: 0.9923\n",
      "Epoch 00172: val_acc did not improve from 0.93360\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1605 - acc: 0.9923 - val_loss: 0.4146 - val_acc: 0.9306\n",
      "\n",
      "Epoch 00173: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 173/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1618 - acc: 0.9911\n",
      "Epoch 00173: val_acc did not improve from 0.93360\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1619 - acc: 0.9911 - val_loss: 0.4111 - val_acc: 0.9305\n",
      "\n",
      "Epoch 00174: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 174/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1606 - acc: 0.9914\n",
      "Epoch 00174: val_acc did not improve from 0.93360\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1606 - acc: 0.9914 - val_loss: 0.4156 - val_acc: 0.9320\n",
      "\n",
      "Epoch 00175: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 175/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1613 - acc: 0.9913\n",
      "Epoch 00175: val_acc did not improve from 0.93360\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1613 - acc: 0.9913 - val_loss: 0.4217 - val_acc: 0.9291\n",
      "\n",
      "Epoch 00176: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 176/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1617 - acc: 0.9906\n",
      "Epoch 00176: val_acc did not improve from 0.93360\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1617 - acc: 0.9905 - val_loss: 0.4191 - val_acc: 0.9321\n",
      "\n",
      "Epoch 00177: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 177/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1603 - acc: 0.9915\n",
      "Epoch 00177: val_acc did not improve from 0.93360\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1603 - acc: 0.9915 - val_loss: 0.4175 - val_acc: 0.9317\n",
      "\n",
      "Epoch 00178: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 178/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1588 - acc: 0.9918\n",
      "Epoch 00178: val_acc did not improve from 0.93360\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1589 - acc: 0.9917 - val_loss: 0.4207 - val_acc: 0.9303\n",
      "\n",
      "Epoch 00179: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 179/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1589 - acc: 0.9916\n",
      "Epoch 00179: val_acc did not improve from 0.93360\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1588 - acc: 0.9916 - val_loss: 0.4188 - val_acc: 0.9318\n",
      "\n",
      "Epoch 00180: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 180/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1571 - acc: 0.9919\n",
      "Epoch 00180: val_acc did not improve from 0.93360\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1574 - acc: 0.9919 - val_loss: 0.4141 - val_acc: 0.9300\n",
      "\n",
      "Epoch 00181: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 181/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1575 - acc: 0.9918\n",
      "Epoch 00181: val_acc did not improve from 0.93360\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1576 - acc: 0.9918 - val_loss: 0.4162 - val_acc: 0.9307\n",
      "\n",
      "Epoch 00182: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 182/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1557 - acc: 0.9921\n",
      "Epoch 00182: val_acc did not improve from 0.93360\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1557 - acc: 0.9922 - val_loss: 0.4192 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00183: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 183/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1553 - acc: 0.9923\n",
      "Epoch 00183: val_acc did not improve from 0.93360\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1553 - acc: 0.9923 - val_loss: 0.4171 - val_acc: 0.9307\n",
      "\n",
      "Epoch 00184: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 184/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1545 - acc: 0.9926\n",
      "Epoch 00184: val_acc did not improve from 0.93360\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1545 - acc: 0.9926 - val_loss: 0.4158 - val_acc: 0.9308\n",
      "\n",
      "Epoch 00185: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 185/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1560 - acc: 0.9923\n",
      "Epoch 00185: val_acc did not improve from 0.93360\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1559 - acc: 0.9923 - val_loss: 0.4161 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00186: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 186/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1546 - acc: 0.9927\n",
      "Epoch 00186: val_acc did not improve from 0.93360\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1546 - acc: 0.9927 - val_loss: 0.4156 - val_acc: 0.9316\n",
      "\n",
      "Epoch 00187: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 187/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1547 - acc: 0.9927\n",
      "Epoch 00187: val_acc did not improve from 0.93360\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1547 - acc: 0.9927 - val_loss: 0.4154 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00188: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 188/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1538 - acc: 0.9933\n",
      "Epoch 00188: val_acc did not improve from 0.93360\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1538 - acc: 0.9934 - val_loss: 0.4153 - val_acc: 0.9322\n",
      "\n",
      "Epoch 00189: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 189/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/313 [============================>.] - ETA: 0s - loss: 0.1542 - acc: 0.9931\n",
      "Epoch 00189: val_acc did not improve from 0.93360\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1542 - acc: 0.9931 - val_loss: 0.4146 - val_acc: 0.9322\n",
      "\n",
      "Epoch 00190: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 190/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1550 - acc: 0.9925\n",
      "Epoch 00190: val_acc did not improve from 0.93360\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1550 - acc: 0.9925 - val_loss: 0.4144 - val_acc: 0.9324\n",
      "\n",
      "Epoch 00191: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 191/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1530 - acc: 0.9932\n",
      "Epoch 00191: val_acc did not improve from 0.93360\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1530 - acc: 0.9932 - val_loss: 0.4138 - val_acc: 0.9322\n",
      "\n",
      "Epoch 00192: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 192/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1556 - acc: 0.9921\n",
      "Epoch 00192: val_acc did not improve from 0.93360\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1556 - acc: 0.9921 - val_loss: 0.4142 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00193: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 193/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1538 - acc: 0.9931\n",
      "Epoch 00193: val_acc did not improve from 0.93360\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1537 - acc: 0.9932 - val_loss: 0.4139 - val_acc: 0.9320\n",
      "\n",
      "Epoch 00194: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 194/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1556 - acc: 0.9922\n",
      "Epoch 00194: val_acc did not improve from 0.93360\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1556 - acc: 0.9922 - val_loss: 0.4133 - val_acc: 0.9323\n",
      "\n",
      "Epoch 00195: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 195/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1546 - acc: 0.9923\n",
      "Epoch 00195: val_acc did not improve from 0.93360\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1545 - acc: 0.9923 - val_loss: 0.4130 - val_acc: 0.9325\n",
      "\n",
      "Epoch 00196: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 196/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1526 - acc: 0.9934\n",
      "Epoch 00196: val_acc did not improve from 0.93360\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1526 - acc: 0.9934 - val_loss: 0.4135 - val_acc: 0.9321\n",
      "\n",
      "Epoch 00197: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 197/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1533 - acc: 0.9928\n",
      "Epoch 00197: val_acc did not improve from 0.93360\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1533 - acc: 0.9927 - val_loss: 0.4142 - val_acc: 0.9327\n",
      "\n",
      "Epoch 00198: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 198/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1537 - acc: 0.9930\n",
      "Epoch 00198: val_acc did not improve from 0.93360\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1537 - acc: 0.9930 - val_loss: 0.4133 - val_acc: 0.9319\n",
      "\n",
      "Epoch 00199: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 199/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1529 - acc: 0.9931\n",
      "Epoch 00199: val_acc did not improve from 0.93360\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1529 - acc: 0.9931 - val_loss: 0.4135 - val_acc: 0.9324\n",
      "\n",
      "Epoch 00200: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 200/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.1533 - acc: 0.9928\n",
      "Epoch 00200: val_acc did not improve from 0.93360\n",
      "313/313 [==============================] - 35s 113ms/step - loss: 0.1533 - acc: 0.9928 - val_loss: 0.4135 - val_acc: 0.9327\n",
      "CPU times: user 1h 57min 17s, sys: 19min 9s, total: 2h 16min 27s\n",
      "Wall time: 1h 58min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "history = model.fit_generator(iterator_train_aug, \n",
    "                              steps_per_epoch=steps_per_epoch_train,\n",
    "                              epochs=EPOCHS,\n",
    "                              verbose=1,\n",
    "                              validation_data=iterator_valid,\n",
    "                              validation_steps=steps_per_epoch_val,\n",
    "                              callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('drop-activation-history.dict', 'wb') as f:\n",
    "    pickle.dump(history.history, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['val_acc', 'loss', 'val_loss', 'acc']\n"
     ]
    }
   ],
   "source": [
    "hist_dict = history.history\n",
    "print(list(hist_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "model.load_weights('model_ckpt_dropactivation_best_drop-activation.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best epoch : 169  |  0.9336\n"
     ]
    }
   ],
   "source": [
    "best_epoch = np.argmax(hist_dict['val_acc'])\n",
    "print(\"best epoch : {}  |  {}\".format(best_epoch, hist_dict['val_acc'][best_epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 2s 23ms/step\n",
      "ACC (test) :  0.9327\n",
      "LOSS (test) :  0.4072032731056213\n"
     ]
    }
   ],
   "source": [
    "loss_test, acc_test = model.evaluate_generator(generator=iterator_test, steps=steps_per_epoch_test, verbose=1)\n",
    "print(\"ACC (test) : \", acc_test)\n",
    "print(\"LOSS (test) : \", loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fad252d2358>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "plt.plot(list(range(EPOCHS)), hist_dict['acc'], color='red', label='train acc')\n",
    "plt.plot(list(range(EPOCHS)), hist_dict['val_acc'], color='green', label='val acc')\n",
    "plt.scatter([best_epoch, ], [acc_test, ], color='blue', label='test acc')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fad65f435c0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "plt.plot(list(range(EPOCHS)), hist_dict['loss'], color='red', label='train loss')\n",
    "plt.plot(list(range(EPOCHS)), hist_dict['val_loss'], color='green', label='val loss')\n",
    "plt.scatter([best_epoch, ], [loss_test, ], color='blue', label='test loss')\n",
    "plt.legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
