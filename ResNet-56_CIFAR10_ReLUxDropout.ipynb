{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pprint import pprint\n",
    "from math import ceil\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version :  1.10.0\n",
      "Keras version :  2.1.6-tf\n",
      "Built with CUDA :  True\n",
      "Available GPU :  True\n",
      "keras data_format :  channels_first\n"
     ]
    }
   ],
   "source": [
    "print(\"TF version : \", tf.__version__)\n",
    "print(\"Keras version : \", keras.__version__)\n",
    "with_cuda = tf.test.is_built_with_cuda()\n",
    "with_gpu = tf.test.is_gpu_available()\n",
    "print(\"Built with CUDA : \", with_cuda)\n",
    "print(\"Available GPU : \", with_gpu)\n",
    "\n",
    "if with_cuda and with_gpu:\n",
    "    keras.backend.set_image_data_format('channels_first')\n",
    "else: \n",
    "    keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "print(\"keras data_format : \", keras.backend.image_data_format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 200\n",
    "INIT_LR = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading CIFAR10 dataset ...\n",
      "\tTRAIN - images (40000, 3, 32, 32) | float32  - labels (40000, 10) - float32\n",
      "\tVAL - images (10000, 3, 32, 32) | float32  - labels (10000, 10) - float32\n",
      "\tTEST - images (10000, 3, 32, 32) | float32  - labels (10000, 10) - float32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"... loading CIFAR10 dataset ...\")\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "y_train = np.squeeze(y_train)\n",
    "y_test = np.squeeze(y_test)\n",
    "\n",
    "x_train, y_train = shuffle(x_train, y_train, random_state=51)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train,\n",
    "                                                  test_size=0.2,\n",
    "                                                  stratify=y_train,\n",
    "                                                  random_state=51)\n",
    "# cast samples and labels\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_val = x_val.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)\n",
    "y_train = keras.utils.to_categorical(y_train.astype(np.int32), num_classes=10)\n",
    "y_val = keras.utils.to_categorical(y_val.astype(np.int32), num_classes=10)\n",
    "y_test = keras.utils.to_categorical(y_test.astype(np.int32), num_classes=10)\n",
    "\n",
    "print(\"\\tTRAIN - images {} | {}  - labels {} - {}\".format(x_train.shape, x_train.dtype, y_train.shape, y_train.dtype))\n",
    "print(\"\\tVAL - images {} | {}  - labels {} - {}\".format(x_val.shape, x_val.dtype, y_val.shape, y_val.dtype))\n",
    "print(\"\\tTEST - images {} | {}  - labels {} - {}\\n\".format(x_test.shape, x_test.dtype, y_test.shape, y_test.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_aug = keras.preprocessing.image.ImageDataGenerator(samplewise_center=True,\n",
    "                                                             samplewise_std_normalization=True,\n",
    "                                                             width_shift_range=5,\n",
    "                                                             height_shift_range=5,\n",
    "                                                             fill_mode='constant',\n",
    "                                                             cval=0.0,\n",
    "                                                             horizontal_flip=True,\n",
    "                                                             vertical_flip=False,\n",
    "                                                             data_format=keras.backend.image_data_format())\n",
    "\n",
    "generator = keras.preprocessing.image.ImageDataGenerator(samplewise_center=True,\n",
    "                                                         samplewise_std_normalization=True,\n",
    "                                                         data_format=keras.backend.image_data_format())\n",
    "\n",
    "# python iterator object that yields augmented samples \n",
    "iterator_train_aug = generator_aug.flow(x_train, y_train, batch_size=BATCH_SIZE)\n",
    "\n",
    "# python iterators object that yields not augmented samples \n",
    "iterator_train = generator.flow(x_train, y_train, batch_size=BATCH_SIZE)\n",
    "iterator_valid = generator.flow(x_val, y_val, batch_size=BATCH_SIZE)\n",
    "iterator_test = generator.flow(x_test, y_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "steps_per_epoch_train = int(ceil(iterator_train.n/BATCH_SIZE))\n",
    "steps_per_epoch_val = int(ceil(iterator_valid.n/BATCH_SIZE))\n",
    "steps_per_epoch_test = int(ceil(iterator_test.n/BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : (128, 3, 32, 32) | float32\n",
      "y : (128, 10) | float32\n"
     ]
    }
   ],
   "source": [
    "# test iterator with data augmentation\n",
    "x, y = iterator_train_aug.next()\n",
    "\n",
    "print(\"x : {} | {}\".format(x.shape, x.dtype))\n",
    "print(\"y : {} | {}\".format(y.shape, y.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resnet import ResNet56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/zenith/miniconda3/envs/dl-1.10-gpu/lib/python3.5/site-packages/tensorflow/python/keras/initializers.py:104: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`normal` is a deprecated alias for `truncated_normal`\n"
     ]
    }
   ],
   "source": [
    "shape = [3, 32, 32] if keras.backend.image_data_format()=='channels_first' else [32, 32, 3]\n",
    "model = ResNet56(input_shape=shape, classes=10, activation='relu-dropout', rate=0.4).build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 3, 32, 32)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 16, 30, 30)   432         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 16, 30, 30)   64          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 16, 30, 30)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 16, 30, 30)   0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 30, 30)   2304        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 16, 30, 30)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 16, 30, 30)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 16, 30, 30)   0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 16, 30, 30)   2304        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 16, 30, 30)   256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 16, 30, 30)   0           conv2d_3[0][0]                   \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16, 30, 30)   64          add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 16, 30, 30)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 16, 30, 30)   0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 16, 30, 30)   2304        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 16, 30, 30)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 16, 30, 30)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16, 30, 30)   0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 16, 30, 30)   2304        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 16, 30, 30)   0           conv2d_5[0][0]                   \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 16, 30, 30)   64          add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 16, 30, 30)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 16, 30, 30)   0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 30, 30)   2304        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 16, 30, 30)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 16, 30, 30)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 16, 30, 30)   0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 30, 30)   2304        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 16, 30, 30)   0           conv2d_7[0][0]                   \n",
      "                                                                 add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 30, 30)   64          add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 16, 30, 30)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 16, 30, 30)   0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 30, 30)   2304        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 30, 30)   64          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 16, 30, 30)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 30, 30)   0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 30, 30)   2304        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 16, 30, 30)   0           conv2d_9[0][0]                   \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 30, 30)   64          add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 16, 30, 30)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 30, 30)   0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 30, 30)   2304        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 30, 30)   64          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 16, 30, 30)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 30, 30)   0           dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 30, 30)   2304        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 30, 30)   0           conv2d_11[0][0]                  \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 30, 30)   64          add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 16, 30, 30)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 30, 30)   0           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 30, 30)   2304        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 30, 30)   64          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 16, 30, 30)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 30, 30)   0           dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 30, 30)   2304        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 30, 30)   0           conv2d_13[0][0]                  \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 30, 30)   64          add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 16, 30, 30)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 30, 30)   0           dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 30, 30)   2304        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 30, 30)   64          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 16, 30, 30)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 30, 30)   0           dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 30, 30)   2304        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 30, 30)   0           conv2d_15[0][0]                  \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 30, 30)   64          add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 16, 30, 30)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 30, 30)   0           dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 30, 30)   2304        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 30, 30)   64          conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 16, 30, 30)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 30, 30)   0           dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 30, 30)   2304        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 16, 30, 30)   0           conv2d_17[0][0]                  \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 30, 30)   64          add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 16, 30, 30)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 30, 30)   0           dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 30, 30)   2304        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 30, 30)   64          conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 16, 30, 30)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 30, 30)   0           dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 16, 30, 30)   2304        activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 16, 30, 30)   0           conv2d_19[0][0]                  \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 16, 30, 30)   64          add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 16, 30, 30)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16, 30, 30)   0           dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 16, 32, 32)   0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 32, 15, 15)   4608        zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 32, 15, 15)   128         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 32, 15, 15)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 32, 15, 15)   0           dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 32, 15, 15)   9216        activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 32, 15, 15)   512         add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 32, 15, 15)   0           conv2d_22[0][0]                  \n",
      "                                                                 conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 32, 15, 15)   128         add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 32, 15, 15)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 32, 15, 15)   0           dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 32, 15, 15)   9216        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 32, 15, 15)   128         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 32, 15, 15)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 32, 15, 15)   0           dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 32, 15, 15)   9216        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 32, 15, 15)   0           conv2d_24[0][0]                  \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 32, 15, 15)   128         add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 32, 15, 15)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 32, 15, 15)   0           dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 32, 15, 15)   9216        activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 32, 15, 15)   128         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 32, 15, 15)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 32, 15, 15)   0           dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 32, 15, 15)   9216        activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 32, 15, 15)   0           conv2d_26[0][0]                  \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 32, 15, 15)   128         add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 32, 15, 15)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 32, 15, 15)   0           dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 32, 15, 15)   9216        activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 32, 15, 15)   128         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 32, 15, 15)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 32, 15, 15)   0           dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 32, 15, 15)   9216        activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 32, 15, 15)   0           conv2d_28[0][0]                  \n",
      "                                                                 add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 32, 15, 15)   128         add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 32, 15, 15)   0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 32, 15, 15)   0           dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 32, 15, 15)   9216        activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 32, 15, 15)   128         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 32, 15, 15)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 32, 15, 15)   0           dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 32, 15, 15)   9216        activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 32, 15, 15)   0           conv2d_30[0][0]                  \n",
      "                                                                 add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 32, 15, 15)   128         add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 32, 15, 15)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 32, 15, 15)   0           dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 32, 15, 15)   9216        activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 32, 15, 15)   128         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 32, 15, 15)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 32, 15, 15)   0           dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 32, 15, 15)   9216        activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 32, 15, 15)   0           conv2d_32[0][0]                  \n",
      "                                                                 add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 32, 15, 15)   128         add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 32, 15, 15)   0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 32, 15, 15)   0           dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 32, 15, 15)   9216        activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 32, 15, 15)   128         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 32, 15, 15)   0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 32, 15, 15)   0           dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 32, 15, 15)   9216        activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 32, 15, 15)   0           conv2d_34[0][0]                  \n",
      "                                                                 add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 32, 15, 15)   128         add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 32, 15, 15)   0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 32, 15, 15)   0           dropout_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 32, 15, 15)   9216        activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 32, 15, 15)   128         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)            (None, 32, 15, 15)   0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 32, 15, 15)   0           dropout_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 32, 15, 15)   9216        activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 32, 15, 15)   0           conv2d_36[0][0]                  \n",
      "                                                                 add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 32, 15, 15)   128         add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, 32, 15, 15)   0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 32, 15, 15)   0           dropout_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 32, 15, 15)   9216        activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 32, 15, 15)   128         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, 32, 15, 15)   0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 32, 15, 15)   0           dropout_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 32, 15, 15)   9216        activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 32, 15, 15)   0           conv2d_38[0][0]                  \n",
      "                                                                 add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 32, 15, 15)   128         add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_36 (Dropout)            (None, 32, 15, 15)   0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 32, 15, 15)   0           dropout_36[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 32, 17, 17)   0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 64, 8, 8)     18432       zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 64, 8, 8)     256         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 64, 8, 8)     0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 64, 8, 8)     0           dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 64, 8, 8)     36864       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 64, 8, 8)     2048        add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 64, 8, 8)     0           conv2d_41[0][0]                  \n",
      "                                                                 conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 64, 8, 8)     256         add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 64, 8, 8)     0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 64, 8, 8)     0           dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 64, 8, 8)     36864       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 64, 8, 8)     256         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 64, 8, 8)     0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 64, 8, 8)     0           dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 64, 8, 8)     36864       activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 64, 8, 8)     0           conv2d_43[0][0]                  \n",
      "                                                                 add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 64, 8, 8)     256         add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 64, 8, 8)     0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 64, 8, 8)     0           dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 64, 8, 8)     36864       activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 64, 8, 8)     256         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 64, 8, 8)     0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 64, 8, 8)     0           dropout_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 64, 8, 8)     36864       activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 64, 8, 8)     0           conv2d_45[0][0]                  \n",
      "                                                                 add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 64, 8, 8)     256         add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 64, 8, 8)     0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 64, 8, 8)     0           dropout_42[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 64, 8, 8)     36864       activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 64, 8, 8)     256         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, 64, 8, 8)     0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 64, 8, 8)     0           dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 64, 8, 8)     36864       activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 64, 8, 8)     0           conv2d_47[0][0]                  \n",
      "                                                                 add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 64, 8, 8)     256         add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_44 (Dropout)            (None, 64, 8, 8)     0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 64, 8, 8)     0           dropout_44[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 64, 8, 8)     36864       activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 64, 8, 8)     256         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_45 (Dropout)            (None, 64, 8, 8)     0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 64, 8, 8)     0           dropout_45[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 64, 8, 8)     36864       activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 64, 8, 8)     0           conv2d_49[0][0]                  \n",
      "                                                                 add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 64, 8, 8)     256         add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_46 (Dropout)            (None, 64, 8, 8)     0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 64, 8, 8)     0           dropout_46[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 64, 8, 8)     36864       activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 64, 8, 8)     256         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_47 (Dropout)            (None, 64, 8, 8)     0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 64, 8, 8)     0           dropout_47[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 64, 8, 8)     36864       activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 64, 8, 8)     0           conv2d_51[0][0]                  \n",
      "                                                                 add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 64, 8, 8)     256         add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_48 (Dropout)            (None, 64, 8, 8)     0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 64, 8, 8)     0           dropout_48[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 64, 8, 8)     36864       activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 64, 8, 8)     256         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)            (None, 64, 8, 8)     0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 64, 8, 8)     0           dropout_49[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 64, 8, 8)     36864       activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 64, 8, 8)     0           conv2d_53[0][0]                  \n",
      "                                                                 add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 64, 8, 8)     256         add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)            (None, 64, 8, 8)     0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 64, 8, 8)     0           dropout_50[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 64, 8, 8)     36864       activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 64, 8, 8)     256         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_51 (Dropout)            (None, 64, 8, 8)     0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 64, 8, 8)     0           dropout_51[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 64, 8, 8)     36864       activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 64, 8, 8)     0           conv2d_55[0][0]                  \n",
      "                                                                 add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 64, 8, 8)     256         add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_52 (Dropout)            (None, 64, 8, 8)     0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 64, 8, 8)     0           dropout_52[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 64, 8, 8)     36864       activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 64, 8, 8)     256         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_53 (Dropout)            (None, 64, 8, 8)     0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 64, 8, 8)     0           dropout_53[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 64, 8, 8)     36864       activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 64, 8, 8)     0           conv2d_57[0][0]                  \n",
      "                                                                 add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 64, 8, 8)     256         add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_54 (Dropout)            (None, 64, 8, 8)     0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 64, 8, 8)     0           dropout_54[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 64)           0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10)           650         avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 859,898\n",
      "Trainable params: 855,834\n",
      "Non-trainable params: 4,064\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=INIT_LR, momentum=0.9)\n",
    "loss = 'categorical_crossentropy'\n",
    "metrics = ['acc', ]\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_weights('random_weights.h5')\n",
    "model.load_weights('random_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "\n",
    "model_ckpt = keras.callbacks.ModelCheckpoint(\"model_ckpt_dropactivation_best_relu-dropout.h5\",\n",
    "                                             monitor='val_acc', verbose=1, save_best_only=True, \n",
    "                                             save_weights_only=True)\n",
    "callbacks.append(model_ckpt)\n",
    "\n",
    "def schedule(epoch):\n",
    "    if epoch < 91:\n",
    "        return INIT_LR\n",
    "    if epoch < 136:\n",
    "        return 0.1*INIT_LR\n",
    "    if epoch < 182:\n",
    "        return 0.01*INIT_LR\n",
    "    else:\n",
    "        return 0.001*INIT_LR\n",
    "    \n",
    "lr_schedule = keras.callbacks.LearningRateScheduler(schedule, verbose=1)\n",
    "callbacks.append(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 1/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 2.6256 - acc: 0.2977\n",
      "Epoch 00001: val_acc improved from -inf to 0.16740, saving model to model_ckpt_dropactivation_best_relu-dropout.h5\n",
      "313/313 [==============================] - 50s 159ms/step - loss: 2.6250 - acc: 0.2979 - val_loss: 5.5074 - val_acc: 0.1674\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 2/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 2.1427 - acc: 0.4362\n",
      "Epoch 00002: val_acc improved from 0.16740 to 0.41790, saving model to model_ckpt_dropactivation_best_relu-dropout.h5\n",
      "313/313 [==============================] - 31s 98ms/step - loss: 2.1424 - acc: 0.4363 - val_loss: 2.1714 - val_acc: 0.4179\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 3/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.8456 - acc: 0.5135\n",
      "Epoch 00003: val_acc did not improve from 0.41790\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 1.8452 - acc: 0.5134 - val_loss: 2.5667 - val_acc: 0.3613\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 4/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.6486 - acc: 0.5555\n",
      "Epoch 00004: val_acc improved from 0.41790 to 0.44070, saving model to model_ckpt_dropactivation_best_relu-dropout.h5\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 1.6486 - acc: 0.5556 - val_loss: 2.0399 - val_acc: 0.4407\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 5/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.5041 - acc: 0.5910\n",
      "Epoch 00005: val_acc did not improve from 0.44070\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 1.5040 - acc: 0.5912 - val_loss: 2.5859 - val_acc: 0.3591\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 6/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.3933 - acc: 0.6202\n",
      "Epoch 00006: val_acc improved from 0.44070 to 0.46470, saving model to model_ckpt_dropactivation_best_relu-dropout.h5\n",
      "313/313 [==============================] - 32s 101ms/step - loss: 1.3935 - acc: 0.6200 - val_loss: 2.1266 - val_acc: 0.4647\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 7/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.3122 - acc: 0.6360\n",
      "Epoch 00007: val_acc improved from 0.46470 to 0.50530, saving model to model_ckpt_dropactivation_best_relu-dropout.h5\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 1.3122 - acc: 0.6360 - val_loss: 1.8507 - val_acc: 0.5053\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 8/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.2610 - acc: 0.6508\n",
      "Epoch 00008: val_acc improved from 0.50530 to 0.56720, saving model to model_ckpt_dropactivation_best_relu-dropout.h5\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 1.2605 - acc: 0.6510 - val_loss: 1.5577 - val_acc: 0.5672\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 9/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.2056 - acc: 0.6652\n",
      "Epoch 00009: val_acc improved from 0.56720 to 0.59060, saving model to model_ckpt_dropactivation_best_relu-dropout.h5\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 1.2051 - acc: 0.6653 - val_loss: 1.4335 - val_acc: 0.5906\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 10/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.1687 - acc: 0.6742\n",
      "Epoch 00010: val_acc improved from 0.59060 to 0.68140, saving model to model_ckpt_dropactivation_best_relu-dropout.h5\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 1.1685 - acc: 0.6743 - val_loss: 1.1833 - val_acc: 0.6814\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 11/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.1314 - acc: 0.6855\n",
      "Epoch 00011: val_acc did not improve from 0.68140\n",
      "313/313 [==============================] - 31s 99ms/step - loss: 1.1311 - acc: 0.6857 - val_loss: 1.3834 - val_acc: 0.6027\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 12/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.1174 - acc: 0.6892\n",
      "Epoch 00012: val_acc did not improve from 0.68140\n",
      "313/313 [==============================] - 31s 99ms/step - loss: 1.1179 - acc: 0.6892 - val_loss: 1.3157 - val_acc: 0.6257\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 13/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.0916 - acc: 0.7004\n",
      "Epoch 00013: val_acc did not improve from 0.68140\n",
      "313/313 [==============================] - 32s 101ms/step - loss: 1.0914 - acc: 0.7004 - val_loss: 1.4218 - val_acc: 0.6298\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 14/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.0714 - acc: 0.7071\n",
      "Epoch 00014: val_acc did not improve from 0.68140\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 1.0714 - acc: 0.7071 - val_loss: 1.3217 - val_acc: 0.6362\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 15/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.0499 - acc: 0.7187\n",
      "Epoch 00015: val_acc did not improve from 0.68140\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 1.0502 - acc: 0.7187 - val_loss: 1.2670 - val_acc: 0.6783\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 16/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.0350 - acc: 0.7233\n",
      "Epoch 00016: val_acc did not improve from 0.68140\n",
      "313/313 [==============================] - 31s 98ms/step - loss: 1.0347 - acc: 0.7235 - val_loss: 1.6826 - val_acc: 0.5431\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 17/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.0168 - acc: 0.7331\n",
      "Epoch 00017: val_acc did not improve from 0.68140\n",
      "313/313 [==============================] - 31s 98ms/step - loss: 1.0168 - acc: 0.7331 - val_loss: 1.3551 - val_acc: 0.6539\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 18/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 1.0041 - acc: 0.7401\n",
      "Epoch 00018: val_acc did not improve from 0.68140\n",
      "313/313 [==============================] - 31s 99ms/step - loss: 1.0045 - acc: 0.7400 - val_loss: 1.2948 - val_acc: 0.6616\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 19/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9974 - acc: 0.7435\n",
      "Epoch 00019: val_acc did not improve from 0.68140\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 0.9972 - acc: 0.7436 - val_loss: 1.6088 - val_acc: 0.5903\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 20/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9783 - acc: 0.7531\n",
      "Epoch 00020: val_acc did not improve from 0.68140\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 0.9782 - acc: 0.7532 - val_loss: 1.7082 - val_acc: 0.5814\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 21/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9814 - acc: 0.7517\n",
      "Epoch 00021: val_acc improved from 0.68140 to 0.73990, saving model to model_ckpt_dropactivation_best_relu-dropout.h5\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 0.9811 - acc: 0.7517 - val_loss: 1.0571 - val_acc: 0.7399\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 22/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9777 - acc: 0.7569\n",
      "Epoch 00022: val_acc did not improve from 0.73990\n",
      "313/313 [==============================] - 31s 99ms/step - loss: 0.9775 - acc: 0.7570 - val_loss: 1.3110 - val_acc: 0.6765\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 23/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9750 - acc: 0.7568\n",
      "Epoch 00023: val_acc did not improve from 0.73990\n",
      "313/313 [==============================] - 31s 98ms/step - loss: 0.9748 - acc: 0.7569 - val_loss: 1.4113 - val_acc: 0.6612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 24/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9634 - acc: 0.7632\n",
      "Epoch 00024: val_acc did not improve from 0.73990\n",
      "313/313 [==============================] - 31s 99ms/step - loss: 0.9635 - acc: 0.7632 - val_loss: 1.0662 - val_acc: 0.7361\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 25/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9660 - acc: 0.7629\n",
      "Epoch 00025: val_acc improved from 0.73990 to 0.74620, saving model to model_ckpt_dropactivation_best_relu-dropout.h5\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 0.9663 - acc: 0.7628 - val_loss: 1.0660 - val_acc: 0.7462\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 26/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9550 - acc: 0.7685\n",
      "Epoch 00026: val_acc did not improve from 0.74620\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 0.9556 - acc: 0.7685 - val_loss: 1.3041 - val_acc: 0.6803\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 27/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9548 - acc: 0.7700\n",
      "Epoch 00027: val_acc did not improve from 0.74620\n",
      "313/313 [==============================] - 31s 99ms/step - loss: 0.9546 - acc: 0.7699 - val_loss: 1.3973 - val_acc: 0.6607\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 28/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9561 - acc: 0.7700\n",
      "Epoch 00028: val_acc improved from 0.74620 to 0.78580, saving model to model_ckpt_dropactivation_best_relu-dropout.h5\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 0.9558 - acc: 0.7701 - val_loss: 0.9418 - val_acc: 0.7858\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 29/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9622 - acc: 0.7696\n",
      "Epoch 00029: val_acc did not improve from 0.78580\n",
      "313/313 [==============================] - 31s 99ms/step - loss: 0.9620 - acc: 0.7697 - val_loss: 0.9681 - val_acc: 0.7777\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 30/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9538 - acc: 0.7735\n",
      "Epoch 00030: val_acc did not improve from 0.78580\n",
      "313/313 [==============================] - 31s 99ms/step - loss: 0.9541 - acc: 0.7734 - val_loss: 0.9535 - val_acc: 0.7690\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 31/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9466 - acc: 0.7768\n",
      "Epoch 00031: val_acc did not improve from 0.78580\n",
      "313/313 [==============================] - 31s 100ms/step - loss: 0.9471 - acc: 0.7766 - val_loss: 1.0998 - val_acc: 0.7334\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 32/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9540 - acc: 0.7746\n",
      "Epoch 00032: val_acc did not improve from 0.78580\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9543 - acc: 0.7746 - val_loss: 1.0010 - val_acc: 0.7642\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 33/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9476 - acc: 0.7781\n",
      "Epoch 00033: val_acc did not improve from 0.78580\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9475 - acc: 0.7781 - val_loss: 1.0176 - val_acc: 0.7639\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 34/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9514 - acc: 0.7788\n",
      "Epoch 00034: val_acc did not improve from 0.78580\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9512 - acc: 0.7789 - val_loss: 1.0885 - val_acc: 0.7402\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 35/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9399 - acc: 0.7816\n",
      "Epoch 00035: val_acc did not improve from 0.78580\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9398 - acc: 0.7816 - val_loss: 1.0592 - val_acc: 0.7490\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 36/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9414 - acc: 0.7818\n",
      "Epoch 00036: val_acc did not improve from 0.78580\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9415 - acc: 0.7817 - val_loss: 1.1027 - val_acc: 0.7438\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 37/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9455 - acc: 0.7818\n",
      "Epoch 00037: val_acc did not improve from 0.78580\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9458 - acc: 0.7816 - val_loss: 1.0167 - val_acc: 0.7766\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 38/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9428 - acc: 0.7842\n",
      "Epoch 00038: val_acc did not improve from 0.78580\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9429 - acc: 0.7842 - val_loss: 1.4996 - val_acc: 0.6593\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 39/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9433 - acc: 0.7817\n",
      "Epoch 00039: val_acc did not improve from 0.78580\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9433 - acc: 0.7816 - val_loss: 0.9850 - val_acc: 0.7752\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 40/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9406 - acc: 0.7841\n",
      "Epoch 00040: val_acc did not improve from 0.78580\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9406 - acc: 0.7841 - val_loss: 1.3076 - val_acc: 0.6919\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 41/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9390 - acc: 0.7884\n",
      "Epoch 00041: val_acc did not improve from 0.78580\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9389 - acc: 0.7885 - val_loss: 1.4144 - val_acc: 0.6586\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 42/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9451 - acc: 0.7855\n",
      "Epoch 00042: val_acc did not improve from 0.78580\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9448 - acc: 0.7855 - val_loss: 1.0498 - val_acc: 0.7647\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 43/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9373 - acc: 0.7898\n",
      "Epoch 00043: val_acc did not improve from 0.78580\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9375 - acc: 0.7897 - val_loss: 1.1144 - val_acc: 0.7498\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 44/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9387 - acc: 0.7857\n",
      "Epoch 00044: val_acc did not improve from 0.78580\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9384 - acc: 0.7858 - val_loss: 1.0088 - val_acc: 0.7718\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 45/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9324 - acc: 0.7902\n",
      "Epoch 00045: val_acc did not improve from 0.78580\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9321 - acc: 0.7902 - val_loss: 1.1361 - val_acc: 0.7334\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 46/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9308 - acc: 0.7905\n",
      "Epoch 00046: val_acc did not improve from 0.78580\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9318 - acc: 0.7904 - val_loss: 0.9780 - val_acc: 0.7815\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 47/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9361 - acc: 0.7935\n",
      "Epoch 00047: val_acc did not improve from 0.78580\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9356 - acc: 0.7936 - val_loss: 1.0159 - val_acc: 0.7696\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 48/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/313 [============================>.] - ETA: 0s - loss: 0.9343 - acc: 0.7926\n",
      "Epoch 00048: val_acc did not improve from 0.78580\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9348 - acc: 0.7924 - val_loss: 1.0789 - val_acc: 0.7509\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 49/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9349 - acc: 0.7923\n",
      "Epoch 00049: val_acc improved from 0.78580 to 0.81280, saving model to model_ckpt_dropactivation_best_relu-dropout.h5\n",
      "313/313 [==============================] - 31s 98ms/step - loss: 0.9350 - acc: 0.7923 - val_loss: 0.8910 - val_acc: 0.8128\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 50/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9371 - acc: 0.7931\n",
      "Epoch 00050: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9375 - acc: 0.7929 - val_loss: 1.1762 - val_acc: 0.7416\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 51/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9308 - acc: 0.7957\n",
      "Epoch 00051: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9310 - acc: 0.7956 - val_loss: 1.0920 - val_acc: 0.7397\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 52/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9281 - acc: 0.7947\n",
      "Epoch 00052: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9281 - acc: 0.7948 - val_loss: 1.0323 - val_acc: 0.7776\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 53/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9321 - acc: 0.7928\n",
      "Epoch 00053: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9319 - acc: 0.7927 - val_loss: 1.1011 - val_acc: 0.7535\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 54/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9334 - acc: 0.7959\n",
      "Epoch 00054: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9329 - acc: 0.7961 - val_loss: 1.0193 - val_acc: 0.7850\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 55/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9342 - acc: 0.7931\n",
      "Epoch 00055: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9343 - acc: 0.7931 - val_loss: 1.0258 - val_acc: 0.7687\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 56/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9311 - acc: 0.7959\n",
      "Epoch 00056: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9309 - acc: 0.7961 - val_loss: 1.0348 - val_acc: 0.7732\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 57/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9322 - acc: 0.7946\n",
      "Epoch 00057: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9326 - acc: 0.7946 - val_loss: 1.0191 - val_acc: 0.7796\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 58/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9344 - acc: 0.7952\n",
      "Epoch 00058: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9339 - acc: 0.7953 - val_loss: 1.0370 - val_acc: 0.7667\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 59/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9310 - acc: 0.7970\n",
      "Epoch 00059: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9307 - acc: 0.7972 - val_loss: 1.1025 - val_acc: 0.7591\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 60/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9321 - acc: 0.7965\n",
      "Epoch 00060: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9323 - acc: 0.7966 - val_loss: 1.0117 - val_acc: 0.7923\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 61/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9317 - acc: 0.7970\n",
      "Epoch 00061: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9316 - acc: 0.7969 - val_loss: 1.0283 - val_acc: 0.7808\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 62/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9328 - acc: 0.7960\n",
      "Epoch 00062: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9322 - acc: 0.7962 - val_loss: 1.0700 - val_acc: 0.7765\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 63/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9317 - acc: 0.7974\n",
      "Epoch 00063: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9314 - acc: 0.7975 - val_loss: 1.1054 - val_acc: 0.7634\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 64/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9293 - acc: 0.7970\n",
      "Epoch 00064: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9294 - acc: 0.7970 - val_loss: 0.9985 - val_acc: 0.7793\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 65/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9271 - acc: 0.7996\n",
      "Epoch 00065: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9275 - acc: 0.7994 - val_loss: 1.0303 - val_acc: 0.7855\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 66/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9343 - acc: 0.7963\n",
      "Epoch 00066: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9340 - acc: 0.7964 - val_loss: 1.1356 - val_acc: 0.7496\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 67/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9308 - acc: 0.7960\n",
      "Epoch 00067: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9308 - acc: 0.7961 - val_loss: 0.9767 - val_acc: 0.7961\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 68/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9283 - acc: 0.7983\n",
      "Epoch 00068: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9285 - acc: 0.7980 - val_loss: 1.1339 - val_acc: 0.7522\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 69/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9334 - acc: 0.7973\n",
      "Epoch 00069: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9336 - acc: 0.7971 - val_loss: 1.1667 - val_acc: 0.7713\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 70/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9305 - acc: 0.8002\n",
      "Epoch 00070: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9303 - acc: 0.8003 - val_loss: 1.2827 - val_acc: 0.7137\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 71/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9320 - acc: 0.7988\n",
      "Epoch 00071: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9320 - acc: 0.7987 - val_loss: 0.9884 - val_acc: 0.8044\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 72/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9276 - acc: 0.8003\n",
      "Epoch 00072: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9273 - acc: 0.8005 - val_loss: 1.0100 - val_acc: 0.7848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 73/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9222 - acc: 0.8026\n",
      "Epoch 00073: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9221 - acc: 0.8027 - val_loss: 0.9406 - val_acc: 0.8091\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 74/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9306 - acc: 0.7995\n",
      "Epoch 00074: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9311 - acc: 0.7994 - val_loss: 1.0364 - val_acc: 0.7758\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 75/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9267 - acc: 0.8003\n",
      "Epoch 00075: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9266 - acc: 0.8003 - val_loss: 1.0400 - val_acc: 0.7750\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 76/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9361 - acc: 0.7981\n",
      "Epoch 00076: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9361 - acc: 0.7982 - val_loss: 0.9804 - val_acc: 0.7918\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 77/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9329 - acc: 0.8016\n",
      "Epoch 00077: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9329 - acc: 0.8016 - val_loss: 1.0577 - val_acc: 0.7634\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 78/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9278 - acc: 0.8006\n",
      "Epoch 00078: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9283 - acc: 0.8004 - val_loss: 1.0141 - val_acc: 0.7905\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 79/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9308 - acc: 0.8002\n",
      "Epoch 00079: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9310 - acc: 0.8001 - val_loss: 1.1636 - val_acc: 0.7572\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 80/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9266 - acc: 0.8032\n",
      "Epoch 00080: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9265 - acc: 0.8033 - val_loss: 0.9887 - val_acc: 0.8032\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 81/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9258 - acc: 0.8004\n",
      "Epoch 00081: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9257 - acc: 0.8003 - val_loss: 1.1244 - val_acc: 0.7480\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 82/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9341 - acc: 0.8005\n",
      "Epoch 00082: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9336 - acc: 0.8006 - val_loss: 1.0809 - val_acc: 0.7653\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 83/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9277 - acc: 0.8023\n",
      "Epoch 00083: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9276 - acc: 0.8024 - val_loss: 1.1919 - val_acc: 0.7414\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 84/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9268 - acc: 0.8032\n",
      "Epoch 00084: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9266 - acc: 0.8034 - val_loss: 1.1048 - val_acc: 0.7650\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 85/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9306 - acc: 0.8017\n",
      "Epoch 00085: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9311 - acc: 0.8015 - val_loss: 1.1164 - val_acc: 0.7604\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 86/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9262 - acc: 0.8023\n",
      "Epoch 00086: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9262 - acc: 0.8022 - val_loss: 1.1515 - val_acc: 0.7623\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 87/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9316 - acc: 0.8037\n",
      "Epoch 00087: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9319 - acc: 0.8035 - val_loss: 0.9820 - val_acc: 0.7961\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 88/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9271 - acc: 0.8029\n",
      "Epoch 00088: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9275 - acc: 0.8029 - val_loss: 1.0464 - val_acc: 0.7897\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 89/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9216 - acc: 0.8069\n",
      "Epoch 00089: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9222 - acc: 0.8068 - val_loss: 1.0331 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 90/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9334 - acc: 0.8004\n",
      "Epoch 00090: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9333 - acc: 0.8003 - val_loss: 0.9452 - val_acc: 0.8059\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 91/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.9285 - acc: 0.8040\n",
      "Epoch 00091: val_acc did not improve from 0.81280\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.9283 - acc: 0.8040 - val_loss: 1.0450 - val_acc: 0.7897\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 92/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.8364 - acc: 0.8329\n",
      "Epoch 00092: val_acc improved from 0.81280 to 0.86040, saving model to model_ckpt_dropactivation_best_relu-dropout.h5\n",
      "313/313 [==============================] - 31s 98ms/step - loss: 0.8365 - acc: 0.8329 - val_loss: 0.7754 - val_acc: 0.8604\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 93/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7668 - acc: 0.8536\n",
      "Epoch 00093: val_acc improved from 0.86040 to 0.87200, saving model to model_ckpt_dropactivation_best_relu-dropout.h5\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.7666 - acc: 0.8538 - val_loss: 0.7396 - val_acc: 0.8720\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 94/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7443 - acc: 0.8597\n",
      "Epoch 00094: val_acc did not improve from 0.87200\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.7442 - acc: 0.8596 - val_loss: 0.7671 - val_acc: 0.8590\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 95/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7290 - acc: 0.8625\n",
      "Epoch 00095: val_acc improved from 0.87200 to 0.87920, saving model to model_ckpt_dropactivation_best_relu-dropout.h5\n",
      "313/313 [==============================] - 31s 98ms/step - loss: 0.7291 - acc: 0.8624 - val_loss: 0.6986 - val_acc: 0.8792\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 96/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.7096 - acc: 0.8667\n",
      "Epoch 00096: val_acc did not improve from 0.87920\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.7095 - acc: 0.8666 - val_loss: 0.7354 - val_acc: 0.8645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 97/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6963 - acc: 0.8695\n",
      "Epoch 00097: val_acc did not improve from 0.87920\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.6964 - acc: 0.8695 - val_loss: 0.7009 - val_acc: 0.8757\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 98/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6887 - acc: 0.8687\n",
      "Epoch 00098: val_acc did not improve from 0.87920\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.6885 - acc: 0.8687 - val_loss: 0.7131 - val_acc: 0.8675\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 99/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6739 - acc: 0.8737\n",
      "Epoch 00099: val_acc did not improve from 0.87920\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.6734 - acc: 0.8739 - val_loss: 0.7010 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 100/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6627 - acc: 0.8757\n",
      "Epoch 00100: val_acc did not improve from 0.87920\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.6623 - acc: 0.8757 - val_loss: 0.6942 - val_acc: 0.8704\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 101/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6499 - acc: 0.8759\n",
      "Epoch 00101: val_acc did not improve from 0.87920\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.6504 - acc: 0.8757 - val_loss: 0.6799 - val_acc: 0.8751\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 102/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6428 - acc: 0.8769\n",
      "Epoch 00102: val_acc did not improve from 0.87920\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.6430 - acc: 0.8768 - val_loss: 0.6687 - val_acc: 0.8744\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 103/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6307 - acc: 0.8787\n",
      "Epoch 00103: val_acc improved from 0.87920 to 0.88170, saving model to model_ckpt_dropactivation_best_relu-dropout.h5\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.6304 - acc: 0.8788 - val_loss: 0.6452 - val_acc: 0.8817\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 104/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6234 - acc: 0.8813\n",
      "Epoch 00104: val_acc did not improve from 0.88170\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.6233 - acc: 0.8813 - val_loss: 0.6804 - val_acc: 0.8743\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 105/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6153 - acc: 0.8815\n",
      "Epoch 00105: val_acc did not improve from 0.88170\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.6153 - acc: 0.8815 - val_loss: 0.6485 - val_acc: 0.8799\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 106/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.6072 - acc: 0.8805\n",
      "Epoch 00106: val_acc did not improve from 0.88170\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.6073 - acc: 0.8804 - val_loss: 0.6435 - val_acc: 0.8785\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 107/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.5984 - acc: 0.8838\n",
      "Epoch 00107: val_acc did not improve from 0.88170\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.5982 - acc: 0.8839 - val_loss: 0.6478 - val_acc: 0.8741\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 108/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.5927 - acc: 0.8838\n",
      "Epoch 00108: val_acc did not improve from 0.88170\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.5925 - acc: 0.8839 - val_loss: 0.6465 - val_acc: 0.8741\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 109/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.5842 - acc: 0.8849\n",
      "Epoch 00109: val_acc improved from 0.88170 to 0.88260, saving model to model_ckpt_dropactivation_best_relu-dropout.h5\n",
      "313/313 [==============================] - 31s 98ms/step - loss: 0.5840 - acc: 0.8850 - val_loss: 0.6118 - val_acc: 0.8826\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 110/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.5812 - acc: 0.8840\n",
      "Epoch 00110: val_acc improved from 0.88260 to 0.88390, saving model to model_ckpt_dropactivation_best_relu-dropout.h5\n",
      "313/313 [==============================] - 31s 97ms/step - loss: 0.5813 - acc: 0.8839 - val_loss: 0.6077 - val_acc: 0.8839\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 111/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.5705 - acc: 0.8864\n",
      "Epoch 00111: val_acc did not improve from 0.88390\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.5706 - acc: 0.8864 - val_loss: 0.6191 - val_acc: 0.8807\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 112/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.5635 - acc: 0.8868\n",
      "Epoch 00112: val_acc did not improve from 0.88390\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.5635 - acc: 0.8869 - val_loss: 0.6314 - val_acc: 0.8775\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 113/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.5615 - acc: 0.8859\n",
      "Epoch 00113: val_acc did not improve from 0.88390\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.5609 - acc: 0.8861 - val_loss: 0.6183 - val_acc: 0.8805\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 114/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.5568 - acc: 0.8904\n",
      "Epoch 00114: val_acc improved from 0.88390 to 0.88580, saving model to model_ckpt_dropactivation_best_relu-dropout.h5\n",
      "313/313 [==============================] - 31s 98ms/step - loss: 0.5572 - acc: 0.8903 - val_loss: 0.5869 - val_acc: 0.8858\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 115/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.5481 - acc: 0.8892\n",
      "Epoch 00115: val_acc did not improve from 0.88580\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.5481 - acc: 0.8892 - val_loss: 0.6064 - val_acc: 0.8848\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 116/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.5473 - acc: 0.8872\n",
      "Epoch 00116: val_acc did not improve from 0.88580\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.5473 - acc: 0.8871 - val_loss: 0.6051 - val_acc: 0.8790\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 117/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.5431 - acc: 0.8886\n",
      "Epoch 00117: val_acc did not improve from 0.88580\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.5434 - acc: 0.8884 - val_loss: 0.6331 - val_acc: 0.8724\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 118/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.5363 - acc: 0.8901\n",
      "Epoch 00118: val_acc did not improve from 0.88580\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.5369 - acc: 0.8899 - val_loss: 0.6034 - val_acc: 0.8781\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 119/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.5302 - acc: 0.8921\n",
      "Epoch 00119: val_acc did not improve from 0.88580\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.5301 - acc: 0.8922 - val_loss: 0.6177 - val_acc: 0.8735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 120/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.5302 - acc: 0.8891\n",
      "Epoch 00120: val_acc improved from 0.88580 to 0.88960, saving model to model_ckpt_dropactivation_best_relu-dropout.h5\n",
      "313/313 [==============================] - 31s 98ms/step - loss: 0.5300 - acc: 0.8891 - val_loss: 0.5730 - val_acc: 0.8896\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 121/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.5216 - acc: 0.8930\n",
      "Epoch 00121: val_acc did not improve from 0.88960\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.5221 - acc: 0.8929 - val_loss: 0.5694 - val_acc: 0.8880\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 122/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.5246 - acc: 0.8892\n",
      "Epoch 00122: val_acc improved from 0.88960 to 0.89220, saving model to model_ckpt_dropactivation_best_relu-dropout.h5\n",
      "313/313 [==============================] - 31s 98ms/step - loss: 0.5248 - acc: 0.8893 - val_loss: 0.5554 - val_acc: 0.8922\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 123/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.5148 - acc: 0.8935\n",
      "Epoch 00123: val_acc did not improve from 0.89220\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.5154 - acc: 0.8934 - val_loss: 0.5998 - val_acc: 0.8763\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 124/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.5158 - acc: 0.8908\n",
      "Epoch 00124: val_acc did not improve from 0.89220\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.5156 - acc: 0.8909 - val_loss: 0.6145 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 125/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.5157 - acc: 0.8906\n",
      "Epoch 00125: val_acc did not improve from 0.89220\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.5156 - acc: 0.8906 - val_loss: 0.5709 - val_acc: 0.8863\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 126/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.5027 - acc: 0.8945\n",
      "Epoch 00126: val_acc did not improve from 0.89220\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.5029 - acc: 0.8943 - val_loss: 0.5969 - val_acc: 0.8803\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 127/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.5009 - acc: 0.8939\n",
      "Epoch 00127: val_acc did not improve from 0.89220\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.5014 - acc: 0.8936 - val_loss: 0.5953 - val_acc: 0.8761\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 128/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.5054 - acc: 0.8922\n",
      "Epoch 00128: val_acc did not improve from 0.89220\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.5051 - acc: 0.8923 - val_loss: 0.5862 - val_acc: 0.8794\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 129/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.5005 - acc: 0.8933\n",
      "Epoch 00129: val_acc did not improve from 0.89220\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.5002 - acc: 0.8934 - val_loss: 0.6301 - val_acc: 0.8657\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 130/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4986 - acc: 0.8934\n",
      "Epoch 00130: val_acc did not improve from 0.89220\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4985 - acc: 0.8934 - val_loss: 0.5747 - val_acc: 0.8831\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 131/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4970 - acc: 0.8925\n",
      "Epoch 00131: val_acc did not improve from 0.89220\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4971 - acc: 0.8925 - val_loss: 0.5728 - val_acc: 0.8781\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 132/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4957 - acc: 0.8913\n",
      "Epoch 00132: val_acc did not improve from 0.89220\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4961 - acc: 0.8912 - val_loss: 0.5666 - val_acc: 0.8813\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 133/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4891 - acc: 0.8941\n",
      "Epoch 00133: val_acc did not improve from 0.89220\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4890 - acc: 0.8942 - val_loss: 0.6091 - val_acc: 0.8767\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 134/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4873 - acc: 0.8950\n",
      "Epoch 00134: val_acc did not improve from 0.89220\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4875 - acc: 0.8950 - val_loss: 0.5580 - val_acc: 0.8834\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 135/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4869 - acc: 0.8954\n",
      "Epoch 00135: val_acc did not improve from 0.89220\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4869 - acc: 0.8955 - val_loss: 0.5823 - val_acc: 0.8812\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
      "Epoch 136/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4817 - acc: 0.8954\n",
      "Epoch 00136: val_acc did not improve from 0.89220\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4817 - acc: 0.8954 - val_loss: 0.5767 - val_acc: 0.8837\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 137/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4592 - acc: 0.9041\n",
      "Epoch 00137: val_acc did not improve from 0.89220\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4591 - acc: 0.9041 - val_loss: 0.5389 - val_acc: 0.8913\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 138/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4479 - acc: 0.9065\n",
      "Epoch 00138: val_acc did not improve from 0.89220\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4475 - acc: 0.9066 - val_loss: 0.5500 - val_acc: 0.8884\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 139/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4412 - acc: 0.9098\n",
      "Epoch 00139: val_acc did not improve from 0.89220\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4411 - acc: 0.9099 - val_loss: 0.5465 - val_acc: 0.8913\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 140/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4385 - acc: 0.9096\n",
      "Epoch 00140: val_acc did not improve from 0.89220\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4384 - acc: 0.9097 - val_loss: 0.5410 - val_acc: 0.8913\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 141/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4367 - acc: 0.9099\n",
      "Epoch 00141: val_acc improved from 0.89220 to 0.89390, saving model to model_ckpt_dropactivation_best_relu-dropout.h5\n",
      "313/313 [==============================] - 31s 98ms/step - loss: 0.4365 - acc: 0.9099 - val_loss: 0.5331 - val_acc: 0.8939\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 142/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4410 - acc: 0.9091\n",
      "Epoch 00142: val_acc did not improve from 0.89390\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4413 - acc: 0.9090 - val_loss: 0.5404 - val_acc: 0.8921\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 143/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/313 [============================>.] - ETA: 0s - loss: 0.4355 - acc: 0.9108\n",
      "Epoch 00143: val_acc did not improve from 0.89390\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4355 - acc: 0.9108 - val_loss: 0.5444 - val_acc: 0.8909\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 144/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4346 - acc: 0.9104\n",
      "Epoch 00144: val_acc did not improve from 0.89390\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4347 - acc: 0.9103 - val_loss: 0.5324 - val_acc: 0.8931\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 145/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4300 - acc: 0.9133\n",
      "Epoch 00145: val_acc did not improve from 0.89390\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4297 - acc: 0.9134 - val_loss: 0.5325 - val_acc: 0.8930\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 146/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4287 - acc: 0.9136\n",
      "Epoch 00146: val_acc did not improve from 0.89390\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4285 - acc: 0.9136 - val_loss: 0.5336 - val_acc: 0.8937\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 147/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4269 - acc: 0.9139\n",
      "Epoch 00147: val_acc did not improve from 0.89390\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4268 - acc: 0.9140 - val_loss: 0.5474 - val_acc: 0.8896\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 148/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4292 - acc: 0.9123\n",
      "Epoch 00148: val_acc did not improve from 0.89390\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4291 - acc: 0.9123 - val_loss: 0.5386 - val_acc: 0.8930\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 149/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4231 - acc: 0.9146\n",
      "Epoch 00149: val_acc improved from 0.89390 to 0.89490, saving model to model_ckpt_dropactivation_best_relu-dropout.h5\n",
      "313/313 [==============================] - 31s 98ms/step - loss: 0.4231 - acc: 0.9146 - val_loss: 0.5312 - val_acc: 0.8949\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 150/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4241 - acc: 0.9124\n",
      "Epoch 00150: val_acc did not improve from 0.89490\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4241 - acc: 0.9124 - val_loss: 0.5430 - val_acc: 0.8927\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 151/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4237 - acc: 0.9132\n",
      "Epoch 00151: val_acc improved from 0.89490 to 0.89610, saving model to model_ckpt_dropactivation_best_relu-dropout.h5\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4237 - acc: 0.9132 - val_loss: 0.5260 - val_acc: 0.8961\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 152/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4184 - acc: 0.9154\n",
      "Epoch 00152: val_acc did not improve from 0.89610\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4189 - acc: 0.9152 - val_loss: 0.5339 - val_acc: 0.8942\n",
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 153/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4209 - acc: 0.9139\n",
      "Epoch 00153: val_acc did not improve from 0.89610\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4209 - acc: 0.9139 - val_loss: 0.5307 - val_acc: 0.8957\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 154/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4237 - acc: 0.9131\n",
      "Epoch 00154: val_acc did not improve from 0.89610\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4235 - acc: 0.9133 - val_loss: 0.5317 - val_acc: 0.8953\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 155/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4194 - acc: 0.9141\n",
      "Epoch 00155: val_acc improved from 0.89610 to 0.89680, saving model to model_ckpt_dropactivation_best_relu-dropout.h5\n",
      "313/313 [==============================] - 31s 98ms/step - loss: 0.4193 - acc: 0.9141 - val_loss: 0.5253 - val_acc: 0.8968\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 156/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4171 - acc: 0.9161\n",
      "Epoch 00156: val_acc did not improve from 0.89680\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4167 - acc: 0.9162 - val_loss: 0.5286 - val_acc: 0.8963\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 157/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4157 - acc: 0.9149\n",
      "Epoch 00157: val_acc improved from 0.89680 to 0.89850, saving model to model_ckpt_dropactivation_best_relu-dropout.h5\n",
      "313/313 [==============================] - 31s 98ms/step - loss: 0.4156 - acc: 0.9150 - val_loss: 0.5207 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 158/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4140 - acc: 0.9156\n",
      "Epoch 00158: val_acc did not improve from 0.89850\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4138 - acc: 0.9156 - val_loss: 0.5228 - val_acc: 0.8961\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 159/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4165 - acc: 0.9149\n",
      "Epoch 00159: val_acc did not improve from 0.89850\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4166 - acc: 0.9148 - val_loss: 0.5240 - val_acc: 0.8958\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 160/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4108 - acc: 0.9164\n",
      "Epoch 00160: val_acc did not improve from 0.89850\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4111 - acc: 0.9162 - val_loss: 0.5261 - val_acc: 0.8951\n",
      "\n",
      "Epoch 00161: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 161/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4137 - acc: 0.9163\n",
      "Epoch 00161: val_acc did not improve from 0.89850\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4134 - acc: 0.9164 - val_loss: 0.5305 - val_acc: 0.8938\n",
      "\n",
      "Epoch 00162: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 162/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4132 - acc: 0.9160\n",
      "Epoch 00162: val_acc did not improve from 0.89850\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4132 - acc: 0.9160 - val_loss: 0.5267 - val_acc: 0.8955\n",
      "\n",
      "Epoch 00163: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 163/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4084 - acc: 0.9169\n",
      "Epoch 00163: val_acc did not improve from 0.89850\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4089 - acc: 0.9167 - val_loss: 0.5181 - val_acc: 0.8964\n",
      "\n",
      "Epoch 00164: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 164/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4086 - acc: 0.9183\n",
      "Epoch 00164: val_acc did not improve from 0.89850\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4086 - acc: 0.9183 - val_loss: 0.5228 - val_acc: 0.8968\n",
      "\n",
      "Epoch 00165: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 165/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4107 - acc: 0.9166\n",
      "Epoch 00165: val_acc did not improve from 0.89850\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4105 - acc: 0.9167 - val_loss: 0.5227 - val_acc: 0.8955\n",
      "\n",
      "Epoch 00166: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 166/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4060 - acc: 0.9185\n",
      "Epoch 00166: val_acc did not improve from 0.89850\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4060 - acc: 0.9186 - val_loss: 0.5287 - val_acc: 0.8938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00167: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 167/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4059 - acc: 0.9168\n",
      "Epoch 00167: val_acc did not improve from 0.89850\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4059 - acc: 0.9167 - val_loss: 0.5247 - val_acc: 0.8941\n",
      "\n",
      "Epoch 00168: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 168/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4056 - acc: 0.9175\n",
      "Epoch 00168: val_acc did not improve from 0.89850\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4057 - acc: 0.9175 - val_loss: 0.5201 - val_acc: 0.8965\n",
      "\n",
      "Epoch 00169: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 169/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4027 - acc: 0.9190\n",
      "Epoch 00169: val_acc improved from 0.89850 to 0.90010, saving model to model_ckpt_dropactivation_best_relu-dropout.h5\n",
      "313/313 [==============================] - 31s 98ms/step - loss: 0.4027 - acc: 0.9189 - val_loss: 0.5135 - val_acc: 0.9001\n",
      "\n",
      "Epoch 00170: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 170/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4032 - acc: 0.9173\n",
      "Epoch 00170: val_acc did not improve from 0.90010\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4031 - acc: 0.9173 - val_loss: 0.5382 - val_acc: 0.8938\n",
      "\n",
      "Epoch 00171: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 171/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4006 - acc: 0.9196\n",
      "Epoch 00171: val_acc did not improve from 0.90010\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4006 - acc: 0.9197 - val_loss: 0.5307 - val_acc: 0.8944\n",
      "\n",
      "Epoch 00172: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 172/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4040 - acc: 0.9177\n",
      "Epoch 00172: val_acc did not improve from 0.90010\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4039 - acc: 0.9178 - val_loss: 0.5236 - val_acc: 0.8969\n",
      "\n",
      "Epoch 00173: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 173/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4021 - acc: 0.9170\n",
      "Epoch 00173: val_acc did not improve from 0.90010\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4019 - acc: 0.9170 - val_loss: 0.5218 - val_acc: 0.8970\n",
      "\n",
      "Epoch 00174: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 174/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3976 - acc: 0.9194\n",
      "Epoch 00174: val_acc did not improve from 0.90010\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.3976 - acc: 0.9194 - val_loss: 0.5242 - val_acc: 0.8979\n",
      "\n",
      "Epoch 00175: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 175/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.4010 - acc: 0.9183\n",
      "Epoch 00175: val_acc did not improve from 0.90010\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.4010 - acc: 0.9182 - val_loss: 0.5163 - val_acc: 0.8970\n",
      "\n",
      "Epoch 00176: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 176/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3956 - acc: 0.9206\n",
      "Epoch 00176: val_acc did not improve from 0.90010\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.3958 - acc: 0.9206 - val_loss: 0.5206 - val_acc: 0.8990\n",
      "\n",
      "Epoch 00177: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 177/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3974 - acc: 0.9188\n",
      "Epoch 00177: val_acc did not improve from 0.90010\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.3975 - acc: 0.9188 - val_loss: 0.5224 - val_acc: 0.8974\n",
      "\n",
      "Epoch 00178: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 178/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3953 - acc: 0.9203\n",
      "Epoch 00178: val_acc did not improve from 0.90010\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.3953 - acc: 0.9203 - val_loss: 0.5145 - val_acc: 0.8994\n",
      "\n",
      "Epoch 00179: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 179/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3966 - acc: 0.9183\n",
      "Epoch 00179: val_acc did not improve from 0.90010\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.3968 - acc: 0.9182 - val_loss: 0.5130 - val_acc: 0.8997\n",
      "\n",
      "Epoch 00180: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 180/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3933 - acc: 0.9188\n",
      "Epoch 00180: val_acc did not improve from 0.90010\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.3934 - acc: 0.9188 - val_loss: 0.5249 - val_acc: 0.8969\n",
      "\n",
      "Epoch 00181: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 181/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3961 - acc: 0.9198\n",
      "Epoch 00181: val_acc did not improve from 0.90010\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.3960 - acc: 0.9198 - val_loss: 0.5226 - val_acc: 0.8957\n",
      "\n",
      "Epoch 00182: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 182/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3965 - acc: 0.9180\n",
      "Epoch 00182: val_acc did not improve from 0.90010\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.3967 - acc: 0.9180 - val_loss: 0.5143 - val_acc: 0.8978\n",
      "\n",
      "Epoch 00183: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 183/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3961 - acc: 0.9192\n",
      "Epoch 00183: val_acc did not improve from 0.90010\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.3960 - acc: 0.9193 - val_loss: 0.5145 - val_acc: 0.8980\n",
      "\n",
      "Epoch 00184: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 184/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3906 - acc: 0.9201\n",
      "Epoch 00184: val_acc did not improve from 0.90010\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.3907 - acc: 0.9202 - val_loss: 0.5126 - val_acc: 0.8995\n",
      "\n",
      "Epoch 00185: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 185/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3883 - acc: 0.9217\n",
      "Epoch 00185: val_acc did not improve from 0.90010\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.3884 - acc: 0.9217 - val_loss: 0.5126 - val_acc: 0.8986\n",
      "\n",
      "Epoch 00186: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 186/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3887 - acc: 0.9196\n",
      "Epoch 00186: val_acc did not improve from 0.90010\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.3894 - acc: 0.9195 - val_loss: 0.5119 - val_acc: 0.8996\n",
      "\n",
      "Epoch 00187: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 187/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3903 - acc: 0.9214\n",
      "Epoch 00187: val_acc did not improve from 0.90010\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.3905 - acc: 0.9214 - val_loss: 0.5115 - val_acc: 0.8991\n",
      "\n",
      "Epoch 00188: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 188/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3836 - acc: 0.9224\n",
      "Epoch 00188: val_acc improved from 0.90010 to 0.90020, saving model to model_ckpt_dropactivation_best_relu-dropout.h5\n",
      "313/313 [==============================] - 31s 98ms/step - loss: 0.3837 - acc: 0.9224 - val_loss: 0.5100 - val_acc: 0.9002\n",
      "\n",
      "Epoch 00189: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 189/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3939 - acc: 0.9196\n",
      "Epoch 00189: val_acc improved from 0.90020 to 0.90040, saving model to model_ckpt_dropactivation_best_relu-dropout.h5\n",
      "313/313 [==============================] - 31s 97ms/step - loss: 0.3938 - acc: 0.9197 - val_loss: 0.5107 - val_acc: 0.9004\n",
      "\n",
      "Epoch 00190: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 190/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3903 - acc: 0.9212\n",
      "Epoch 00190: val_acc did not improve from 0.90040\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.3903 - acc: 0.9212 - val_loss: 0.5122 - val_acc: 0.8997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00191: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 191/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3906 - acc: 0.9221\n",
      "Epoch 00191: val_acc did not improve from 0.90040\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.3904 - acc: 0.9221 - val_loss: 0.5130 - val_acc: 0.8994\n",
      "\n",
      "Epoch 00192: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 192/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3915 - acc: 0.9207\n",
      "Epoch 00192: val_acc did not improve from 0.90040\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.3913 - acc: 0.9207 - val_loss: 0.5126 - val_acc: 0.8998\n",
      "\n",
      "Epoch 00193: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 193/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3863 - acc: 0.9222\n",
      "Epoch 00193: val_acc did not improve from 0.90040\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.3862 - acc: 0.9222 - val_loss: 0.5134 - val_acc: 0.9001\n",
      "\n",
      "Epoch 00194: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 194/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3887 - acc: 0.9220\n",
      "Epoch 00194: val_acc did not improve from 0.90040\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.3889 - acc: 0.9219 - val_loss: 0.5120 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00195: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 195/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3881 - acc: 0.9219\n",
      "Epoch 00195: val_acc did not improve from 0.90040\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.3882 - acc: 0.9219 - val_loss: 0.5138 - val_acc: 0.8986\n",
      "\n",
      "Epoch 00196: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 196/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3856 - acc: 0.9226\n",
      "Epoch 00196: val_acc did not improve from 0.90040\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.3855 - acc: 0.9226 - val_loss: 0.5128 - val_acc: 0.8995\n",
      "\n",
      "Epoch 00197: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 197/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3864 - acc: 0.9217\n",
      "Epoch 00197: val_acc did not improve from 0.90040\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.3865 - acc: 0.9216 - val_loss: 0.5126 - val_acc: 0.8993\n",
      "\n",
      "Epoch 00198: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 198/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3863 - acc: 0.9220\n",
      "Epoch 00198: val_acc did not improve from 0.90040\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.3863 - acc: 0.9220 - val_loss: 0.5134 - val_acc: 0.8983\n",
      "\n",
      "Epoch 00199: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 199/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3857 - acc: 0.9227\n",
      "Epoch 00199: val_acc did not improve from 0.90040\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.3859 - acc: 0.9225 - val_loss: 0.5124 - val_acc: 0.8993\n",
      "\n",
      "Epoch 00200: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 200/200\n",
      "312/313 [============================>.] - ETA: 0s - loss: 0.3943 - acc: 0.9196\n",
      "Epoch 00200: val_acc did not improve from 0.90040\n",
      "313/313 [==============================] - 30s 97ms/step - loss: 0.3943 - acc: 0.9196 - val_loss: 0.5121 - val_acc: 0.8997\n",
      "CPU times: user 1h 41min 26s, sys: 14min 48s, total: 1h 56min 15s\n",
      "Wall time: 1h 41min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "history = model.fit_generator(iterator_train_aug, \n",
    "                              steps_per_epoch=steps_per_epoch_train,\n",
    "                              epochs=EPOCHS,\n",
    "                              verbose=1,\n",
    "                              validation_data=iterator_valid,\n",
    "                              validation_steps=steps_per_epoch_val,\n",
    "                              callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('relu-dropout-history.dict', 'wb') as f:\n",
    "    pickle.dump(history.history, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'val_loss', 'val_acc', 'acc']\n"
     ]
    }
   ],
   "source": [
    "hist_dict = history.history\n",
    "print(list(hist_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "model.load_weights('model_ckpt_dropactivation_best_relu-dropout.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best epoch : 188  |  0.9004\n"
     ]
    }
   ],
   "source": [
    "best_epoch = np.argmax(hist_dict['val_acc'])\n",
    "print(\"best epoch : {}  |  {}\".format(best_epoch, hist_dict['val_acc'][best_epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 1s 18ms/step\n",
      "ACC (test) :  0.8992\n",
      "LOSS (test) :  0.5142434755563736\n"
     ]
    }
   ],
   "source": [
    "loss_test, acc_test = model.evaluate_generator(generator=iterator_test, steps=steps_per_epoch_test, verbose=1)\n",
    "print(\"ACC (test) : \", acc_test)\n",
    "print(\"LOSS (test) : \", loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f3a7abb5e10>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "plt.plot(list(range(EPOCHS)), hist_dict['acc'], color='red', label='train acc')\n",
    "plt.plot(list(range(EPOCHS)), hist_dict['val_acc'], color='green', label='val acc')\n",
    "plt.scatter([best_epoch, ], [acc_test, ], color='blue', label='test acc')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f3a7b140e48>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "plt.plot(list(range(EPOCHS)), hist_dict['loss'], color='red', label='train loss')\n",
    "plt.plot(list(range(EPOCHS)), hist_dict['val_loss'], color='green', label='val loss')\n",
    "plt.scatter([best_epoch, ], [loss_test, ], color='blue', label='test loss')\n",
    "plt.legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
